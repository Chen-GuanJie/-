# 北京邮电大学

# 博士学位论文

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/626ed3100bd0d394a90df39e61fbce9f4ad1347d16d11b9fe86c65bd0840c389.jpg)


# 题目： 视觉语言模型驱动的移动端GUI 智能体的构建与评估方法研究

学号： 2021010274

姓名： 张力

学科专业： 计算机科学与技术

学习方式： 全日制

导师： 徐梦炜

学 院：计算机学院(国家示范性软件学院)

2025年11月10日

中国·北京

# 北京邮电大学

# 博士学位论文(学术学位)

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/e85cb28ca0ede4d3213fc11da0d0873acf76ebdce3420df54a7f9dbc8748b6ff.jpg)


# 题目： 视觉语言模型驱动的移动端GUI智能体的构建与评估方法研究

学号： 2021010274

姓名： 张力

学科专业： 计算机科学与技术

学习方式： 全日制

导师： 徐梦炜

学院：计算机学院（国家示范性软件学院）

2025年11月10日

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/42266e99e5ebf92e110fe4a2b15011c40547d368161bafdbfc72760c80610fbc.jpg)


# BEIJING UNIVERSITY OF POSTS AND TELECOMMUNICATIONS

# Doctoral Dissertation

# Research on Construction and Evaluation

# Methods for VLM-Powered Mobile GUI Agents

Student ID: 2021010274

Author: Li Zhang

Subject: Computer Science and Technology

Supervisor: Mengwei Xu

Institute: School of Computer Science

(National Pilot Software Engineering School)

10 November 2025


答辩委员会名单


<table><tr><td>职务</td><td>姓名</td><td>职称</td><td>工作单位</td></tr><tr><td>主席</td><td>崔斌</td><td>教授</td><td>北京大学</td></tr><tr><td>委员</td><td>任炬</td><td>副教授</td><td>清华大学</td></tr><tr><td>委员</td><td>王尚广</td><td>教授</td><td>北京邮电大学</td></tr><tr><td>委员</td><td>牛琨</td><td>教授</td><td>北京邮电大学</td></tr><tr><td>委员</td><td>徐梦炜</td><td>副教授</td><td>北京邮电大学</td></tr><tr><td>秘书</td><td>李元哲</td><td>岗位特聘副教授</td><td>北京邮电大学</td></tr><tr><td>答辩日期</td><td colspan="3">2025年10月27日</td></tr></table>

# 摘要

智能手机已成为连接人与数字世界的重要载体，它为使用者带来便利的同时，也引入了频繁、重复且复杂的人机交互负担。此类交互对普通用户增加了操作成本，对驾驶场景使用者或视障人群则可能造成额外的障碍与安全风险。为减轻此类负担，Apple Siri、Google Assistant等移动端智能体通过自动化执行高重复性和冗长的操作以提升用户效率与使用体验。然而，这类智能体主要依赖应用编程接口完成任务，在任务覆盖范围与灵活性上存在显著局限，难以满足日益多样化的用户需求。近年来，视觉语言模型（Vision-Language Model，VLM）凭借其卓越的语言理解能力与视觉感知能力，为构建通用移动端智能体提供了新的机遇。然而，现有基于VLM的移动端GUI 智能体仍存在模型能力受限、系统执行效率低及评估体系失真的问题，制约其在真实移动端环境中的推广与应用。为构建具备强感知与决策能力、高执行效率的移动端GUI 智能体及其科学评估体系，本文针对模型、系统与评估方法三个维度的挑战，提出如下创新性方案。

在模型层面，现有 VLM 多数依赖自然图像上的对比学习进行预训练，鲜有在移动端 GUI 数据集上进行大规模系统性训练，导致其难以理解复杂 GUI 结构并据此做出准确决策。其根本原因在于高质量、大规模移动端 GUI 数据集的长期匮乏。为解决该问题，本研究基于 SoC（System-on-Chip，片上系统）阵列服务器构建了高密度的原生移动端运行环境，并引入大语言模型以辅助移动端应用的自动化遍历。基于该系统，本研究收集并开源了大规模移动端 GUI 数据集 MobileViews，共包含来自 Google Play Store 超过 2 万个应用与 60 万张不同 GUI 表征。与此前最大规模的数据集 Rico 相比，MobileViews 在应用数量和 GUI 表征数量上分别提升约 2 倍和近 10 倍。随后，本研究在 MobileViews 上设计了基于自监督强化学习的 K 步界面转移任务，以充分利用大规模无标注的 GUI 转换轨迹强化现有 VLM。实验结果表明，该训练任务有效增强了 VLM 的屏幕感知与决策能力：在 GUI 动作预测任务中，相比基础模型，准确率最高提升  $7.5\%$ ；相比有监督微调方法，准确率平均提升  $36\%$ 。该方法为 VLM 在移动端 GUI 智能体上的应用奠定了数据与训练范式基础。

在系统层面，现有移动端GUI智能体通常集成多种测试时

扩展方法以提升任务完成率。然而，其顺序执行的工作流程导致此类方法显著增加了任务完成时延。为解决该问题，本研究首先对基于推理能力的VLM的测试时扩展方法进行了系统性实证研究，展现其在静态基准测试上评估性能的低准确率，强调了在交互式环境中评估GUI智能体的必要性。随后，本研究提出面向交互式环境的移动端GUI智能体并行推测执行工作流，以替代传统的顺序单路径执行范式。其设计核心是通过并行探索多条任务执行路径，并动态舍弃进展滞后或潜在错误的分支，在保证任务完成率的同时提高执行效率。实验结果表明，该方法相较于传统GUI智能体，以更少的操作步数实现了  $6.2\%$  的任务完成率提升，并在  $74\%$  的已完成任务中表现出更高的执行效率，优于顺序扩展方法。该方法为构建兼具高响应速度与强鲁棒性的移动端GUI智能体提供了一种全新的系统级解决方案。

在评估方法层面，现有面向移动端GUI智能体的基准测试大多依赖静态基准测试上的精确动作序列匹配。然而，该方法忽略了交互式环境中任务完成路径的多样性。基于这一观察，本研究构建并开源了基于关键状态匹配的交互式基准测试框架LlamaTouch。其通过检测任务执行过程中或任务完成后，系统与应用是否达成预定义的关键状态来判断任务是否成功完成，从而突破了传统动作级匹配的限制。LlamaTouch设计了多粒度的GUI组件与系统状态标注原语，并提出多层次的匹配算法，实现了高保真度的任务完成状态标注与匹配。在496个测试任务上的实验表明，LlamaTouch的自动化评估结果与人工评判一致性接近  $80\%$  ，同时该基准测试框架的细粒度标注方式具备良好的跨应用、跨任务扩展性。作为一个高保真度、可扩展的评估平台，LlamaTouch提供了更贴近真实场景的基准测试工具，推动了移动端GUI智能体评估标准的演进。

关键词：视觉语言模型；GUI智能体；测试时扩展

# ABSTRACT

Smartphones have become a vital bridge connecting people to the digital world. While they offer convenience, they also introduce frequent, repetitive, and complex human-computer interactions. Such interactions not only increase operational burdens for general users but also pose additional accessibility and safety challenges for users in driving scenarios or those with visual impairments. To address these issues, mobile agents such as Apple Siri and Google Assistant have emerged, aiming to reduce user workload and enhance efficiency and experience by automating highly repetitive and lengthy operations. However, these agents primarily rely on invoking application programming interfaces to complete tasks, which significantly limits their task coverage and flexibility, making them inadequate for meeting increasingly diverse user demands. In recent years, Vision-Language Models (VLMs), with their powerful language understanding and visual perception capabilities, have opened new opportunities for building general-purpose mobile agents. Nonetheless, current VLM-based mobile GUI agents still suffer from limited model capability, low system execution efficiency, and flawed evaluation methodologies, hindering their deployment and adoption in real-world mobile environments. To build mobile GUI agents with strong perception and decision-making capabilities, high execution efficiency, and a sound evaluation framework, this thesis addresses the challenges from three aspects: model, system, and evaluation methodology, and proposes a set of innovative solutions.

At the model level, most existing VLMs are pretrained via contrastive learning on natural images, with little training on mobile GUI datasets. This hinders their ability to understand complex GUI structures and therefore making accurate decisions. A fundamental reason for this is the longstanding lack of high-quality, large-scale mobile GUI datasets. To address this, this study built a high-density native mobile environment using SoC Clusters, and introduced a

large language model (LLM)-based automated explorer to traverse mobile applications. The use of LLM is to handle complex GUI interactions such as registration and login screen. Based on this system, this study collects and open-sources a large-scale mobile GUI dataset, MobileViews, which contains over 600,000 unique screen representations from more than 20,000 applications on the Google Play Store. The number of applications and screen representations in MobileViews is approximately twice and nearly ten times that of the previously largest dataset, Rico, respectively. Then, this study design a self-supervised reinforcement learning task, K-step GUI transition, to effectively utilize the large volume of unlabeled GUI transition trajectories in MobileViews. Experimental results indicate that the training task effectively enhances the VLM's screen perception and decision-making capabilities. In the GUI action prediction task, the model achieves up to a  $7.5\%$  increase in accuracy, with an average improvement of  $36\%$  over supervised fine-tuning methods. This approach establishes both a data and methodological foundation for applying VLMs to mobile GUI agents.

At the system level, current mobile GUI agents often integrate multiple test-time scaling modules to improve task completion rates. However, the sequential execution workflow of mobile GUI agents substantially increases task completion latency. To overcome this, this work first conducts a systematic empirical study of utilizing reasoning-enabled VLM in mobile GUI agents, highlighting the necessity of evaluating mobile GUI agents in interactive environments. Based on our findings, this study proposes a parallel speculative execution (PSE) workflow in interactive environments, aiming to replace the traditional sequential execution paradigm. The core idea of PSE is to explore multiple task execution paths in parallel while dynamically pruning lagging or potentially incorrect branches. Experiments show that PSE-powered mobile GUI agents achieve a  $6.2\%$  improvement in task completion rate with fewer interaction steps. They also demonstrate higher task execution efficiency, i.e., completing tasks with fewer steps, in  $74\%$  of

completed tasks, outperforming the sequential test-time scaling approach. The proposed method offers a novel system-level solution for building responsive and robust mobile GUI agents.

At the evaluation level, existing benchmarks for mobile GUI agents primarily rely on exact action matching within static datasets, which fails to evaluate the diversity of task completion paths in interactive environments. To address this limitation, this study introduces and open-sources LlamaTouch, an interactive benchmark based on essential application state matching. LlamaTouch determines task success by checking whether the system or application reaches predefined essential states during or after execution, thus overcoming the limitations of traditional step-level exact action matching. LlamaTouch supports flexible and fine-grained evaluation by incorporating various GUI and system state annotation primitives, enabling multi-granularity and high-fidelity evaluation. Experiments across 496 test tasks show that LlamaTouch achieves around  $80\%$  accuracy through automatic evaluation when taking human evaluation results as the baseline. LlamaTouch can also extend its test sets to diverse commercial mobile applications. As a high-fidelity, automatic evaluation platform, LlamaTouch provides the research community with a benchmark that better reflects real-world scenarios, advancing the evaluation standards for mobile GUI agents.

KEY WORDS: vision-language model; GUI agent; test-time scaling

# 目录

# 第一章绪论

1.1 研究背景与意义 1

1.2 国内外研究现状 5

1.2.1 移动端智能体: 从 API 到 GUI 的演进 ..... 5

1.2.2 移动端GUI智能体工作流 6

1.2.3 面向移动端GUI智能体的基准测试 7

1.3 关键挑战 8

1.4研究内容及贡献 10

1.4.1大规模GUI数据驱动的VLM能力强化方法 10

1.4.2 面向GUI任务的高效测试时扩展方法 11

1.4.3 基于关键状态匹配的自动化评估方法 ..... 12

1.5 论文组织架构 13

1.6 课题来源 ..... 14

# 第二章 国内外研究现状及分析 15

2.1 视觉语言模型发展与演进 15

2.2移动端智能体的发展与演进 16

2.3移动端GUI数据集 18

2.4 面向GUI智能体的测试时扩展方法 ..... 20

2.5 面向GUI智能体的基准测试 21

2.6 本章小结 ..... 24

# 第三章 大规模GUI数据驱动的VLM能力强化方法 25

3.1 本章概述 ..... 25

3.2基于SoC阵列的GUI数据收集方法 28

3.2.1 总体流程 ..... 29

3.2.2 LLM 驱动的单应用渐进式遍历方法 ..... 29

3.2.3 基于 SoC 阵列的多应用并行交互框架 31

3.3 GUI 数据集 MobileViews 统计分析 33

3.3.1 统计数据 33

3.3.2 GUI 组件标签对比分析 35

3.4基于自监督强化学习的VLM强化方法 38

3.4.1K步界面转移任务 38

3.4.2 奖励函数设计 39

3.5GUI数据集对比评估 40

3.5.1 两阶段训练任务与数据构造 ..... 40

3.5.2 实验设置 42

3.5.3 实验结果 43

3.6 实验与分析 43

3.6.1 实验设置 44

3.6.2 实验结果 45

3.7 本章小结 ..... 48

第四章 面向GUI任务的高效测试时扩展方法 49

4.1 本章概述 ..... 49

4.2基于推理模型的测试时扩展方法与实证研究 52

4.2.1 问题描述 ..... 52

4.2.2 实证研究方法 ..... 54

4.2.3 实证研究结论与分析 55

4.3 基于并行推测执行测试时扩展方法 60

4.3.1 问题描述 60

4.3.2 系统架构设计与执行流程 60

4.3.3 意图感知的动作采样与去重方法 63

4.3.4基于快速环境复制的分支并行执行方法 65

4.3.5 子任务进度自感知的执行分支剪枝策略 66

4.4 实验与分析 67

4.4.1 系统实现 67

4.4.2 实验设置 67

4.4.3 实验结果 68

4.5 本章小结 ..... 71

第五章 基于关键状态匹配的自动化评估方法 73

5.1 本章概述 ..... 73

5.2 交互式GUI任务评估系统LlamaTouch 75

5.2.1 基于交互式环境的GUI任务执行流程 76

5.2.2 关键状态标注原语设计 77

5.2.3 多层次状态匹配的评估方法 ..... 81

5.3 基于关键状态标注原语的任务集构建 ..... 82

5.3.1 任务集构建方法 ..... 83

5.3.2 任务集统计信息 ..... 85

5.4 实验与分析 86

5.4.1 实验设置 86

5.4.2 任务完成率与评估方法准确率 ..... 88

5.4.3 消融实验 89

5.4.4 GUI智能体性能分析 90

5.5 本章小结 91

第六章 工作总结与展望 93

6.1 论文主要研究工作总结 93

6.2 未来研究工作与展望 ..... 95

参考文献. 99

附录. 115

# 图目录

图1-1API智能体与GUI智能体 2

图1-2本文研究内容示意图 10

图1-3 论文组织结构示意图 13

图2-1移动端GUI智能体现状与关键组件 17

图2-2 精确动作匹配中假阴性评估示例 23

图2-3不同任务执行路径相同执行结果示例 23

图3-1本章研究内容示意图及屏幕问答任务示例 25

图3-2MobileViews数据集构建流程与硬件平台 29

图3-3自动化应用遍历流程的输入与输出示例 31

图3-4SoC阵列服务器架构图 32

图3-5MobileViews中不同应用程序GUI数量分布 34

图3-6不同GUI数据集中相似屏幕数量的累计分布函数 34

图3-7 Rico和MobileViews中无标签GUI组件比例 35

图3-8不同数据集和GUI组件类型下标签长度分布 36

图3-9自监督强化学习训练任务-K步界面转移 38

图3-10两阶段训练任务示意图 41

图4-1GUI智能体顺序执行与并行推测执行示意图 50

图4-2 Gemini 2.0 Flash推理模型输出示例.. 53

图4-3并行推测执行工作流程 61

图4-4动作采样与VLM驱动的去重示例 63

图4-5AndroidWorld任务完成率与任务平均消耗步数 68

图4-6AndroidWorld任务完成率与VLM令牌消耗 69

图4-7 顺序扩展方法与并行扩展方法逐任务步数消耗 ..... 70

图4-8 顺序扩展方法与并行扩展方法逐任务令牌消耗 ..... 71

图5-1LlamaTouch工作流和各组件交互流程 76

图5-2交互式环境中GUI智能体任务执行流程 76

图5-3关键GUI组件提取与视觉强化流程示意图 78

图5-4使用VH强化的GUI任务轨迹与标注示意图 79

图5-5LlamaTouch任务集构建流程 83

图5-6关键状态标注系统用户界面示意图 84

# 表目录

表 2-1 开源移动端 GUI 数据集对比 ..... 19

表 2-2 移动端 GUI 智能体基准测试对比 ..... 22

表3-1开源GUI数据集收集方式与硬件架构 26

表3-2自动化移动端应用遍历方法成功率 30

表3-3标签到视图匹配任务提示词设计 37

表3-4不同标签长度下GPT-4o视图识别任务匹配准确率. 37

表3-5VH生成任务数据规模对基础GUI任务性能影响. 43

表3-6VH生成任务数据规模对ScreenQA性能的影响 43

表3-7用于GRPO训练的超参数设置 44

表 3-8 AndroidControl-Low 测试集性能比较 ..... 45

表3-9AndroidControl-High测试集性能比较 46

表3-10K值与数据规模对Qwen2.5-VL-7B性能影响 47

表4-1移动端GUI智能体任务完成时延与完成率 51

表4-2ScreenSpot测试集上定位任务准确率 56

表4-3AndroidControl测试集上动作预测任务准确率 56

表4-4AndroidWorld测试集上端到端任务完成率 57

表4-5 Gemini2.0Flash推理与非推理模型任务完成率 57

表 4-6 Claude 3.7 Sonnet 推理与非推理模型任务完成率 ..... 58

表4-7推理流程引发失败任务人工归因与分类 59

表4-8意图-动作采样流程提示词设计 63

表4-9意图感知的动作去重提示词示例 64

表5-1LlamaTouch提供给GUI智能体的编程接口. 77

表5-2LlamaTouch中的标注原语设计及用例说明 79

表5-3LlamaTouch任务集统计数据 85

表5-4LlamaTouch任务集动作类型统计信息 86

表5-5LlamaTouch任务集关键状态统计信息 86

表5-6GUI智能体端到端任务完成率与评估方法准确率 88

表5-7人工评估成功任务中评估方法准确率 89

表 5-8 LlamaTouch 针对不同关键状态标注原语的消融实验 ..... 90

表5-9不同任务来源下GUI智能体性能分析 91

表5-10不同任务复杂度下GUI智能体性能分析 91


符号说明


<table><tr><td>符号</td><td>代表意义</td><td>单位</td></tr><tr><td>St</td><td>t时刻GUI表征</td><td>-</td></tr><tr><td>Rf</td><td>输出格式奖励函数</td><td>-</td></tr><tr><td>Ra</td><td>动作奖励函数</td><td>-</td></tr><tr><td>R</td><td>总体奖励函数</td><td>-</td></tr><tr><td>ε</td><td>梯度裁剪参数</td><td>-</td></tr><tr><td>β</td><td>KL散度系数</td><td>-</td></tr><tr><td>T</td><td>任务指令</td><td>-</td></tr><tr><td>S</td><td>子任务列表</td><td>-</td></tr><tr><td>env</td><td>任务执行环境</td><td>-</td></tr><tr><td>b</td><td>任务执行分支</td><td>-</td></tr><tr><td>B</td><td>任务执行分支集合</td><td>-</td></tr><tr><td>a</td><td>采样动作</td><td>-</td></tr><tr><td>A</td><td>动作集合</td><td>-</td></tr><tr><td>x</td><td>执行环境GUI表征</td><td>-</td></tr><tr><td>k</td><td>执行分支子任务进度索引</td><td>-</td></tr><tr><td>II</td><td>指示函数</td><td>-</td></tr></table>

# 第一章 绪论

# 1.1 研究背景与意义

在当今数字化浪潮席卷全球的时代，手机已从最初的通讯工具，演变为集通信、娱乐、工作、学习和生活服务于一体的个人智能终端，重塑了人与现代社会交互形式。Exploding Topics发布的报告显示，到2025年，全球智能手机用户数量预计将达到74.9亿①，这意味着全球绝大多数人口将依赖智能手机作为主要的数字接入终端。图形用户界面（Graphical User Interface, GUI）是用户与智能手机纷繁复杂的功能和服务进行交互的主要桥梁。GUI通过图标、按钮、菜单、窗口等视觉元素，将底层复杂的程序代码转化为直观、可操作的界面，极大地降低了用户使用高科技产品的门槛。据统计，成年人平均每天花费约4.5小时使用智能手机②，交互次数超过2600次③。这种高度渗透性和依赖性，使得智能手机的易用性和交互效率成为影响数十亿用户日常体验的关键因素。然而，无论是查阅信息、处理邮件、完成支付，还是使用地图导航、在线购物与社交沟通，用户都需要频繁操作屏幕进行多步交互，显著增加了用户的时间开销和认知负担。心理学与人机交互研究表明，长时间使用智能终端容易造成注意力分散、操作疲劳，甚至引发高达  $40\%$  的效率损失④。在跨应用信息收集、表单填写及多步骤设置等场景中，用户往往需要频繁切换界面、输入与确认，过程既冗长又易出错；而在驾驶、烹饪等特定情境，或针对视力、肢体障碍人群，传统的点击与滑动交互更显繁琐与不便。这一矛盾凸显了任务自动化的必要性——只有将高重复性、冗长且难以手动完成的操作交由系统自动执行，才能减轻用户负担，提升交互体验与工作效率。

为应对这一需求，产业界与学术界先后提出了多种任务自动化方式[1-8]。商用系统方面，语音助手（如Apple Siri[1]、Google Assistant[4]、小爱同学[2]，小艺助手[3]等）和自动化工具（如IFTTT[9]、Tasker[10]、iOS Shortcuts[11]）已在全球范围广泛应用。例如，美国市场中，Google Assistant与Siri的月活跃用户分别超过9000万和8700万。这些系统能够帮助用户完成天气查询、闹钟设置、音乐播放等常见操作，在简单场景下极大减轻了用户的交

互负担。然而，如图1-1所示，这些语音助手和自动化工具大幅度依赖于应用暴露的应用编程接口（Application Programming Interface, API），它们的功能覆盖面相对有限，用户通常只能依赖开发者预设的少量功能，难以像人类一样灵活处理广泛的应用界面上的多样化需求。这种“高普及率、低满意度”的现象表明，现有自动化工具难以有效适配复杂多变的应用环境，缺乏广泛通用的交互能力。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/73b0cb9602aa7af8f03edd8997e5723aba302da2e7050464376fcaaf065410e4.jpg)


提醒我明晚7：30参加主题为GUI智能体的在线会议，会议号为170-256-215

GUI智能体

API智能体

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/8693b2f7d358415aba531bcb3485a9976b5a03c3bae54a3a1ce7093442a63c8e.jpg)


SetCalendarEvent(

Date=2025-08-15,

Time=19:30,

Title="GUI Agent",

Note  $=$  "Room:170-256-215)

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/8fd780b8a2c6a23486c8d5b227c9a7e8282279ee619c09bcad6a915dd9996cea.jpg)


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/472a2f21a67ec0c5e6ae1b124e2ba9e0f62354f9c74ea00e8e760138e6cbf880.jpg)


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/ccf8629f214d97d94c28b34def5c87a6c40838ccd9b594c7f126d06a991442ac.jpg)


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/422ab96ea6a28418264621f5d2d50f7fee7687b24e5cc41b2a343bbe9f186e04.jpg)


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/f073949bcee91e4fe3c973baa33b25e6a9e6912c5f5e571a338f78c98b883713.jpg)


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/16571f1b4f68411afb99f7fc76566379851b2dacf852755d0e7464e232ae7f0e.jpg)


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/3dbc501483ef412c6a7c18d36a4ce40beb1b7889ab9c48cb8cbc5885a6d497e5.jpg)



Completed


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/a8663eb1717440063979b6d7259371b1ebaaf3921a1b324c58a14843ec87a0a5.jpg)



图1-1API智能体与GUI智能体


为了应对这一挑战，提升人机交互的自然性和效率，移动端GUI智能体（Mobile GUI Agent）的概念应运而生，并迅速成为人工智能、人机交互和软件工程等领域的研究热点。移动端GUI智能体是一种能够理解用户意图、感知并解析手机屏幕内容，并能自主地在GUI上执行一系列操作以完成特定任务的智能程序[12,13]。与只能执行特定任务的API驱动的智能体相比，移动端GUI智能体就像一个“虚拟的个人助理”，能够将用户通过自然语言（语音或文字）下达的复杂指令，转化为在手机屏幕上连续、精准的点击、输入、滑动等具体动作，因而有潜力处理任意用户指令。例如，用户只需说一句“帮我在购物App里买一箱牛奶送到家”，GUI智能体就能自动完成解锁手机、打开App、搜索商品、加入购物车、选择地址、完成支付等一系列屏幕交互动作。

移动端GUI智能体的出现极大地提升了移动设备的可访问性（Accessibility）。对于老年人、视障人士或肢体残疾人士等特殊群体而言，GUI智能体通过语音等更自然的交互方式，为他们跨越“数字鸿沟”、平等地享受数字化生活带来的便利提供了可能。移动端GUI智能体也显著提

高了交互效率。对于普通用户，GUI 智能体能够将多步骤、跨应用的复杂任务自动化，将用户从重复、繁琐的屏幕操作中解放出来，从而节省宝贵的时间和精力，让他们能够更专注于任务本身的目标而非过程。其次，它推动了人机交互范式的革新。GUI 智能体将交互的中心从人适应机器转变为机器理解人，使得人机交互更加符合人类的自然交流习惯。这种以用户意图为导向的交互模式，预示着下一代智能终端交互体验的未来方向，即更加智能、主动和个性化的服务。移动端 GUI 智能体也为软件的自动化测试、用户行为分析、智能推荐等领域带来了全新的技术路径和广阔的应用前景。例如，开发者可以利用 GUI 智能体自动地在应用中执行各种复杂的用户指令，从而发现潜在的软件缺陷[14-16]；平台也可以通过分析 GUI 智能体的行为模式，深入理解用户需求，提供更加精准的服务[17-19]。

然而，构建一个鲁棒、通用且高效的移动端GUI智能体仍然面临着巨大的挑战。移动端GUI具有高度的动态性、多样性和复杂性。仅GooglePlayStore的应用数量已超过260万①，不同应用的界面设计风格迥异，同一应用的版本更新也可能导致界面布局和元素发生巨大变化。这要求GUI智能体必须具备极强的视觉理解和泛化能力，能够准确地识别和定位屏幕上的各种控件，并理解它们的语义和功能。此外，用户的自然语言指令往往是模糊的、口语化的，甚至可能包含省略和歧义。这需要GUI智能体拥有强大的自然语言理解和推理能力，能够准确地将用户的模糊意图映射到具体的GUI操作序列上。更进一步，任务的执行过程往往不是线性的，可能会遇到各种预料之外的情况，如弹出广告、网络延迟、账户验证等，智能体必须具备一定的反思和纠错能力，以应对这些动态变化，保证任务的顺利完成。

为了克服这些挑战，近年来，研究者日益关注以视觉语言模型（Vision-Language Model，VLM）为代表的先进人工智能技术。视觉语言模型是一种能够同时接收图像与文本作为输入，并对两种模态信息进行融合理解，从而完成如图文问答、图像描述生成等文本生成任务的多模态模型。基于VLM的移动端GUI智能体具备更强的跨模态感知、指令理解与交互推理能力，为复杂场景下的任务自动化提供了新的技术路径。在视觉层面，VLM赋能的GUI智能体能够解析手机屏幕，将其中的图标、文字、输入框等元素及其空间布局转化为结构化信息；在语言层面，其能准确理解用户的自然语言指令，提取其中的意图与约束条件；在推理层面，其基于视觉与语言的信息融合与理解，实现多步规划并生成合理的操作序列，以完成复杂

任务。因此，VLM的发展为通用移动端GUI智能体的实现提供了关键的技术支撑，使其真正具备现实可行性。

尽管基于 VLM 的移动端 GUI 智能体展现出广阔的应用前景，但该领域的研究尚处于起步阶段，仍面临一系列关键的科学问题与技术瓶颈。其中，如何构建能够支持 VLM 在复杂 GUI 环境中完成感知、决策与执行的高效系统，并建立全面且高保真的评估标准，是推动该领域向前发展的核心挑战。具体而言，本文的研究问题及意义体现在如下三个层面：

（1）VLM感知与决策能力的强化：高性能VLM的训练依赖于大规模、高质量的“屏幕-指令-动作”轨迹数据集。然而，获取大规模移动端GUI数据成本高昂，需要大量的人工标注，且难以覆盖真实世界中数以百万的应用和任务场景。现有的移动端GUI数据集在规模、GUI表征的多样性和覆盖度均存在严重不足。因此，研究高效的移动端GUI数据构建方法，例如通过自动化或半自动化的方式采集和标注数据，并以此为基础，设计新颖的自监督训练方法，是突破数据瓶颈、强化VLM在移动端GUI任务中的感知与决策能力的关键。

（2）GUI 智能体执行效率提升：为了保证复杂任务的成功率，现有移动端 GUI 智能体普遍采用“感知-决策-执行”的顺序循环模式，并集成反思与回溯等多种测试时扩展（Test-time Scaling）方法[20]。然而，这种顺序执行范式在遇到复杂 GUI 状态或未知情况时，会陷入冗长的试错循环，显著提高了任务完成时延，严重影响用户体验。因此，探索并改善现有测试时扩展方法以打破顺序执行的限制，在保证甚至提高任务成功率的前提下，降低任务完成时延，对于推动 GUI 智能体的实用化至关重要。

（3）高保真度评估方法的构建：当前对移动端GUI智能体的评估多依赖静态基准测试，通常基于智能体预测的单步动作与固定轨迹的精确比对计算成功率。然而，这类静态方法未能充分考虑真实交互环境的动态特征和任务完成路径的多样性，导致对智能体能力的评估不够全面与准确。因而，亟需开发一种能够在动态交互环境中，以任务完成状态为核心、兼容多路径解法的高保真自动化评估方法与系统，以实现对移动端GUI智能体性能的精确评估。

综上所述，移动端GUI智能体是应对当前移动设备交互复杂性挑战、提升用户体验、实现普惠计算的关键技术，具有重大的研究意义和广阔的应用前景。然而，其发展仍受到前述在模型能力、系统执行效率和评估方法三个维度的核心挑战制约。为应对这些挑战，本论文聚焦于“视觉语言模型驱动的移动端GUI智能体的构建与评估方法”这一核心议题，开展了

系统性的研究工作：首先，针对VLM能力欠缺的问题，本文提出了一套高效的自动化数据收集方案以收集大规模移动端GUI数据，并设计了相应的自监督学习方法，旨在从数据层面增强VLM对复杂GUI环境的感知与决策能力。其次，为解决系统层级高任务完成时延问题，本文创新性地提出了一种并行推测执行的测试时扩展方法，旨在优化GUI智能体的动作执行流程，显著降低任务完成时延。最后，为了实现准确评估，本文设计并实现了一个高保真度的自动化评估系统，它能够在动态交互环境中，基于对应用关键状态的匹配来评估任务执行情况，从而克服传统评估方法的局限性。本论文通过上述一系列的研究工作，旨在为构建更强大、更高效、可科学评估的移动端GUI智能体提供一套完整的解决方案，从而推动该技术从理论研究向实际应用的跨越。

# 1.2 国内外研究现状

本节综述了国内外在移动端智能体与VLM结合方向上的主要研究进展。通过回顾从API驱动到GUI驱动的智能体演进历程，分析当前移动端GUI智能体的典型工作流及其在真实交互环境中的性能瓶颈，并梳理现有基准测试体系的设计思路与不足。本节旨在为后续章节提出的模型、系统与评估三方面的创新方法奠定研究背景与问题依据，明确本文研究在现有工作体系中的定位。

# 1.2.1 移动端智能体：从API到GUI的演进

移动端智能体的核心目标是自动化执行用户在智能手机上的任务，其发展历程清晰地体现了人工智能技术从封闭指令系统向开放环境理解的跨越。早期移动端智能体，如Apple Siri[1]、Google Assistant[4]及小米公司的小爱同学[2]等，主要基于应用编程接口（API）工作。这类智能体通过调用预先定义的API与应用进行交互，在执行天气查询、设置提醒等结构化任务时表现稳定可靠。然而，其能力边界也因此被严格限定：一旦用户意图超出API的预设范围，或目标应用未提供相应接口，此类API智能体便无法完成任务。这种对结构化接口的强依赖性，使其难以适应移动生态中功能日益复杂的应用以及多样化的用户需求。随着LLM（Large Language Model，大语言模型）与VLM的发展，移动端智能体迎来了革命性的范式转变。研究者们开始探索一种更接近人类交互方式的智能体——基于GUI的智能体。与API智能体不同，GUI智能体通过对屏幕视觉信息的感知和理解GUI结构来执行操作，从而摆脱了对API的依赖，具备在任意应用中实现通用自动化控制的潜力。这一转变的背后，是VLM的飞跃式发展。

特别是自2023年起，以OpenAI的GPT-4o[21]、Google的Gemini[22]以及Anthropic的Claude  $3^{[23]}$  为代表的视觉语言模型，从设计之初就原生集成了图像、文本等多种模态信息的处理能力。它们在复杂的跨模态推理、对话和内容创作上表现出接近甚至超越人类的水平，为真正意义上的通用GUI智能体提供了坚实的推理基础。

然而，尽管现有的 VLM 已具备较强的多模态理解与推理能力，但在处理复杂的移动端 GUI 任务时，仍难以满足精确感知与多步推理的需求。这主要体现在以下几个方面：首先，移动端 GUI 具有高动态性和异构性，不同应用的界面布局、交互逻辑和组件样式千差万别，VLM 需要具备极强的泛化能力才能准确理解不同的 GUI 语义。其次，用户的自然语言指令往往存在模糊性和多义性，将这些高层级的意图定位到屏幕上具体的、可操作的 GUI 组件（如图标、按钮、输入框等），对 VLM 的细粒度视觉理解能力构成了巨大挑战。最后，完成一个复杂任务通常需要多步骤、跨应用的连续操作，这对 VLM 的长期记忆、任务规划和状态跟踪能力提出了更高的要求。因此，如何增强 VLM 在真实、复杂移动端环境中的感知与决策能力，是当前该领域面临的核心挑战。

# 1.2.2 移动端GUI智能体工作流

现有的移动端GUI智能体普遍采用标准的顺序工作流程，并通过引入多种测试时扩展模块来强化其决策能力。但这套复杂机制导致执行延迟显著增加，严重制约了其在真实应用场景中的部署与扩展。一个典型的移动端GUI智能体工作流程可被视为一个“感知-决策-行动”的循环。在每一轮决策中，智能体的输入通常由三部分组成：（1）用户下达的基于自然语言的任务指令；（2）GUI表征，即对当前GUI的数字化描述。现有GUI智能体通常使用提供视觉信息的屏幕截图，或提供文本形式GUI组件结构与属性的视图层级（ViewHierarchy,VH）[24]信息。此外，研究者们常通过OCR、图标检测或Set-of-Mark等技术对GUI表征进行增强[25-27]，以强化VLM对GUI的视觉理解；（3）工作内存，用于存储智能体历史操作序列、执行结果等上下文信息，帮助其理解任务进度并避免重复错误。VLM作为GUI智能体的大脑，负责整合所有输入信息，并作出下一步操作的决策。现有工作中，主流使用的VLM包括GPT-4o[21]，Google Gemini[22]，Qwen-VL[28]系列模型。GUI智能体的工作流程多样，包括直接输出操作、调用系统工具[29,30]、利用反思与回溯机制进行自我修正[31-34]，或通过多智能体协作完成复杂任务[35-37]。VLM最终生成具体的操作指令，如点击特定坐标（例如：click[238,465]）、在标记的组件上进行交互（例如：long_press

[10])、或输入指定文本（例如：type“Alice”）。该操作随后被发送到真实的移动设备或模拟器上执行，GUI界面状态转换后进入下一轮决策，直至任务完成或达到最大步骤限制。

为了提升GUI智能体处理复杂任务的准确率，研究者在工作流中引入了多种测试时扩展方法。这类方法的核心思想是在推理阶段使用更多的计算资源，以换取更高的性能。常见的方法包括：（1）提示词工程（Prompt Engineering）：例如采用思维链（Chain-of-Thought，CoT）提示，引导VLM在生成最终动作前，先进行详细的分析和推理[38-40]。（2）多候选动作生成与筛选：让VLM针对当前屏幕生成多个可能的候选动作，然后通过多数投票、基于世界模型的推演或基于奖励的最佳优先搜索等策略，从中筛选出最优解[41-44]。（3）自我改进与反思方法通过引入评估模块，让智能体在执行动作后自行判断当前状态是否符合预期。如果发现当前状态偏离任务指令，GUI智能体则回溯到之前的状态，或者修正未来的执行计划，从而实现自我纠错[31-33]。尽管测试时扩展模块在多项任务中显著提升了性能，但其代价同样不容忽视。无论是生成推理链、评估候选动作，还是进行多轮反思修正，都会成倍增加VLM的调用次数与计算负载，从而显著提高端到端时延。高延迟不仅削弱了用户体验，也限制了智能体在实际场景中的应用。因此，如何在保证任务成功率的同时有效降低由测试时扩展引入的推理时延，已成为移动端GUI智能体研究亟待解决的核心问题。

# 1.2.3 面向移动端GUI智能体的基准测试

如何构建面向移动端GUI智能体的科学、准确且可扩展的评估体系，为模型训练和系统迭代提供反馈优化，是推动领域进步的关键。然而，现有的基准测试方法在评估的高保真性与高可扩展之间难以兼顾，存在明显的局限性。目前，主流评估方法分为如下两类：

第一类是基于静态数据集的评估。代表性基准包括  $\mathbf{Rico}^{[45]}$  、  $\mathbf{PixelHelp}^{[46]}$  以及大规模的  $\mathbf{AITW}^{[47]}$  。此类静态数据集提供任务描述、屏幕截图与数据集中预定的单一动作序列。评估时，GUI 智能体需在静态 GUI 表征上预测动作，并与数据集内的真值标签进行精确比对。该方法易于自动化，拥有良好的扩展性。但其评估准确率低，主要体现在：（1）精确动作匹配的谬误，即 VLM 输出动作类型与参数需与数据集内唯一真值完全一致，忽视了 GUI 任务执行流程中功能等价的多样操作；（2）缺乏对多路径的容忍度，任务完成常被简化为单一预设路径，限制了移动端 GUI 智能体的自主探索能力。

第二类是基于交互式环境的评估。为了克服基于静态数据集评估的缺

陷，AndroidEnv[48]、Mobile-Env[49]、AndroidArena[50]等基准提供了可供GUI智能体实时交互的真实移动端环境，允许GUI智能体在环境中自由探索，并支持对同一任务多种完成路径的评估。然而，此类方法也面临新的挑战。首先是任务完成的标准难以定义。由于缺乏精细的评估标准，这类环境通常难以自动、准确地判断任务是否最终完成。例如，AndroidArena[50]虽然引入了基于最长公共子序列的动作匹配以放宽评估条件，但仍未脱离动作匹配的范式。WebArena[51]在网页端引入了关键状态匹配的思想，即在可控Web端环境中评估任务完成页面是否包含某些关键信息，但此方法难以直接应用于界面多样且动态性强的移动端应用。在缺乏自动化评估手段的情况下，前人研究依赖高保真度，但成本高昂且低效的人工评判方式来判断任务成功与否。但人工评判难以扩展到需要对多个智能体、在多种设备上进行大规模多任务测试的场景[52]。

综上，当前面向移动端GUI智能体的评估体系存在显著空白：缺乏一种既能支持真实环境下的交互式评估，又能自动化、高保真且可扩展地判定任务完成情况的基准。构建这样一套新型评估体系，对推动移动端GUI智能体从理论研究走向实际应用具有重要意义。

# 1.3 关键挑战

本研究从构建实用化移动端GUI智能体的需求出发，旨在解决当VLM在该领域应用中存在的模型能力不足、执行效率低和评估方法失真三大核心问题。VLM的引入为移动端GUI智能体带来了机遇，但将此潜力完全转化为在真实、复杂、动态的移动端环境中的可靠能力，仍面临以下三个核心挑战。

VLM 感知与决策能力受限。移动端 GUI 智能体的性能，取决于驱动其的 VLM 对复杂屏幕界面的感知理解能力与基于用户意图的决策能力。当前，VLM 在此能力上表现出显著局限，其根源在于训练数据的质量与数量双重匮乏，以及从数据中进行有效学习的范式缺失。主流 VLM 的预训练数据与移动端 GUI 场景存在显著的领域鸿沟。VLM 的多模态理解能力主要是在海量通用图文数据对上通过预训练获得的。移动端 GUI 本质上并非自然图像，而是一种作为人机交互接口而设计的高度结构化、信息密集的人造视觉表征。其屏幕空间内密集分布着文本、图标及各类功能控件，元素间的空间布局、层级结构和相互关联性共同承载着丰富的操作语义。因此，将主要基于自然图像训练的 VLM 直接应用于解析 GUI 界面，会导致模型在识别控件功能、理解界面状态等方面的精度不足。强化 VLM 需

依赖大量移动端GUI数据，但现有移动端GUI数据集普遍存在规模不足、时效性差或关键信息（如视图层级信息）缺失等问题。此外，构建大规模、高质量的移动端GUI训练数据面临严峻的工程挑战。为增强VLM对移动端GUI的感知与决策能力，理想的训练数据形式应为“屏幕状态-用户指令-动作序列”的完整轨迹。当前，依赖人类专家进行数据标注是保证数据质量的主要手段，但这需要标注员在数以万计的应用中执行海量任务，并精确记录每一个交互步骤。此过程的成本和时间开销使其难以规模化扩展。自动化数据采集方法虽能提升效率，但在处理登录验证和动态弹窗等复杂GUI状态时，常表现出鲁棒性不足的问题，限制了所采集数据的深度与多样性。最后，从大规模GUI数据中高效学习并提高感知与决策能力的训练方法尚不成熟。即使能够通过自动化手段收集到海量的无标注GUI交互轨迹，如何设计有效的自监督学习方法，以引导VLM从中学习到泛化能力强的感知与决策知识，仍是一个开放性问题。

GUI 任务顺序执行效率低。智能体执行任务的效率，特别是任务完成时延，是衡量其实用性的关键指标。为提升在复杂任务中的成功率，现有 GUI 智能体普遍在执行流程中集成了多种测试时扩展方法，如引入反思与回溯机制以修正错误。然而，现有 GUI 智能体的执行流程遵循着“感知-决策-执行”的循环。一旦智能体在某一交互步骤中做出错误决策，系统必须完成一次高成本的试错循环：首先执行该错误动作，随后通过反思模块评估任务进展，若发现偏离则启动回溯机制，将环境恢复至前一状态，最终重新进行决策。此种顺序试错模式的累积时间成本非常高昂，在多步骤的复杂任务中，数次错误修正便可导致总时延呈显著增长，这与用户使用智能体以提升效率的初衷相悖。因此，现有 GUI 智能体执行流程形成了一种任务成功率与完成时延之间的固有权衡，其高昂的时间代价在许多实时交互场景中会显著影响用户体验，阻碍技术真实部署与应用。

GUI 任务评估方法失真。科学、准确的评估基准是技术迭代与发展的必要前提。然而，当前移动端 GUI 智能体领域的评估方法存在系统性失真，其结果无法全面、真实地反映 GUI 智能体在交互式环境中的性能，从而可能对研究方向产生误导。此挑战主要源于现有评估方法的静态简化与单一路径依赖。具体来说，现有对 GUI 智能体的评估多在离线环境中展开，其核心是基于精确动作匹配的轨迹比对：该方法依赖于一个静态数据集，其中每个任务均预设了一条唯一的动作轨迹作为基准。评估时，GUI 智能体生成的动作序列必须在动作类型、参数等所有字段上与该标准轨迹完全匹配，方可被判断为成功匹配。这种基于静态数据集的评估方式忽略了移动

端GUI交互的两个根本特性——环境的动态性与任务路径的多样性。真实世界的GUI环境多变，GUI布局更新、网络延迟或弹窗都可能使实际交互环境偏离静态数据集的预设状态。此外，完成同一任务目标通常存在多种等价的有效路径。然而，在单一路径依赖的静态评估框架下，所有正确但路径不同的解决方案均会被判定为失败。此类静态评估方法的固有缺陷也导致了两个严重问题：其一，评估结果的准确性不足，特别是存在极高的假阴性率，使得GUI智能体的真实性能被低估。其二，可能对技术研究产生负向激励，即鼓励GUI智能体去精确复现任务，而非智能地完成任务，从而阻碍了对智能体规划、探索与泛化等高级能力的探索。因此，构建一套能够在动态交互环境中，以任务目标达成度为核心评判标准、并能兼容多路径解决方案的高保真自动化评估体系，是科学评估智能体性能，并推动该领域发展的当务之急。

# 1.4 研究内容及贡献

为了解决上述关键挑战，本文归结出三个核心研究问题，并相应地提出三点研究内容。图1-2展示了本研究在模型、系统与评估三个维度上要解决的关键挑战、研究问题与研究内容之间的对应关系。具体而言，本文的研究内容与主要贡献如下。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/92e187f4755b6d77eab3ed53a243ed89313871a2e67f8a6350e57c0999b69f15.jpg)



图1-2 本文研究内容示意图


# 1.4.1 大规模GUI数据驱动的VLM能力强化方法

针对移动端GUI数据不足导致VLM感知与决策能力受限的问题，本研究首先构建了大规模移动端GUI数据集。为降低传统GUI数据集依赖人工收集的高昂成本，本研究设计并实现了一套LLM增强的自动化应用遍历框架。该框架的核心在于：（1）在自动化交互逻辑中引入LLM的理解与推理能力，使其能够智能处理登录、弹窗等复杂交互场景。实证结果表明，与完全自动化遍历相比，引入LLM可将遍历阻塞概率降低约  $30\%$  。

对于LLM仍无法处理的复杂场景，该框架通过人工介入确保遍历的连续性与完整性；（2）在硬件层面，基于System-on-Chip(SoC，片上系统)阵列服务器构建大规模、高保真运行环境。该服务器由60个QualcommSnapdragon865SoC组成，每个SoC原生运行Android系统，并通过虚拟化技术在单个SoC上实例化多个Android系统，显著提升了应用部署密度。依托该软件一硬件协同设计，本研究在GooglePlayStore中超过2万个应用上完成高效遍历，采集并开源了大规模GUI数据集MobileViews，包含60万余对屏幕截图、视图层级结构与GUI交互轨迹。

本研究将MobileViews和前人研究工作中，数据丰富度匹配且规模最大的移动端GUI数据集Rico进行对比，发现MobileViews数据集中更精确的GUI结构和语义信息更有利于VLM理解移动端GUI界面的整体布局并捕获GUI元素的细节属性。进一步地，为充分利用海量无标注数据，本研究提出了一种基于自监督强化学习的VLM感知与决策能力增强范式。其核心在于构造“K步界面转移”训练任务，即令VLM预测相邻K步GUI界面间的初始操作，使其自主学习界面动态变化的因果关系及控件操作的语义。实验结果显示，经该任务强化的VLM在AndroidControl[53]测试集上展现出泛化能力，其性能相比于基础模型提升最高可达  $7.48\%$  ，相比于传统监督微调的方法，平均性能提升  $36.24\%$  。该结果证明了在无指令标注下强化VLM感知与决策能力的可行性，为摆脱对昂贵人工标注的依赖提供了一条经济高效的新路径。

# 1.4.2 面向GUI任务的高效测试时扩展方法

现有移动端GUI智能体为增强性能，采用反思与回溯、增加交互步骤等多种测试时扩展方法。然而，将此类测试时扩展方法集成于GUI智能体的顺序执行流程之内，导致了冗长的操作步数和高任务完成时延。本研究首先将现有测试时扩展方法在GUI智能体中的应用归类为模型层级和工作流层级，并在两个层级对现有测试时扩展方法分别进行实证分析和优化。

在模型层级，本研究首先对具备推理能力的 VLM 是否能提升移动端 GUI 智能体性能进行实证研究。在两组商用 VLM——Gemini 2.0 Flash[54] 和 Claude 3.7 Sonnet[55]——以及多个基准测试上的评估结果显示，具备推理能力的 VLM 在交互式评估环境 AndroidWorld[34] 中可展现出现有移动端 GUI 智能体的最佳性能，而静态评估集上由于静态评估方法的限制，无显著性能提升。该实证分析展现了静态测试集的局限，强调了测试时扩展方法的效果需在交互式环境中进行准确评估。

在工作流层级，为解决现有GUI智能体因顺序执行流程而导致的任务

完成时延过高问题，本研究提出了基于并行推测执行（Parallel Speculative Execution，PSE）的高效测试时扩展方法。该方法将传统的单路径顺序执行模式优化为多路径并行推测执行：在每个GUI状态下，智能体基于VLM生成多个可能推进任务进度的候选动作，并通过快速状态复制技术在并行环境中同步执行。为支持PSE，本研究在工作流层面提出三项优化：（1）意图感知的动作去重：利用VLM对动作背后的语义意图进行分析，过滤掉功能等价的冗余操作；（2）基于写时复制（Copy-on-Write，CoW）的移动端GUI环境快速复制：利用OverlayFS将现有执行环境作为基准状态，仅为复制的移动端GUI环境创建覆盖层，大幅度降低了执行环境复制开销；（3）基于子任务进度的分支剪枝：通过将复杂任务分解为一系列可量化的子目标，动态评估并裁剪在任务进展上显著落后的探索分支，避免计算与硬件资源的浪费。实验结果显示，PSE方法在大多数场景中不仅降低了任务完成时延，还提升了交互式评估环境AndroidWorld[34]上的整体任务完成率。该研究内容为构建高响应速度、强鲁棒性的移动端GUI智能体提供了全新的架构思路，并为其向时延敏感的真实应用场景部署奠定了实践基础。

# 1.4.3 基于关键状态匹配的自动化评估方法

针对现有静态GUI评估方法因依赖单步精确动作匹配而导致的评估结果失真问题，本研究提出了一套基于应用关键状态的高保真度的移动端GUI智能体自动化评估方法与系统。该评估方法的原理为：GUI任务执行过程中转移系统或应用状态，而非执行固定的动作序列。基于此，本研究提出基于应用关键状态的匹配方法，其以任务完成状态为导向，能够兼容功能等价的任务执行路径，解决了传统静态评估方法因单一路径依赖而导致的假阴性率高，评估结果失真的核心问题。为实现应用关键状态匹配，本研究设计了一套包含丰富原语的关键状态标注方法及对应的评估实现。标注原语涵盖精确匹配和模糊匹配两种匹配模式，允许标注者分别在屏幕层级、GUI组件层级、以及系统状态层级对任务完成状态进行标注。基于该标注原语，标注者无需录制完整的操作序列，仅需以极低的成本，通过描述性的语言定义出任务完成所必须达成的、一系列核心的应用关键状态检查点。基于该方法，本研究提出了移动端GUI智能体交互式评估系统LlamaTouch。其任务集涵盖了近500个移动端GUI自动化任务，其中一部分来自现有的静态评估集AITW[47]，另一部分来自于LlamaTouch额外标注的、贴近日常使用场景并涵盖多类应用的任务。每个应用的关键状态均为人工标注。LlamaTouch能够在真实交互式移动端环境中部署并运行GUI智能体，完整记录其包含屏幕截图与视图层级信息在内的多模态执行轨迹。

随后，系统将该轨迹与预定义关键状态进行多层次匹配，实现高保真度的自动化评估。实验表明，在多项测试中LlamaTouch的自动评估结果与人工判定的一致性接近  $80\%$  ，验证了其可靠性与实用性。作为一个高保真、可扩展的基准平台，LlamaTouch推动了移动端GUI智能体评估标准向更贴近真实世界应用场景的方向发展。

# 1.5 论文组织架构

本文围绕构建和评估VLM驱动的移动端GUI智能体所面临的三大关键问题——VLM感知与决策能力受限、任务执行效率低、评估方法失真展开深入研究。论文共包含六个章节，各章节内容与结构安排如图1-3所示，具体组织如下：

第一章为绪论。本章首先介绍了研究背景，阐明了现有移动端智能助手从API路线到GUI路线的转化，强调了VLM能力提升为移动端GUI智能体带来的新机会；随后分析了当前该领域的研究现状，并阐述了模型、系统和评估三个方面的关键挑战；最后明确了本文研究的具体内容、主要贡献以及各章节的结构安排。

第二章为国内外研究现状。本章着重分析了与本文研究内容密切相关的前沿成果，包括VLM的发展与演进、移动端智能体的发展与演进、移动端GUI数据集、面向移动端GUI的测试时扩展方法、以及现有基准测试等。通过对上述工作的归纳与分析，总结了现有方案存在的不足，进一步明确了本文需要着重解决的关键问题以及研究探索的具体方向。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/6fedba821239e19a5325ce0a157953cc43abaa80d70e62a1f2d2a901ee05991f.jpg)



图1-3 论文组织结构示意图


第三章为大规模GUI数据驱动的VLM能力强化方法。针对移动端GUI数据集收集的高开销问题，本章提出了一种基于SoC阵列服务器，由

LLM驱动的自动化移动端应用自动化遍历方法，并基于该方法收集到了大规模移动端GUI数据集MobileViews。通过对该数据集进行对比分析，并构建了自监督学习任务，阐明其相比于传统GUI数据集的优势，并有效提高VLM的感知与决策能力。

第四章为面向GUI任务的高效测试时扩展方法。针对GUI任务顺序执行效率低的问题，本章在模型和工作流两个层级探究GUI任务的测试时扩展方法。在模型层级，本章首先开展了实证研究，证明模型层级的扩展在交互式环境中性能有提升，而静态基准测试无法反映其能力。基于此观察，本研究进一步在交互式环境中，对GUI智能体的工作流进行优化，将GUI智能体顺序执行优化为并行推测执行，提高了任务响应速度与任务完成率，为其在真实世界中的高效部署提供技术基础。

第五章为基于关键状态匹配的自动化评估方法。针对静态基准测试难以准确评估移动端GUI智能体性能的问题，本章首先设计了面向应用关键状态的标注原语，并基于此提出高保真、可扩展的关键状态评估方法。在此基础上，构建了交互式、自动化的评估平台LlamaTouch，实现了对智能体执行过程的多层次状态匹配。实验结果表明，LlamaTouch在多个移动端GUI智能体测试中展现出较高的评估准确率，为智能体的开发与性能评估提供了高保真度的基准支持。

第六章为总结与展望。本章首先总结了本文的主要研究内容和创新贡献，随后探讨了移动端GUI智能体的未来发展趋势和挑战，并提出了下一步可能的研究方向。

# 1.6 课题来源

（1）国家重点研发计划项目，跨域异质分布式人工智能架构技术研究，课题编号：2021ZD0113001。

（2）CCF-阿里巴巴创新研究计划青年科学基金，面向边缘云低功耗异构处理器系统的度量、分析和优化技术研究。

# 第二章 国内外研究现状及分析

本章将介绍本文涉及的背景及国内外研究现状。第2.1节和第2.2节分别介绍视觉语言模型以及移动端智能体的发展及演进。第2.3节对现有移动端GUI数据集进行讨论分析；第2.4节阐述测试时扩展方法在移动端GUI智能体上的应用；第2.5节介绍面向移动端GUI智能体的基准测试。

# 2.1 视觉语言模型发展与演进

早期的视觉语言模型主要聚焦于图像描述任务（Image Captioning）[56-60]。谷歌于2015年发表的论文“Show and Tell: A Neural Image Caption Generator”首次将卷积神经网络与循环神经网络结合，实现了端到端的看图说话，为视觉与语言的结合奠定了重要基础[61]。随后，研究逐步扩展到更复杂的视觉问答（Visual Question Answering, VQA）领域[62-65]。Antol等人提出首个大规模VQA数据集，要求模型基于图像与自然语言问题生成答案[63]。该任务强调跨模态推理，使视觉语言模型从简单的描述生成迈向细粒度理解与多样化交互。在图像描述与问答任务的推动下，注意力机制成为突破性进展[66,67]。Xu等人发表于ICML2015的工作的引入软注意力，使生成过程能够聚焦于图像中的相关区域[68]。随后，Anderson等人提出结合目标检测与任务驱动的分层注意力机制，实现对象级语义对齐，并在COCO与VQA挑战中取得领先成绩[69]。这标志着视觉语言模型进入注重细粒度跨模态对齐的阶段。自2019年起，受BERT等预训练语言模型启发[70-73]，视觉语言模型进入跨模态预训练阶段。代表性工作如LXMERT通过大规模图文对数据和多任务预训练学习通用的跨模态表示，并在VQA等任务上取得显著性能提升[74]。同类模型还包括ViLBERT[75]、VisualBERT[76]和UNITER[77]。这一系列工作推动了视觉语言模型从面向特定任务的模型向通用基础模型的转变，为后续多模态智能的发展奠定了坚实基础。

进入2021年前后，大规模弱监督学习推动了视觉语言模型的跨越式发展。OpenAI提出的CLIP模型[78]在4亿图文对上进行对比学习，训练出共享嵌入空间的图像与文本表征，展现出显著的零样本能力，标志着视觉语言模型向开放域应用迈出重要一步。Google随后提出的ALIGN模型进一步验证了这一范式在跨任务迁移中的有效性[79]。在此基础上，研究逐渐转向与生成式模型的结合，例如BLIP实现了理解与生成任务的统一[80]，而BLIP-2通过轻量化适配器将图像编码器与大型语言模型高效衔接，显

著提升了跨模态建模效率[81]。与此同时，DeepMind提出的Flamingo模型展示了多模态少样本学习的潜力，仅依赖上下文提示即可在视觉问答、图像描述等任务中取得接近甚至超越专用模型的表现[82]。这些研究表明，在弱监督与生成式预训练的推动下，视觉语言模型正逐步具备开放环境中的灵活推理与广泛适应能力。进入2023年以后，视觉语言模型的研究重心进一步转向多模态大模型的构建。OpenAI在技术报告“GPT-4 Technical Report”中提出的GPT-4首次具备了多模态输入能力，能够接受文本和图像作为输入并生成高质量文本输出[83]。GPT-4不仅在传统语言任务上表现出色，在图像理解、复杂推理以及跨模态对话中也展现出接近人类水平的表现。几乎同期，Google DeepMind推出Gemini系列模型，该模型自设计之初便整合了文本、图像、代码、音频与视频等多模态信息，实现了更广泛的任务覆盖[22]。Gemini展示了在跨模态推理和创作上的新能力，例如基于图像生成程序或表格，或融合多模态内容生成复合答案。Anthropic推出的Claude3系列同样在多模态交互上展现出强大性能[23]。

这些多模态大模型的出现，推动视觉语言模型从单一任务工具转向通用人工智能应用基座，并在智能体研究中迅速落地。例如，基于视觉语言模型的桌面或移动GUI 智能体能够通过看屏幕和理解用户指令完成复杂操作，代表性应用包括微软提出的UFO[84]、Anthropic的Claude Computer Use[85]，以及OpenAI发布的Computer-Using Agent与Operator系统[86,87]。这些应用表明，视觉语言模型已经从感知理解任务扩展到实际的人机交互与自动化执行层面，逐步成为支撑智能体与下一代交互系统的核心。

# 2.2 移动端智能体的发展与演进

移动端智能体极大地简化了用户在智能手机上的繁琐操作，其发展大致经历了三个阶段。最早出现的是基于API的智能体，如Google Assistant[4]、Apple Siri[1]、小米公司的小爱同学[2]以及华为小艺[3]等。它们通过预定义的应用程序编程接口与应用进行交互。这类方法在执行结构化、可预测的任务时表现稳定可靠，但一旦用户意图难以准确理解，或应用未提供对应的功能接口，便无法完成用户指令。随着研究的推进，学界和工业界开始探索基于学习的智能体[40,46,47,88,89]。这类方法依托深度学习技术从历史交互轨迹中学习如何操作智能手机。然而，它们的能力仍受限于训练数据，泛化性不足。进入大模型时代后，LLM与VLM的出现推动了智能体能力的飞跃[90-92]。早期的LLM智能体依旧以调用API为核心，借助定义明确的程序化接口来执行任务[93-95]。这种模式能够高效调度微服务、查询搜索

引擎或控制第三方应用，凭借稳健的自动化能力和良好的可扩展性，迅速从研究原型演进为主流工业解决方案，如微软的Copilot on Windows[96]。几乎在同一时期，VLM的进展催生了一类新型的基于GUI的智能体[12,13,97]。与依赖后端接口的API智能体不同，GUI智能体通过观察并操作用户界面完成任务，以更接近人类的方式与软件互动。它们整合了视觉感知与文本推理能力，从而拓展了人机交互的边界，实现了对软件更通用和灵活的自动化控制。

图2-1展示了移动端GUI智能体的现状。在离线阶段，现有VLM通常需要依赖移动端GUI数据集来构造各类下游任务，以强化其能力。例如，最基础的定位任务（Grounding）旨在将指令描述映射到GUI上对应的操作组件[98-100]；屏幕问答（ScreenQA）[101.102]和GUI导航（GUINavigation）[47,88]则更贴近用户的日常需求，前者通过解析GUI上下文来回答用户问题，后者则直接替代用户执行操作，这两类任务均要求模型能够准确感知任务指令与界面上下文并作出合理决策。上述任务多为有监督学习，需要人工或大语言模型生成标注。另一类任务则通过外部知识来辅助VLM决策，例如基于应用遍历获得的GUI组件与转换关系[92,103,104]，或是基于历史操作进行知识积累[105]。本文第三章进一步探讨了如何利用大规模无标签的GUI转换关系作为训练数据，为未来更大规模的移动端GUI数据集构建与训练提供基础。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/6770af6a9e7e414f3f53d1e8a70473ecafccecc9da47c028bc1fe5913cbd392b.jpg)



图2-1移动端GUI智能体现状与关键组件


在任务执行阶段，移动端GUI智能体通常由三个关键组成部分构成：输入、控制器与输出。在输入层面，任务指令、GUI表征和工作内存共同作为核心输入来源。任务指令多为基于自然语言的用户需求，例如“提醒我明晚7:30参加会议”。GUI表征则包括屏幕截图与视图层级信息：前者提供当前界面的视觉图像，后者以树状结构描述界面元素的类型、位置与

文本内容。在此基础上，屏幕截图和视图层级信息结合可进一步增强GUI语义。例如，Yan等人[25]使用OCR工具检测文本和图标，并叠加数字标签；AXNav[106]将截图转化为带有边界框与标签的结构化形式，便于VLM理解；微软提出的Set-of-Mark提示词设计[26]则基于VH信息在屏幕截图上对GUI组件进行标注，允许GUI智能体输出目标组件的编号而非具体坐标信息，从而降低了定位错误发生的概率。对VH的再处理（如转化为简化的HTML结构）也被广泛应用，以提升模型对界面结构的理解效率与推理速度[91,92]。此外，工作内存则用来增强GUI智能体对当前环境上下文的感知，其中，最典型的工作内存是GUI智能体的历史操作轨迹。例如：Auto-UI[40]和M3A[34]等移动端GUI智能体将历史动作嵌入VLM的提示词，以提高智能体对行为连贯性的感知；AITZ[38]进一步将历史动作执行结果添加到工作内存中，以支持更准确的动作生成。

在决策层面，控制器作为GUI智能体的大脑，负责解析任务指令与工作内存，从移动端环境中提取GUI表征并输出动作决策。控制器的实现方式多样，既包括针对特定应用设计的深度学习模型[40,46,47,88]，也包括通用大模型，LLM（GPT-4、Llama）[91,92,105]或VLM（如GPT-4o、Google Gemini等)[25,90,107]。现有研究工作中，GUI智能体的工作流可分为如下几类：（1）直接输出当前GUI上要执行的动作；（2）通过工具调用机制与Android系统或应用暴露的意图接口交互[29,30]；（3）使用反思与回溯机制，即允许GUI智能体自我改正错误决策，并返回至正确执行路径[32,33,108]；（4）多智能体协作，例如：微软公司提出的UFO²允许系统级智能体调用各应用内的子智能体以完成操作[35]；CoAct-1将GUI智能体和API智能体统一在同一工作流下协同完成用户指令[37]；（5）近年来针对测试时扩展方法的探索也不断涌现，其细节将在第2.4节展开讨论。

在输出层面，控制器将决策转化为具体操作，如点击、滑动或文本输入。动作的抽象层级因设计而异，例如基于屏幕具体坐标的直接操作[40.47.88.90]或是基于图标标记的目标指令[25.34]。现有研究还通过HTML结构来精确匹配屏幕上的元素（文本或图标）[40.91.105]。在交互式测试平台上，这些输出会直接作用于移动端环境（通常是真实设备或模拟器），引发界面状态转化。随后，智能体会捕获新的GUI状态作为输入，进入下一轮动作决策。该循环将持续进行，直至任务完成或达到预设的最大步数上限。

# 2.3 移动端GUI数据集

本节对现有的移动端GUI数据集进行对比，评估的维度包括数据规模

与数据类型，随后探讨了各GUI数据集的收集方法与所用硬件设备。比较结果汇总于表2-1中。


表 2-1 开源移动端 GUI 数据集对比


<table><tr><td rowspan="2">GUI数据集</td><td rowspan="2">年份</td><td colspan="2">规模</td><td colspan="4">数据类型</td></tr><tr><td>应用</td><td>GUI</td><td>元数据</td><td>屏幕截图</td><td>视图层级</td><td>GUI轨迹</td></tr><tr><td>Rico[45]</td><td>2017</td><td>9772</td><td>63370</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr><tr><td>PixelHelp[46]</td><td>2020</td><td>4</td><td>187</td><td>×</td><td>✓</td><td>✓</td><td>✓</td></tr><tr><td>Screen2Words[109]</td><td>2021</td><td>6269</td><td>22417</td><td>✓</td><td>✓</td><td>✓</td><td>×</td></tr><tr><td>ScreenQA[101]</td><td>2022</td><td>/</td><td>35352</td><td>✓</td><td>✓</td><td>✓</td><td>×</td></tr><tr><td>META-GUI[88]</td><td>2022</td><td>11</td><td>24825</td><td>×</td><td>✓</td><td>✓</td><td>✓</td></tr><tr><td>DroidTask[92]</td><td>2023</td><td>13</td><td>362</td><td>×</td><td>✓</td><td>✓</td><td>✓</td></tr><tr><td>AITW[47]</td><td>2023</td><td>357</td><td>2282533</td><td>×</td><td>✓</td><td>×</td><td>✓</td></tr><tr><td>MobileViews</td><td>2024</td><td>21053</td><td>612471</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr></table>

早期的移动端GUI数据集之一是2017年提出的  $\mathbf{Rico}^{[45]}$  。该数据集从超过9000个移动端应用中采集GUI数据，总计包含超过7.2万个移动端GUI，其中不同GUI数量超过6.3万。Rico的主要优势在于其数据的全面性——它包含来自应用商店的应用元数据、屏幕截图与视图层级信息对，以及GUI交互轨迹。由于Rico拥有较大的规模和丰富的内容，许多后续的数据集都是在此基础上进行扩展。例如，Screen2Words[109]从Rico中选取了一部分具有代表性的GUI，并由人工注释其屏幕摘要；ScreenQA[101]采用了类似的方式，但其关注点是基于屏幕内容的问答任务。移动端GUI任务自动化推动了另一类移动屏幕数据集的发展，用于训练和评估相关模型[34,46,47,88,110]，其中，Google公司开源了诸如PixelHelp[46]，AITW[47]等多个可用于评估GUI自动化任务的数据集。然而，这些数据集大多来源于有限数量的应用，且GUI数量较少，因此难以满足训练移动端GUI智能体的需求。此类数据集的规模受限于其数据采集方式——它们大多由人工操控真实智能手机或移动端模拟器进行采集，效率较低。

移动端GUI数据集的数据完整性是另一个层级的关键因素，例如是否包含屏幕截图、视图层级信息以及GUI交互轨迹。在面向单个移动端屏幕的任务中，两类GUI表征方式——像素级的屏幕截图和文本形式的视图层级信息——对于理解移动端界面都至关重要。屏幕截图能够准确捕捉屏幕的视觉状态，提供界面布局与基于图像的GUI组件的直观参考；而视图层级信息则揭示了GUI组件之间的结构与关系，有助于识别隐藏的GUI组

件及其属性。两者结合，可以从视觉和结构两个维度全面理解移动端屏幕。尽管当前最大的移动屏幕数据集 AITW 包含超过 500 万个屏幕（其中不同 GUI 超过 220 万），覆盖 357 个以上的 Android 应用，但它不包含视图层级信息，这使得移动端 GUI 智能体难以学习屏幕截图与结构化 GUI 组件之间的映射关系。本研究旨在构建大规模开源移动端 GUI 数据集，并确保其中包含屏幕截图、视图层级信息以及如 GUI 交互轨迹等其他关键数据，以支持未来相关研究的发展。表 2-1 的最后一行展示了本工作收集的 MobileViews 数据集。其涵盖了超过 2 万个来自 Google Play Store 的日常应用，并包含超过 60 万张各不相同的 GUI 表征，为强化驱动移动端 GUI 智能体的 VLM 提供了坚实的数据基础。此外，基于 SoC 阵列服务器所提供的高保真度移动端应用遍历环境，以及使用 LLM 强化的自动化遍历流程，大大减少了构建 GUI 数据集的人工开销，为未来扩展到更大规模的 GUI 数据集提供了一套硬件-软件协同解决方案。

# 2.4 面向GUI智能体的测试时扩展方法

测试时扩展方法允许LLM与VLM在推理阶段分配更多的计算资源，在代码生成、数学以及推理等多种任务上展现出大幅度性能提升[20,111,112]。移动端GUI智能体作为基于VLM的典型应用，近期研究也探讨了在推理时分配更多计算资源，以提高基于VLM的GUI智能体的性能。这些方法大致可以分为模型层级和工作流层级的优化。

模型级测试时扩展。使用具有推理能力的 VLM 驱动移动端 GUI 智能体是应用测试时扩展最简单直接的方法。商用 VLM，例如 ChatGPT-o3, Gemini 2.0/2.5 Flash 以及 Claude 3.7 Sonnet 等通用模型，已经集成了推理能力，并在多模态任务上展现了优异的性能。相关研究已验证了推理能力在商用的 VLM（如 Gemini 和 Claude 系列）以及小型自建模型的有效性[32,39,113]。例如，本文在第四章的实证研究比较了推理型与非推理型 VLM 在移动端 GUI 智能体上的表现。在此基础上，研究者还训练了面向 GUI 任务、具备内在推理能力的专用模型[32,90,113-115]。这些高性能的推理型 VLM 可以与工作流层面的优化方法结合使用，从而进一步增强 GUI 智能体的整体表现。

工作流级测试时扩展。工作流级测试时扩展是与模型级测试时扩展方法相互独立的另一类研究，两类方式可互相补充，共同提升移动端GUI智能体性能。移动端GUI智能体的工作流通常由环境感知、动作生成和动作执行的多轮循环组成，并受到预设最大步数的限制。在推理阶段，已有多

种工作流层级的测试时扩展方法被提出。一类典型的研究是利用提示词工程，例如链式思维提示，促使VLM在生成动作时进行更细致的推理[38,40]。通过在提示中加入先前步骤的工作记忆（如历史动作），智能体能够更好地追踪任务进度，并在当前步骤作出更合理的判断。另一类方式是允许VLM针对同一屏幕生成多个候选动作，并通过筛选过程选出最优解。常见筛选策略包括多数投票[41]、基于世界模型的推演[41-43,116]以及基于奖励的最佳优先搜索[41-44,116-118]。例如，在世界模型推演中，智能体将当前状态与其中候选动作作为输入，预测执行该动作之后的GUI表征。通过在模拟环境中（通常是VLM）尝试多条假设动作序列，智能体根据观察到的结果选择最优动作。自我改进机制（Self-refinement）这一在语言模型优化中已被广泛应用的机制[119-121]也被引入移动端GUI智能体的工作流程中[32,33,36,108,122]。常见的实现方式是通过评估模块帮助智能体识别并纠正错误，随后可回溯到历史状态，或是修改未来的执行计划。提高交互预算（即任务允许的最大步数）同样能够带来显著性能提升[86,113]。与固定步数预算不同，TTI方法[123]提出的自适应策略可以动态扩展交互预算，从而为智能体提供更深入的环境感知能力，帮助其做出更优的决策。

本文所提出的基于并行推测执行的移动端GUI智能体是面向工作流层面的优化。与现有方法相似，其会在给定屏幕上生成多个候选动作。区别在于，该智能体并不依赖可能不准确的移动端GUI世界模型，而是直接与真实移动端环境交互，以观察候选动作的实际执行结果。不同于通过反思或回溯进行顺序探索的方法，基于并行推测执行的移动端GUI智能体能够同时保留多个执行分支，且并行运行多条推测性动作序列。这种并行机制依托于系统层面的实现支持，使智能体能够并行探索多种可能的未来，从而显著缩短任务完成时延。为了快速识别出更优分支，并行推测执行引入了基于子目标进度索引的剪枝机制，有效避免了以往依赖VLM进行评分和排序所带来的巨大计算开销。需要强调的是，本文提出的方法可以与现有的VLM层面或工作流层面的优化手段（如扩展交互预算、增加反思阶段、或是基于世界模型进行推演）相结合，发挥互补优势。

# 2.5 面向GUI智能体的基准测试

科学、精确的基准测试驱动特定领域的发展。如表2-2所示，已有多种数据集与评估环境用于评估移动端GUI智能体，但它们未能同时兼顾评估的高保真性与高可扩展性。本文提出的LlamaTouch系统通过关键状态匹配方法，实现了高保真度与自动化的移动端GUI智能体评估。


表 2-2 移动端 GUI 智能体基准测试对比


<table><tr><td>基准测试</td><td>硬件平台</td><td>真实任务</td><td>真实环境执行任务</td><td>GUI组件标注</td><td>关键状态匹配</td></tr><tr><td>Rico[124]</td><td></td><td>✓</td><td>×</td><td>×</td><td>×</td></tr><tr><td>PixelHelp[46]</td><td></td><td>✓</td><td>×</td><td>×</td><td>×</td></tr><tr><td>AndroidEnv[48]</td><td></td><td>×</td><td>✓</td><td>×</td><td>×</td></tr><tr><td>META-GUI[88]</td><td>移动端</td><td>✓</td><td>×</td><td>×</td><td>×</td></tr><tr><td>MoTIF[125]</td><td></td><td>✓</td><td>×</td><td>×</td><td>×</td></tr><tr><td>AITW[47]</td><td></td><td>✓</td><td>×</td><td>×</td><td>×</td></tr><tr><td>Mobile-Env[49]</td><td></td><td>✓</td><td>✓</td><td>×</td><td>×</td></tr><tr><td>AndroidArena[50]</td><td></td><td>✓</td><td>✓</td><td>×</td><td>×</td></tr><tr><td>WebArena[51]</td><td>网页端</td><td>✓</td><td>✓</td><td>×</td><td>✓</td></tr><tr><td>LlamaTouch</td><td>移动端</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr></table>

部分研究如  $\mathrm{Rico}^{[124]}$  、PixelHelp[46]和AITW[47]提供了包含任务描述、GUI表征及动作序列的静态数据集。移动端GUI智能体基于静态GUI表征预测具体动作，后与数据集中的真实动作进行比对。这种方法虽然直观，但存在两方面不足，因而难以全面反映移动端智能体的性能。首先，精确动作匹配无法准确反映GUI智能体性能。功能上正确的动作可能因动作参数不同而被判定为错误。图2-2与图2-3展示了两种典型情况：其一，对于点击操作，数据集定义的可点击区域受限，而现实中该区域往往覆盖整个GUI组件（红色边框标出）；其二，在大多数搜索任务中，输入的文本即便不一致，也可能得到相同的正确搜索结果（绿色边框标出）。该问题在以往文献中已被发现，但仍未被解决[25,47]。其次，精确动作匹配缺乏对不同执行路径的容忍度。如图2-3所示，在真实环境中，受设备及应用状态差异的影响，完成同一任务通常存在多种执行路径。然而，预定义静态数据集往往仅提供一条确定路径作为参考，从而导致评估结果不准确。

AndroidEnv[48]和Mobile-Env[49]等研究支持移动端GUI智能体在真实环境中执行任务。然而，这些方法在端到端任务执行过程中并未天然支持关键状态匹配，因此在评估准确率上存在不足。具体来说，AndroidArena[50]的研究者发现静态数据集上的精确动作匹配方式存在缺陷：它无法充分容忍任务执行路径中的冗余动作。为此，他们提出了基于最长公共子序列的动作匹配方法，即只要执行序列中包含真实标签动作序列作为子序列，就判定任务完成。本文第五章的研究工作中，也将AndroidArena作为基线方法，与本研究提出的LlamaTouch评估方法的准确率进行对比。WebArena[51]

则为网页智能体提供了真实的实验环境，并采用关键状态来评估任务完成情况（如最终结果应为或应包含某些关键信息）。LlamaTouch与WebArena的差异在于移动平台上的关键状态标注与评估流程：（1）LlamaTouch将同一屏幕的屏幕截图与文本化VH相结合，实现对GUI组件的细粒度、精准识别；（2）LlamaTouch使用更丰富的标注原语，对关键GUI状态进行全面标注，并在屏幕内容动态变化较大的情况下仍能进行高保真的评估。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/cbd1bd2b6efd7f92ae3bdc3c363929f625d9d88637da6b1d3c8ebf13010f4ec4.jpg)


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/4a2586dc330cbe584898102e6495860867ffb60c30ce05a7452ab41297cd5196.jpg)



图2-2 精确动作匹配中假阴性评估示例


任务执行路径#1：Excel应用已安装情况

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/9765cf29549fda62b58f85e6e4574874fe4ebe50a2c60a55017039069e26df0c.jpg)



图2-3 不同任务执行路径相同执行结果示例


此外，人工验证通常被用来判断GUI自动化任务是否完成[34.47.106]，其

优势在于能够通过人工观察者对任务执行的细节与结果进行精确判别，从而避免因界面动态变化或任务语义模糊而导致的误判。然而，这种方式在实践中成本高昂，需要大量人力投入，且难以满足多任务、多智能体以及跨设备评估场景的需求。例如，AXNav[106]和AITW[47]等基准测试均引入了人工对实验结果进行部分核验，以确保任务执行的正确性。这类方法虽然保证了结果的高可信度，但在评估规模扩大或实验环境复杂化时，显著受限于其可扩展性。针对这一矛盾，近期的研究逐渐转向如何在保证评估可靠性的同时提升自动化与扩展性。LlamaTouch的提出正是在这一背景下展开：它旨在摆脱对人工验证的过度依赖，在保持与静态数据集评估相同高扩展性的前提下，达成人工验证近似的高保真性。通过结合交互式环境中的关键状态匹配设计与多层次的屏幕/GUI组件匹配方法，LlamaTouch为移动端GUI智能体的评估建立了新的基准，既解决了以往人工验证难以大规模推广的挑战，也为后续研究提供了统一、可复现且低成本的实验平台。

# 2.6 本章小结

本章系统梳理了国内外在视觉语言模型、移动端智能体、GUI数据集构建、测试时扩展方法及评估基准体系等方面的研究进展。总体而言，尽管相关领域已在多模态输入理解、GUI解析与交互式控制方面取得重要成果，但现有研究仍存在以下三方面的关键不足：其一，在模型层面，当前视觉语言模型的预训练数据主要来源于自然图像场景，与移动端GUI的结构化语义存在显著域差异，受限于高质量、大规模GUI数据集的匮乏及高昂的人工标注成本，模型在复杂界面结构解析与跨任务决策泛化方面的能力仍然有限；其二，在系统层面，现有移动端GUI智能体普遍采用顺序执行范式，并通过嵌入反思、回溯、多候选生成等测试时扩展模块提升任务成功率，但该串行推理流程导致任务完成时延显著增加，难以兼顾高效性与实时性；其三，在评估层面，主流评估体系多依赖静态数据集的精确动作匹配方法，未能充分反映交互式环境中多路径、多状态的任务完成特征，评估结果存在高假阴性率与低保真度问题，且缺乏可扩展的自动化评估机制。针对上述局限，本文后续章节将从模型、系统与评估三个维度展开研究，分别提出大规模GUI数据驱动的VLM能力强化方法以增强模型感知与决策能力，设计面向GUI任务的高效测试时扩展方法以优化任务执行效率，并基于关键状态匹配的自动化评估方法实现高保真度与高可扩展性的智能体性能评估，从而为移动端GUI智能体的系统化构建与科学评估提供理论与技术支撑。

# 第三章 大规模GUI数据驱动的VLM能力强化方法

# 3.1 本章概述

移动端屏幕助手能够为智能手机用户概括屏幕内容并执行用户指令[101,102,126-128]。在用户存在身体障碍，或处于无法专注于屏幕的情境（如驾驶过程）时，这类功能显得尤为关键。屏幕助手通常以对话形式接收用户指令，并以屏幕截图或文本化的视图层级信息作为屏幕状态输入，随后生成对应的输出，例如屏幕概要，或基于屏幕内容直接提取答案。视图层级信息以树状结构记录界面元素的类型、位置及文本属性，能够提供结构化的语义描述；而屏幕截图则直观反映了界面的视觉呈现。二者结合能够更全面地刻画屏幕状态。图3-1左侧展示了屏幕截图与视图层级的一一对应关系，右侧则给出了移动端屏幕助手的一项典型任务示例一一屏幕问答（ScreenQA）[101,102]。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/a73c5f194528dfb0ccadf4a3ac498b0934340522e5bdbee759cf8e0f4312a7be.jpg)



图3-1本章研究内容示意图及屏幕问答任务示例


近年来，VLM[21,129,130]在处理移动端GUI任务时展现出卓越的性能。然而，现有能力强大的VLM大多数部署在云端，其往往需要用户将屏幕内容上传至服务器进行解析，这可能导致聊天记录、邮件、密码等敏感信息的泄露。隐私风险使得不少用户对这些功能强大的云端模型心存顾虑[131,132]。为应对这一问题，Google[130]和Apple[133]等公司已开始将VLM驱动的屏幕助手由云端部署转向端侧部署，这也成为近年来学术界和工业界热门的研究方向[126,127]。然而，与云端模型相比，端侧小模型在处理移动端GUI任务时受到上下文长度有限、参数量较少等限制，因而在精准信息提取与屏幕语义理解方面能力不足。若要显著提升VLM性能，须利用大

规模且高质量的移动端GUI数据集对VLM进行训练。然而，现有视觉语言模型的训练数据大多来自通用的图文对[78,80]，移动端GUI数据所占比例很小，而移动端GUI与自然图像在表现模式上存在明显差异（例如密集的GUI组件，它们之间的结构与关联性，以及大量的文本信息）。因此，构建一个大规模的移动端GUI数据集用于强化驱动移动端GUI智能体的VLM已成为当务之急。

现有的开源移动端屏幕数据集在规模和全面性方面都存在不足。第2.3节的表2-1显示，2017年收集的  $\mathbf{Rico}^{[124]}$  数据集拥有最多的应用数量，但仅包含约7.2万张屏幕截图（其中去重之后的屏幕截图数量约为6.3万张）；近来由Google公司开源的AITW数据集虽然拥有500万张移动端屏幕数据，却缺少能够精确描述屏幕文本与结构信息的视图层级信息，使得基于其训练的模型难以仅凭截图准确理解内容。PixelHelp[91]与META-GUI[88]的数据规模比AITW小了几个数量级。除此之外，还有部分数据集专注于移动GUI任务自动化[34,50,92,110]，或为屏幕摘要、屏幕问答等特定任务设计[27,109,124]。总体而言，这些数据集规模有限，覆盖的移动应用范围较窄，难以支持端侧VLM的有效训练。


表 3-1 开源 GUI 数据集收集方式与硬件架构


<table><tr><td rowspan="2">GUI数据集</td><td rowspan="2">年份</td><td colspan="2">数据收集方式</td></tr><tr><td>方法</td><td>硬件</td></tr><tr><td>Rico[45]</td><td>2017</td><td></td><td>智能手机集群</td></tr><tr><td>PixelHelp[46]</td><td>2020</td><td></td><td>移动端模拟器</td></tr><tr><td>Screen2Words[109]</td><td>2021</td><td></td><td>/</td></tr><tr><td>ScreenQA[101]</td><td>2022</td><td>人工标注</td><td>/</td></tr><tr><td>META-GUI[88]</td><td>2022</td><td></td><td>智能手机</td></tr><tr><td>DroidTask[92]</td><td>2023</td><td></td><td>移动端模拟器</td></tr><tr><td>AITW[47]</td><td>2023</td><td></td><td>移动端模拟器</td></tr><tr><td>MobileViews</td><td>2024</td><td>LLM驱动的自动遍历</td><td>SoC阵列服务器</td></tr></table>

收集大规模移动端GUI数据集的挑战与关键技术。若要大规模收集移动端GUI数据集，需要同时解决软件和硬件层面的核心难题。在软件层面，如何实现面向海量移动端应用的高效可扩展的应用程序遍历是关键。然而，现有方法存在诸多不足：（1）现有的自动化遍历工具[134,135]无法灵活应对多样化的应用界面和复杂的导航流程，导致应用常常在登录、注册等环节陷入停滞，导致数据质量与数量低。（2）依赖众包标注员人工采集

数据[47,124]虽能保证准确性，但该方法经济与时间开销极高，难以大规模扩展。表3-1汇总了标志性的开源移动端GUI数据集的采集策略，其列出数据集均基于人工标注者进行收集。为了保证GUI数据集数量与质量的同时，降低应用遍历开销，本研究基于DroidBot[135]构建了基于LLM的自动化应用遍历工具。它通过自动化遍历方法的静态交互规则进行GUI常规交互，并适时引入LLM处理复杂GUI状态。具体而言，该工具先获取并简化屏幕的视图层级信息，以锁定可交互的GUI组件，再按顺序与可交互组件交互。当遇到复杂登录界面或静态交互规则无法解决的停滞等预设触发条件时，LLM会介入，用于理解当前GUI状态并执行绕过操作。在对GooglePlayStore的80余款热门应用的实验中，该方法将遍历成功率从  $18.6\%$  提升至  $44.2\%$  。最后，该工具在账号注册、验证码输入等敏感环节引入人工介入，以保证非阻塞的应用遍历。

硬件层面挑战在于要部署大量高保真度的移动端环境以并行运行大规模移动端应用。现有移动端屏幕数据集大多数基于Android模拟器（Android Emulator）采集[47,48,88,110]，虽然在传统服务器上易于部署，但受限于模拟器（多运行于x86架构的服务器之上）与移动应用（为ARM架构设计）之间的兼容性问题，能运行的应用类型有限。例如，AITW的500万屏幕数据仅收集自357个可在Android模拟器运行的应用，导致数据集中出现大量相似屏幕。相比之下，Rico基于物理设备集群（智能手机）运行应用，收集了来自9000余个应用的屏幕数据。然而，物理手机集群不可避免地面临分辨率、系统版本等设备异构性问题，增加了遍历工具的设计与维护复杂度；同时，其在扩展性上也受到空间与成本的限制。为应对这些问题，本研究提出了一种基于SoC阵列服务器的可扩展、高保真度应用遍历方法[136]。具体而言，该方法采用了2台SoC阵列服务器，每台由60个QualcommSnapdragon865SoC组成，构建高密度移动端运行环境，并在其上运行大规模移动端应用。这些SoC提供与真实智能手机一致的原生Android环境，并支持Android系统虚拟化。最终，通过在2台SoC阵列服务器的120个SoC上部署超过200个Android实例，以组建高密度且高保真的移动端应用运行环境。

基于所提出的软硬件协同自动化移动端应用遍历系统，本研究进一步构建了MobileViews——首个覆盖两万余款现代移动应用，并同时包含超过60万对截图与视图层级数据的开源数据集。与  $\mathbf{Rico}^{[124]}$  相比，MobileViews的应用覆盖数量提升约两倍，屏幕规模则扩大至九倍。与依赖人工收集的Rico数据集不同，MobileViews在时间与成本上更具优势。

如图3-1所示，MobileViews为每个应用同时提供截图与VH信息，使移动端可部署的小参数量VLM能够更有效地提取屏幕中的关键信息，从而提升移动端GUI智能体的性能。对MobileViews与Rico的对比分析发现近七年来GUI组件的标签质量显著提高，这表明在更新、更大规模的移动端GUI数据集上训练的模型有望获得更优性能。

为验证MobileViews数据集的优势，本研究在屏幕问答、可点击性预测、元素关系预测和GUI组件识别等任务上进行了实验。首先，通过无监督的VH生成任务，VLM学习了屏幕截图与VH结构之间的映射关系，并在四类下游任务上进行微调评估。结果显示，在相同数据规模下，基于MobileViews训练的模型在可点击性预测和元素关系预测任务上优于使用Rico数据集的模型；当数据规模扩展至500K时，屏幕问答任务性能显著提升，充分体现了大规模高质量GUI数据的重要性。进一步地，本研究提出了基于自监督强化学习的K步界面转移任务，其利用MobileViews中的无标注GUI轨迹，结合强化学习以增强VLM的感知与决策能力。实验结果表明，传统SFT会导致性能明显下降，而基于GRPO的强化学习不仅避免了这一问题，还在复杂的指令级GUI自动化任务中带来了最高  $7.48\%$  的准确率提升。同时，实验也揭示了数据规模与K值在不同任务粒度下的互补作用，为未来在更大规模无标签GUI数据上强化VLM提供了有益指引。

本章的主要贡献如下：

（1）本章提出并实现了一种自动化且高并行度的移动端应用遍历方案，在低开销、少人工干预的条件下能够高效收集大规模移动端GUI数据。

（2）基于该方案，本章构建了大规模开源移动端GUI数据集MobileViews，其覆盖超过2万个来自GooglePlayStore的应用，包含60万余对截图与VH数据，其规模较此前最大数据集提升近10倍。

（3）本章系统评估了MobileViews相较现有最大移动端GUI数据集的优势。实验结果表明，基于MobileViews训练的GUI智能体在多项基础定位与判别任务，以及复杂的屏幕问答任务中均展现出显著性能提升。

(4)本章提出了自监督的K步界面转移任务以充分利用MobileViews数据集中无标注的GUI状态转换数据，配合强化学习方法增强现有VLM的感知与决策能力。

# 3.2 基于SoC阵列的GUI数据收集方法

针对现有移动端GUI数据集在数据规模、应用覆盖度及屏幕信息完整性上的不足，本研究构建了MobileViews数据集。它涵盖了大量来自现代

移动端应用的屏幕截图-视图层级数据对，旨在帮助移动端GUI智能体提升对屏幕的感知与理解能力。构建这一数据集的核心难题是如何在尽量减少人工参与的情况下，高效地获取大规模且有代表性的屏幕数据。与依赖众包标注的方式不同，本研究设计了一套LLM增强的自动化移动端应用遍历系统，其能够完成绝大部分应用遍历与屏幕数据采集工作，仅在预设触发条件下才需要人工干预。该系统部署于两个SoC阵列服务器中，实现了高保真度且高扩展性的移动端应用遍历。本节将从软件和硬件两个层面详细介绍该移动端应用遍历系统。

# 3.2.1 总体流程

图3-2展示了MobileViews数据集的构建流程，主要包含三个环节：（1）应用元数据收集：本研究首先从Google PlayStore获取超过2万个Android应用的名称、包名等元数据，覆盖33个类别。与近七年前采集的旧数据集  $\mathbf{Rico}^{[124]}$  相比，其包括了大量热门应用，这些现代应用的屏幕更能反映当前GUI设计趋势。（2）应用遍历调度：元数据传入应用交互调度器，其实时监控SoC阵列服务器中的Android实例状态，当有空闲Android实例时，将一个应用遍历任务下发并绑定至对应Android实例。当应用遍历完成或意外中断时，该协调器会记录相关状态信息，并负责收集遍历结果。（3）LLM增强的应用自动遍历：从安装应用开始，启动并遍历其屏幕，优先通过规则探索可交互GUI组件。当规则无法处理的复杂GUI（如登录界面）时，调用LLM进行处理；仅在检测到长时间无响应等异常时才需人工介入。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/81d22d2fc564582f736a40ab7b6e375c6a75cdf9286cd92bd1401c29ede39526.jpg)



图3-2MobileViews数据集构建流程与硬件平台


接下来的内容首先介绍LLM增强的自动化应用遍历工具在单个应用和Android实例中的工作机制，随后将其扩展至SoC阵列服务器，以实现规模化和高度并行的应用交互。

# 3.2.2 LLM驱动的单应用渐进式遍历方法

应用交互流程开始后，首先进行应用安装和启动。应用交互调度器会将应用的包名（如com.spotify.music）发送至遍历进程，后者通过GUI自

动化脚本[137]在Google Play Store中搜索并安装目标应用，并利用adb命令（Android Debug Bridge，adb）[138]启动应用。传统的移动端应用遍历方法需要大量人工标注者操作并收集具有代表性的GUI界面，但其难以扩展到海量应用，不适用于大规模应用遍历。为此，本研究提出了一种渐进式应用遍历流程：首先基于静态交互规则自动遍历，在遇到复杂GUI状态时引入LLM负责理解屏幕状态与决策，在无法自动解决的情况下由人工处理。表3-2展示了在86个Google Play Store中热门的应用上，该设计的遍历成功率（即不阻塞在特定屏幕状态的概率）。结果显示，本节设计的自动化应用遍历方法降低了  $25.6\%$  应用阻塞概率；引入人工干预方法后，在86个热门应用上遍历成功率可达  $100\%$  。该方法分为基于固定交互规则的自动应用遍历、LLM增强的应用遍历与人工干预三个步骤。


表 3-2 自动化移动端应用遍历方法成功率


<table><tr><td rowspan="2">指标</td><td rowspan="2">数值</td><td colspan="3">应用遍历设计</td></tr><tr><td>完全自动化</td><td>+ LLM 增强</td><td>+ 人工干预</td></tr><tr><td>应用数量</td><td>86</td><td>16</td><td>38</td><td>86</td></tr><tr><td>遍历成功率</td><td>/</td><td>18.6%</td><td>44.2%</td><td>100%</td></tr></table>

基于固定交互规则的自动应用遍历。本研究首先提出的方法是，通过直接感知移动端屏幕并对其中的可交互GUI组件执行操作，从而实现完全自动化的应用遍历。具体而言，该方法使用自动遍历工具Droidbot[135]获取屏幕的VH信息并进行简化处理，从而识别可交互GUI组件；随后在每个屏幕上依次向这些组件发送相应的操作指令。图3-3展示了遍历过程中某个具体屏幕的示例及其输入/输出。每当程序发现新的屏幕状态时，该流程会重复相同的简化与操作步骤，直至达到最大操作次数（本研究中为1000次）。尽管完全自动化遍历方法无需人工参与，但在应对复杂应用状态（如注册登录界面或广告弹窗）时仍存在瓶颈。通过对GooglePlayStore的80余款应用进行分析后发现，该自动遍历工具仅能在  $18.6\%$  的应用（86个中的16个）中正常运行，其余多数因登录或注册界面而受阻。这类界面往往需要比固定遍历策略更复杂的逻辑处理，因此该方法在流程中引入LLM，以应对此类复杂的屏幕状态。

LLM增强的应用遍历。本研究引入LLM来补充自动遍历工具的原因在于，云端LLM在执行用户指令和完成GUI自动化任务方面展现出卓越的能力[25,90,92,107]。在该系统中，LLM主要在两种情况下接管复杂屏幕状态：（1）当遍历工具检测到预定义的人工触发条件时；（2）当系统在同一GUI状态上长时间无响应时。在触发条件场景下，该系统需提前设定触发条件，并在LLM中定义相应的处理流程。基于本研究的实证观察，大多数现代

应用在访问更深层次且更具代表性的界面前，都需要用户完成账号注册或登录。因此，该方法设计了“账号注册/登录”触发条件，并配套提示词来处理注册/登录逻辑。具体来说，该触发器会在遍历过程中分析捕获到的GUI状态，检测诸如“login”、“sign in”等关键词。一旦触发，LLM将接管注册/登录流程，直至完成。借助GPT-4o[21]，该方法在自动遍历任务中实现了 $44.2\%$  的成功率（86个应用中有38个可成功遍历）。值得一提的是，这种基于人工触发条件与LLM结合的方法，可拓展至注册、登录之外的多类复杂屏幕状态处理，体现了良好的可扩展性。而在屏幕状态长时间停滞的情况下，该方法会向LLM明确遍历目标一一捕获更多GUI数据，从而由其接管遍历逻辑并在当前界面执行后续操作。


屏幕截图


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/f7846c2d0c6d816fd4b49e947b02d4feb7c5884f5ce1d438c99d6fa9e6016942.jpg)



原始视图层级信息


```python
<?xml version='1.0' encoding='UTF-8' standalone='yes'?>
< hierarchy rotation="0">
    <node index="0"
        text="resource-id=" class=" android.widget.FrameLayout" content desc=""
        checkable="false"
        checked="false"
        clickable="false"
        selected="false"
        bounds=[[0,0][1080, 2340]]
```


输入：可交互GUI组件


（简化后视图层级信息）

```txt
<scrollbar id=0>Home</scrollbar>
<button id=1>Main menu</button>
<input id=2></input>
<button id=3>Edit image</button>
<button id=4>Social media</button>
...
```

输出：GUI组件及对应动作

```txt
- id = 0: Scroll, Left
- id = 1: Touch
- id = 2: Input,
    "Conference"
- id = 3: Touch
- id = 4: Touch
- id = 5: Touch
```


图3-3 自动化应用遍历流程的输入与输出示例


人工干预。虽然LLM的引入显著降低了应用遍历对人工的依赖，但依旧存在部分屏幕状态超出其处理能力，特别是某些应用的注册或登录流程需要验证码输入或基于手机号的人机验证。这类复杂任务仍无法由LLM独立完成。对此，该方法引入人工标注人员作为补充。为提升遍历效率，人工干预流程采用延迟处理策略：当遍历工具遇到无法自动处理的屏幕状态时，会记录当前界面并暂停任务；人工标注人员随后集中处理这些应用，手动解决卡住的屏幕状态后，再恢复自动遍历。此方法在保障遍历完整性的同时，大幅降低了人工投入。在实际执行过程中，数据收集人员每隔12小时对所有Android实例的屏幕状态进行检查，并在需要接管的Android实例上执行人工干预。

# 3.2.3 基于SoC阵列的多应用并行交互框架

自动应用遍历工具旨在优化单应用的遍历效率，但在大规模应用场景

下，还需解决如何高效抓取大量依赖原生移动端环境运行的现代应用[139]。传统方案多依赖物理智能手机集群或移动端模拟器，但面临基础设施成本高、仿真精度不足以及性能瓶颈等问题，导致爬取过程耗时较长。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/7619c219d7fab63b37dd9128f0811dd9faeeb392141cfb17aa72bd21c5477188.jpg)



图3-4SoC阵列服务器架构图


为提供高密度、原生移动端环境，本研究利用广泛部署于现有阿里云边缘云节点的移动端SoC阵列服务器。如图3-4所示，SoC阵列服务器的整体架构由多个移动端SoC板卡（图中ARM SoC）组成，每五个SoC板卡集成在一块底板上，底板上具备专用电源与网络模块，同时也充当SoC间通信的交换节点。此外，服务器中还包含一块以太网交换板，用于将所有SoC统一连接至外部网络。底板和以太网交换板通过一块背板互联。系统还配备有服务器管理单元，可实时监测并管理服务器的电源状态、温度和冷却设备等。其服务器内部的硬件组件可分为如下几类：

（1）计算节点：SoC阵列服务器配备60颗QualcommSnapdragon865SoC作为计算核心[140]。每个SoC拥有12GB内存和256GB存储空间，搭配Android10系统，提供了原生移动端系统软件环境。

（2）网络模块：网络通信由两部分硬件提供支撑。一是以太网交换板，通过其双SFP+端口将计算节点连接到外部网络，支持最高20Gbps吞吐率；二是12块具备交换能力的底板，它们在SoC与以太网交换板之间建立以太网通信通道，每块底板支持最高1Gbps的带宽。

（3）管理模块：SoC阵列服务器通过独立的服务器管理单元对整机进行管理，包括计算单元、电源、温度和故障检测等。控制消息传输依赖于I2C、USB、UART等多种协议与接口。服务器管理单元还具备以太网端口，便于外部系统接入管理。

（4）散热与电源模块：系统配备8个风扇用于移动端SoC、以太网交

换板与服务器管理单元的空气循环，并通过机箱后部排风。供电方面采用双电源模块提供冗余，最大支持约700W功率，确保服务器稳定运行。

作为云手机和移动端云游戏的硬件基础架构，SoC阵列服务器可提供原生移动端环境，相比于在x86架构的服务器上以安卓模拟器提供移动端环境，SoC阵列服务器可保证绝大多数移动端应用可在其上运行。本节使用了共由120颗QualcommSnapdragon865SoC组成的两个SoC阵列服务器[136]，以提供120个独立的Android系统。鉴于单个应用通常无法充分利用一颗SoC的全部算力，本研究进一步引入SoC虚拟化技术，即在同一个Android系统上运行多个Android Framework实例，每个实例之间互相隔离，保证了单个移动端SoC可运行多个应用，最大化利用硬件资源。最终，该方法在120个SoC上部署了超过200个Android实例，显著提升了应用爬取的规模化能力。

为最大化利用这些Android实例，应用交互协调器实时监控可用Android实例数量。当检测到实例空闲时，协调器会在Linux服务器上启动一个应用遍历进程，并通过网络使用adb连接至目标Android实例，之后由应用遍历工具接管遍历任务。遍历结束或进程异常退出后，该Android实例会被释放回可用状态。该调度策略平均在一个月内即可高效完成81600设备小时的应用遍历工作。

# 3.3 GUI数据集MobileViews统计分析

# 3.3.1 统计数据

基于上一章节所提出的系统，本研究收集了MobileViews数据集。MobileViews收录了来自2万余款现代Android应用的逾60万张不同GUI界面。相较于此前规模最大且数据完整性相当的移动端GUI数据集，其GUI界面数量提升了近10倍，应用数量提升了2倍。其数据覆盖了GooglePlayStore的33个应用类别，大幅增强了界面的多样性。与仅提供截图的AITW[47]数据集不同，MobileViews同时包含屏幕截图与VH信息，其中VH能够为移动端GUI智能体提供精确的GUI上下文解析（如组件标签、类别与边界框）。这种全面的数据资源使得GUI智能体能够建立像素级屏幕截图与文本化GUI组件元数据的映射关系。此外，该数据集还包含GUI轨迹与交互操作等数据，这些信息在未来的研究中有望进一步挖掘其潜力

除了表2-1中所展示的数据集规模与GUI数据完整性之外，本研究进一步关注两个统计问题：其一是MobileViews中每个应用所能收集的GUI数量，其二是大规模移动端GUI数据集中界面相似度的水平。研究界面相

似度的动机在于，AITW数据集虽然仅涵盖约300个应用，却包含超过500万张GUI，这意味着其中可能存在大量相似样本。而对于移动端GUI智能体而言，过多相似界面的学习极易导致过拟合。为此，本研究在分析中采用ImageHash库[141]计算每张屏幕截图的感知哈希值，并将感知哈希汉明距离小于或等于5的两张图片视为相似界面。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/7cecb086d45b63ac17bca6180f744b0b80b30980755fecf46658550fa2e93d18.jpg)



图3-5MobileViews中不同应用程序GUI数量分布


针对问题一，在MobileViews中，每个应用的界面数量分布如图3-5所示。大部分应用仅产生10-30张不同的GUI，该结果在MobileViews爬取了大量应用的情况下尤为合理，因为多数非热门应用功能单一；此外，约  $6.8\%$  的应用能够生成超过100张不同的GUI。总体上，本研究共采集了超过60万张不同的移动端GUI数据，充分体现了MobileViews的多样性。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/fee48fdd77d1d3f89a79f3d97c5184a97e61c133a1fe3ae8e04e31545fbabe33.jpg)



图3-6不同GUI数据集中相似屏幕数量的累计分布函数


针对问题二，本研究比较了 Rico、AITW 与 MobileViews 内的 GUI 相似度。针对每张界面，统计其在同一数据集中的相似界面数量，并绘制其累积分布函数（Cumulative Distribution Function, CDF）。图 3-6 结果显示，Rico 与 MobileViews 均具有较高的 GUI 独特性，这得益于丰富且多样的应用来源以及精心设计的遍历流程。相较之下，AITW 相似界面比例显著偏高：其中超过 120 万张界面拥有 100 张以上的相似界面，最常见的界面甚至有 6.7 万相似版本。这一对比突显了 MobileViews 在应用内及跨应用界面多样性上的优势。基于这一分析，本研究认为 MobileViews 将为提升移动端 GUI 智能体的屏幕感知能力提供重要支持。

# 3.3.2 GUI组件标签对比分析

本节将MobileViews与此前规模最大的同时包含屏幕截图与VH的数据集Rico进行比较，重点关注两者的VH数据，以评估其在提供全面、高质量GUI组件信息方面的表现，并进一步探讨这些信息对移动端GUI智能体学习的价值。研究主要围绕两个核心问题展开：其一，现代移动应用的VH中文本化GUI组件信息的完备性是否优于以往的数据集；其二，考虑到Rico数据集收集于2017年，GUI组件的标注质量是否在近年来得到显著提升，以及这一变化对移动端GUI理解能力产生了怎样的影响。

# 3.3.2.1 标签覆盖率分析

为评估移动应用中GUI组件标签信息的覆盖程度，本节首先对MobileViews与Rico在标注质量上的差异进行了比较，重点考察两个方面：其一，GUI组件整体的标签完备率；其二，先前研究已指出的基于图像的GUI组件是否得到有效标注[142]。为执行该问题，本节首先根据组件属性进行分类：通用组件（如文本框、按钮、输入框等）通过clickable、editable、scrollable等属性识别；基于图像的组件（ImageViews与ImageButtons）通过类名识别。该分类方式允许本研究系统性地分析MobileViews中通用与图像组件的标签覆盖情况。

通用组件上的结果表明，可点击组件、输入框、按钮、滚动条及复选框等虽为核心交互元素，但在MobileViews中仍有较大比例缺少标签（见图3-7(a)）。其中滚动组件与输入组件的缺失率最高，这不仅影响应用的可访问性（Accessibility），也给GUI智能体的模型训练带来困难。ImageViews与ImageButtons作为重要的视觉交互元素，其标签缺失更为严重。图3-7(b)显示，与Rico的  $37.13\%$  和  $48.90\%$  相比，MobileViews中未标注的比例分别达到  $81.27\%$  与  $44.55\%$  。若将两类组件合并为可聚焦图像组件整体，缺失比例高达  $68.8\%$  ，显著高于2022年研究[142]的  $55.6\%$  及2017年Rico的  $48.3\%$  。这表明当前对关键图像组件的标注严重不足，而这些组件对于VLM的精准理解和交互至关重要。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/a426c8a72eece9d9fee0a3535307213ebe5096d83573aea4d06a34688233e481.jpg)



(a) 无标签通用GUI组件


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/4d0bd81b58be6f2f887841a4f9b54d6be089f9130e4ed5e1cec4dec2a4429f8e.jpg)



(b) 无标签图像GUI组件



图3-7 Rico和MobileViews中无标签GUI组件比例


对MobileViews中通用组件与图像组件的分析表明，标签缺失在各类GUI组件中普遍存在，且在可聚焦的图像组件中尤为严重。较高的未标注比例不仅削弱了移动端助手的学习效果，也对应用的可访问性造成不利影响。该发现凸显了优化标签标注流程的紧迫性，以提升现代移动应用的可用性与交互性能。

# 3.3.2.2 标签质量分析

为评估标签质量随时间的变化及其对移动界面理解的影响，本节首先对Rico与MobileViews的标签长度分布进行了分析。随后，利用GPT-4o[21]执行“标签到视图”匹配任务，对比其在不同标签长度区间下、使用两个数据集标签时的表现，从而同时衡量标签标注实践的演变与其对移动端智能体GUI组件识别精度的影响。本节先从Rico和MobileViews的VH中提取全部标签，统计其长度分布，并进一步分析MobileViews中不同GUI组件的标签长度特征，以探索可能影响GUI智能体屏幕理解效果的因素。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/eeb6566c9a00c7fd9d78fef850838356e6c252bde4bfa1215bf16cb647a9651c.jpg)



(a) 跨数据集分析


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/63929128ab3b3750d921090f2a9fdc43b0cac4549c9d84bec06c98b42db484f9.jpg)



(b) 跨GUI组件分析



图3-8 不同数据集和GUI组件类型下标签长度分布


跨数据集对比结果。图3-8(a)展示了两个数据集在标签长度分布上的显著差异：Rico中近  $90\%$  的标签为1-5个单词，而MobileViews则呈现更为均衡的分布，6-10个单词及11-15个单词区间比例明显提高。这一趋势暗示，以MobileViews为代表的现代应用更倾向于使用描述性更强的标签，从而有助于提升移动GUI理解的准确性与可用性。

标签长度在不同GUI组件类型中的分布。如图3-8(b)所示，MobileViews中不同GUI组件的标签长度分布差异明显。文本框标签偏长，11-15个单词区间占比最高，这与其对详细描述内容的需求相符。按钮与输入框标签相对较短，多集中在6-10个单词，但为了更好地体现其交互功能，理应配备更具描述性的标签。若能提升这些交互组件标签的细节丰富度，将有助于移动端GUI智能体在交互准确性与用户体验上取得更好效果。

标签质量及其影响。本节进一步的通过GPT-4o进行“标签到视图”匹配任务，以评估标签质量随时间的变化及其对移动界面理解的影响。在该任务中，GPT-4o需要在给定截图中定位与指定标签对应的GUI组件。

根据此前的长度分布分析，该方法将标签划分为1-5个单词、6-10个单词、11-15个单词和16-20个单词四个区间，并在每个区间中从MobileViews和Rico分别随机抽取500个截图及标签，总计每区间1,000个样本。任务提示词的结构如下表3-3所示。


表 3-3 标签到视图匹配任务提示词设计


<table><tr><td>提示词</td><td>I will provide you with a screenshot of mobile devices and a caption of one of the GUI components in the screenshot.
Can you confidently identify the specific view that corresponds to the caption? Please respond only with “True” or “False” without any further explanation.</td></tr><tr><td>输入</td><td>[GUI component label]</td></tr><tr><td>回答</td><td>True or False</td></tr></table>

表3-4对比了Rico与MobileViews在不同标签长度区间（每区间500个样本）中的正确匹配数量与比例。分析结果表明：首先，在总体表现上，GPT-4o在MobileViews上的匹配准确率全面优于Rico，反映出过去七年中标签质量的整体提升，这一进步直接推动了模型性能的改善。其次，在长标签方面，Rico中随着标签长度增加（11-15词及16-20词），正确匹配率反而下降，说明其在定位正确视图方面作用有限；而在MobileViews中，长标签的匹配准确率显著提高，表明现代应用的数据标注更倾向于提供信息丰富、描述充分的标签，从而更有效地支持移动端GUI智能体对GUI组件的精准识别与交互。


表 3-4 不同标签长度下 GPT-4o 视图识别任务匹配准确率


<table><tr><td>标签长度</td><td>Rico
(正确匹配百分比)</td><td>MobileViews
(正确匹配百分比)</td></tr><tr><td>1-5</td><td>213 (42.60%)</td><td>218 (43.60%)</td></tr><tr><td>6-10</td><td>121 (24.20%)</td><td>278 (55.60%)</td></tr><tr><td>11-15</td><td>91 (18.20%)</td><td>257 (51.40%)</td></tr><tr><td>16-20</td><td>54 (10.80%)</td><td>279 (55.80%)</td></tr></table>

# 3.3.2.3 本节小结

本节的研究结果表明，移动端应用GUI组件标签的覆盖率与质量在过去七年间取得了明显进步，但依旧存在重大挑战。MobileViews中更具描述性的标签以及GPT-4o在匹配任务中的更优表现，验证了标签质量的提升。然而，标签覆盖不足依然是瓶颈，本节的评估显示近半数标签未能正确匹配，凸显出这一问题尚未根本解决。尽管Apple[143]与Google[144]通过

更新开发者指南推动了标注实践优化，Rico与MobileViews仍普遍存在覆盖率低的现象。因此，在标签缺失或质量不足的情况下，更应依赖如可点击性和父子关系等稳定属性，以降低其对移动端屏幕助手训练带来的不利影响。

# 3.4 基于自监督强化学习的VLM强化方法

现有利用移动端GUI数据强化VLM的方法是通过有监督微调（Supervised Fine-Tuning，SFT）在标注数据集上进行训练，例如在AITW和AndroidWorld中预先录制的、带有人类注释任务指令的GUI操作轨迹。尽管这种方法有效，但收集带任务标注的GUI轨迹仍然是一项劳动密集且容易出错的工作[47,53,124,145]。例如，收集AndroidControl数据集就需要一年的付费标注工作，才能生成15,283条任务演示[53]。高昂的数据采集成本使得这一范式难以扩展。由于MobileViews数据集以自动化的方式收集，并未有任务指令标注，在MobileViews数据集之上进行监督学习需要额外的标注开销。本节内容提出一种基于自监督强化学习的方法与任务：K步界面转移，利用MobileViews数据集内收集到大量未标注的GUI操作轨迹数据，以增强现有VLM的感知与决策能力。图3-9为该自监督强化学习任务的示意图。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/bd6e30ac79cee4d79f247eaaf8908783ec4e8605976fb61cf2602578f075a5c8.jpg)



图3-9自监督强化学习训练任务  $-\mathbf{K}$  步界面转移


# 3.4.1 K步界面转移任务

为了充分利用MobileViews中无人工标注的大规模的GUI操作轨迹数据，本研究设计了一种自监督训练任务，称为K步界面转移。该方法受到机器人与生物力学中的反向动力学建模的启发[146-148]，在这些应用中，网络模型预测连接两个连续物理状态之间的控制命令；而在本节所提出的任务中，将屏幕截图视为状态，将单张GUI上的动作视为控制命令。如图3-9所示，K步界面转移任务的每条训练样本包含一对GUI截图：当前界面状

态  $S_{t}$ ，以及通过对  $S_{t}$  执行  $\mathbf{k}$  步操作后得到的未来状态  $S_{t + k}$ 。模型无需任何额外指令，仅通过观察这两个界面状态，预测使  $S_{t}$  转变为  $S_{t + 1}$  的第一个动作。尽管 VLM 模型在预训练阶段已接触过 GUI 数据，具备理解单一界面的能力，但其难以建模状态之间的变化与动作序列规划。而 K 步界面转移任务通过引导模型对比  $S_{t}$  与  $S_{t + 1}$ ，让其主动识别语义上的变化、定位  $S_{t}$  中发生改变的区域，并生成相应的结构化动作，这一过程本质上与机器人中的反向动力学建模相似：通过预测导致状态转变的控制信号，使模型获得对“发生了什么变化”与“如何产生该变化”的联合理解。在 GUI 情境下，当模型聚焦于两个界面状态之间的差异时，它能自动忽略无关信息，更专注于实际可操作区域。这种能力对于提升模型的感知定位能力，以及在需要多步动作推理的自动化任务中的决策能力与泛化表现，具有重要意义。

随后，本节使用 GRPO（Group Relative Policy Optimization）方法[149]对视觉语言模型进行 K 步界面转移任务的微调。对于每一组输入的截图对，模型会生成 m 个候选动作，并通过结合格式正确性  $R_{f}$  与动作准确性  $R_{a}$  的规则奖励函数 R 为每个动作评分。接着，该方法对这些候选动作进行组内归一化优势计算，并基于 GRPO 目标进行优化。与传统的有监督微调方式相比，GRPO 允许模型提出多种可能的动作并进行排序，从而为训练过程提供比单一标签更丰富的反馈。这种机制促进了模型的主动探索，而非死板地复制标签动作。将 GUI 界面转移任务与 GRPO 相结合，不仅能摆脱对人工标注数据的依赖，还能降低模型过拟合于单一动作选择的风险，实现更加稳健和可扩展的训练流程。下一章节将介绍自监督强化学习任务中的奖励函数设计。

# 3.4.2 奖励函数设计

为了评估推理能力在GUI相关任务中的影响，本节实验设计了两种训练配置：启用推理（reasoning-enabled）和不启用推理（reasoning-free），其区别在于模型在训练过程中是否需要执行思维链推理。这两种设置均采用一个基于规则的奖励函数  $R$  来指导训练，该奖励函数由格式奖励  $R_{f}$  与动作准确性奖励  $R_{a}$  组成：  $R = R_{f} + R_{a}$  。

格式奖励  $R_{f}$  设计。格式奖励用于约束模型输出遵循特定结构规范，本节奖励函数设计采用DeepSeek-R1[111]中的格式规则。在启用推理的训练设置中，模型必须将推理过程用<think>...</think>标签包裹，并将最终答案写在<answer>...</answer>标签中；而在不启用推理的设置中，仅要求输出包含<answer>...</answer>标签块。只有当模型输出严格符合所需格式时，才会获得  $R_{f} = 1$  的奖励，否则为  $R_{f} = 0$  。

动作准确性奖励  $R_{a}$  设计。准确性奖励依据模型生成的动作与真实动作之间的一致性进行打分。在训练与推理过程中，统一使用包含8种动作类型的动作空间：Click（点击）、Scroll（滑动）、Open_app（打开应用）、Long_press（长按）、Navigate_home（返回主界面）、Navigate_back（返回上一页）、Type（输入文本）和Terminate（结束任务，返回任务成功或失败）。每个动作以结构化JSON格式表示，包含动作类型及其相关参数。例如，Click动作用中心坐标(x,y)表示目标控件的位置；Scroll指定滑动方向（上、下、左、右）；Open_app与Type分别接受一个字符串参数；Navigate_back与Navigate_home则无需任何参数；Terminate接受布尔类型参数，其需要判断任务是否成功完成，若成功参数值为True，否则为False。对于非Click动作，只有在动作类型及所有参数均与真实标签完全一致时，才给予准确性奖励  $R_{a} = 1$  ，否则为0。对于点击动作，本节设计采用更贴近实际应用的容错机制：只要预测坐标(x,y)落入真实目标GUI元素的边界框（Bounding Box,BBox）内，即认为动作正确。为实现该策略，该设计从每组界面转移样本中第一个截图所对应的视图层级结构中，提取出真值点击动作所对应元素的BBox，并将其添加至训练数据中。模型训练时仅需输出坐标(x,y)，该坐标再与真实BBox进行比较。若预测位置位于BBox范围内，则判定为正确，  $R_{a} = 1$  ；否则为0。

# 3.5 GUI数据集对比评估

为验证MobileViews数据集在提升移动屏幕助手性能方面的有效性，本研究设计了一种两阶段训练策略，并将其与现有的包含相同数据完备度的移动端GUI数据集Rico进行了性能对比。训练阶段一利用MobileViews数据集中丰富的结构与语义信息对VLM进行微调。该训练使模型能够精准理解移动端界面的整体布局，并捕捉GUI元素的细节属性。阶段二将经过第一阶段训练的VLM在四个下游移动端GUI任务上进行进一步微调。值得注意的是，其中三项任务旨在利用MobileViews提供的通用属性，以解决GUI组件标签缺失或不一致的挑战。实验结果表明，在第一阶段采用MobileViews数据集进行预训练的模型，在绝大多数下游任务中的性能显著优于使用其他移动端GUI数据集训练的模型，充分证明了MobileViews数据集的优越性。

# 3.5.1 两阶段训练任务与数据构造

两阶段训练方法旨在有效提升模型对移动端用户界面的理解与交互能力。通过结合全参数微调与LoRA（Low-RankAdaption）微调[150]，在增强

VLM 感知能力的同时，强化处理常见端侧 GUI 任务的决策能力。图 3-10 展示了训练任务的构造。整个训练过程分为两个阶段：在第一阶段，该方法构建了基于 VH 信息的全参数微调，旨在培养模型对 GUI 结构的深度理解，使其能够掌握空间关系和整体界面布局。为实现该目的，本节设计了 VH 生成任务，以帮助模型全面理解 GUI 元素及其相互关系。这一过程通过从移动端 GUI 截图生成 VH 信息来实现。本节从 Rico 和 MobileViews 两个数据集构建训练数据。根据实验设置不同，数据集规模从 66000 对到 500000 对截图-VH 样本不等。为确保不同 GUI 间的缩放一致，边界框坐标被归一化到(0,1)范围。此外，该任务对 VH 进行了简化，去除了模型难以从图像中推断的细节，如应用包名和前台活动名。该阶段采用全参数微调以强化 VLM。

第二阶段中，该方法使用应用面向移动屏幕任务的 LoRA 微调。在第一阶段建立的结构基础之上，第二阶段针对四类移动屏幕任务进行 LoRA 微调，其中包含三个元素级任务和一个屏幕级任务。元素级任务包括可点击性预测、元素关系预测和 GUI 组件识别，这些任务利用诸如可点击性、父子关系等常见的 VH 属性，提升模型解释单个视图的能力。具体来说：

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/788b7d577ac818fbb902c99ff42c7919babfbbe6a7f199e9e35b8fa6e4d6b9fd.jpg)


# 阶段一：基于屏幕截图-视图层级数据对的全参数微调

# VH生成任务

Q: Given a mobile UI, generate the corresponding view hierarchy.

A: (Take a view in box1 as an example)  
{"visible":true, "checkable":false, "child_count":0, "editable":false, "clickable":true, "is_password":false, "focusable":true, "enabled":true, "content_description":null, "children": [], "focused":false, "checked":false, "text":"Logos", "selected":false, "parent":47, "temp_id":48, "bound_box":[0.041, 0.184, 0.351, 0.243]}

# 阶段二：在多个移动端屏幕任务下进行LoRA微调

# 可点击性预测

Q: Predict whether the UI element [box1] is clickable. (Answer with True or False)  
A: True

# 元素关系预测

Q: Predict whether the relationship between the UI elements in [box1] and [box2] is a parent-child relationship. (Answer with True or False)

A: False

# GUI组件识别

Q: Identify the type of the UI element located in [box2]. (Choose from BUTTON, CHECKBOX, TEXT, INPUT FIELD, SCROLL ITEM)

A: BUTTON

# 屏幕问答

Q: How many templates are available for search in total? A: 91072


图3-10 两阶段训练任务示意图


（1）可点击性预测（Tappability Prediction）：模型根据视觉与空间线索预测GUI组件是否可点击，帮助基于视觉的智能体在缺少显式标签的情况下识别交互元素。本节从300000张截图中随机选取595000个GUI组件。每张截图通常包含一个可见且可点击的GUI组件，以及一个可见但不

可点击的GUI组件，从而提供多样化的可点击与不可点击样本。

（2）元素关系预测（Element Relationship Prediction）：模型判断两个GUI元素是否存在父子关系，这对理解GUI的层级结构及复杂布局至关重要。本节从150000张截图中随机选取约150000对可见且可聚焦的GUI组件，其中包含69156对父子关系样本和80844对非父子关系样本，保证两类关系的均衡分布。该筛选标准确保所有选定元素均与交互相关，从而提升模型解释关键GUI组件之间关系的能力。

（3）GUI组件识别（GUI Element Identification）：模型基于GUI组件的边界框坐标，将其分类为button、checkbox、text、input field和scroll item五类，用于测试模型对不同GUI组件功能与用途的理解能力。本节从300000张独立截图中随机选取约300000个视图，每张截图仅选取一个视图。每个视图根据其交互属性（源自对应VH）被划分为五类GUI组件之一。具体分布为：button类占157590个实例，为最大类别；text类55049个实例；scroll item类36816个实例；checkbox类24023个实例；input field类35361个实例。整个数据集共包含308839个GUI组件实例。

（4）屏幕问答（ScreenQA）：该任务聚焦于理解整个屏幕，以回答关于其内容的问题。作为屏幕级任务，ScreenQA包含两个子任务：ShortQA用于对特定GUI元素的简单问题给出简洁答案；ComplexQA则应对更复杂的问题，涉及在多个元素间进行计数、运算或比较。本节使用开源的ScreenAI[102]数据集，其中包括FullQA（85984对）、ScreenAnnotation（22076对）、ShortQA（171115对）和ComplexQA（11781对），总计约29万对数据。在评估中重点使用ShortQA与ComplexQA子集，以衡量模型在应对简单与复杂查询任务中的表现。

# 3.5.2 实验设置

本节在实验中采用三种数据集配置，以探究数据规模与构成对模型性能的影响：Rico[124]数据集包含约66000对截图-VH样本，作为标准规模下的性能基准；MobileViews数据集则包含两个子集，分别为66000对和500000对截图-VH样本，用于分析数据规模差异带来的影响。在微调策略上，本节比较了两种方案：一种是先进行全参数微调再对每个下游任务进行LoRA微调，以量化任务特定适配带来的额外性能提升；另一种是仅对VH生成任务进行全参数微调，不进行任务特定微调，以评估仅依赖VH生成的性能表现。模型方面，本节在元素级与屏幕级任务上评估MiniCPM-V2.0（2.4B）[151]，并在屏幕级任务上评估InternVL2-2B[152]，二者均能在设备端运行且在多模态相关基准测试中表现出SOTA性能。数据划分上，VH

生成与三个元素级任务采用90/10的训练与评估比例，屏幕级任务ScreenQA则将官方训练集与验证集合并用于训练，并保留测试集用于评估，从而确保所有任务均在未见数据上测试。评估指标方面，可点击性预测与元素关系预测使用二分类F1分数，GUI组件识别使用多分类F1分数，ShortQA与ComplexQA则采用SQuAD F1分数[153]进行评估。

# 3.5.3 实验结果

对比MobileViews的66K与500K数据规模可以发现，更大规模的数据通常能够带来更显著的性能提升。500K数据集中更丰富的GUI元素与更高的界面复杂性，增强了模型在不同界面与场景中的泛化能力。例如，表3-5显示，在LoRA微调条件下，可点击性预测的F1分数由66K数据集的77.10提升至500K数据集的80.14；表3-6则表明，MiniCPM-V2.0在ShortQA任务上的得分由69.83提高至73.33。然而，值得注意的是，表3-5中Rico数据集（66K）在GUI组件识别任务上取得了最佳成绩，F1分数为73.08。这一现象可能源于Rico界面设计更为简单、结构更为统一，相比MobileViews，其样本中边缘情况较少，从而降低了学习难度，使模型在该任务中更易获得较高精度。


表 3-5 VH 生成任务数据规模对基础 GUI 任务性能影响


<table><tr><td rowspan="2">VH生成任务
数据来源</td><td rowspan="2">数据
规模</td><td colspan="3">任务</td></tr><tr><td>可点击性预
测性能</td><td>元素关系
预测性能</td><td>GUI组件
识别性能</td></tr><tr><td>N/A</td><td>N/A</td><td>77.96</td><td>71.08</td><td>59.33</td></tr><tr><td>Rico</td><td>66K</td><td>66.13</td><td>90.57</td><td>73.08</td></tr><tr><td>MobileViews</td><td>66K</td><td>77.10</td><td>91.17</td><td>63.96</td></tr><tr><td>MobileViews</td><td>500K</td><td>80.14</td><td>93.48</td><td>68.78</td></tr></table>


表 3-6 VH 生成任务数据规模对 ScreenQA 性能的影响


<table><tr><td rowspan="2">VH生成任务
数据规模</td><td colspan="2">ShortQA</td><td colspan="2">ComplexQA</td></tr><tr><td>MiniCPM</td><td>InternVL</td><td>MiniCPM</td><td>InternVL</td></tr><tr><td>N/A</td><td>68.39</td><td>72.75</td><td>55.21</td><td>68.80</td></tr><tr><td>66K</td><td>69.83</td><td>69.73</td><td>55.11</td><td>67.05</td></tr><tr><td>500K</td><td>73.33</td><td>71.76</td><td>60.56</td><td>70.18</td></tr></table>

# 3.6 实验与分析

本节中对所提出的基于自监督强化学习的K步界面转移任务对移动

端GUI智能体感知和决策性能的提升情况进行评估。其中，GUI智能体的感知能力通过GUI定位测试集进行测试，GUI智能体的决策能力则通过诸如AndroidControl的GUI数据上的任务自动化测试集评估。

# 3.6.1 实验设置

本节基于前文章节提出的训练流程，结合开源强化学习框架 VLM-R1[154]，对 Qwen2.5-VL 7B 模型和 InternVL3-8B 模型在 8 张 NVIDIA A800 GPU 上进行微调。微调过程中，该方法保持视觉编码器和 MLP 层冻结，具体的训练参数如表 3-7 所示。对于 K 步界面转移任务中，对每个样本使用当前的 VLM 进行 8 次采样，并确保采样数据中包含正确答案。实验过程中，该方法构造了不同规模的 K 步界面转移任务训练数据。为了与依赖人工标注的任务进行对比，本节额外引入了两种任务：（1）基于精确指令的任务自动化（Task Automation Low），即给出截图和精确动作指令（例如“点击分享按钮”）；（2）基于任务级目标的 GUI 任务自动化（Task Automation High），即给出截图和任务级目标（例如“将新闻分享到 Gmail”），需要模型自主判断当前状态和应执行的动作。其中，K 步界面转移任务的训练数据来自 MobileViews，后两种依赖于任务指令的数据来自于 AndroidControl。本节实验分别从不同的应用（MobileViews 数据集）以及不同的任务/任务完成轨迹（AndroidControl 数据集）中挑选样本，确保界面和任务的多样性。


表 3-7 用于 GRPO 训练的超参数设置


<table><tr><td>超参数</td><td>值</td></tr><tr><td>学习率</td><td>从1e-6到0</td></tr><tr><td>温度系数</td><td>0.9</td></tr><tr><td>VLM采样数量</td><td>8</td></tr><tr><td>训练轮数</td><td>8</td></tr><tr><td>每设备训练批大小</td><td>2</td></tr><tr><td>梯度累积步数</td><td>8</td></tr><tr><td>ε（梯度裁剪参数）</td><td>0.2</td></tr><tr><td>β（KL散度系数）</td><td>0.04</td></tr></table>

为了评估GUI智能体的感知与决策能力，实验使用AndroidControl-Low与AndroidControl-High两个测试集。两者都要求模型根据截图与指令预测下一步动作，其中AndroidControl-Low测试集提供即低层级、细粒度的操作指令，也会提供高层级的任务指令；AndroidControl-High测试集则仅提供高层级的任务指令，需要模型综合判断任务进展状态，进行动作决策。

本实验将动作预测准确率作为评估指标，其判断标准为动作类型与所有参数是否与真实动作完全一致。

# 3.6.2 实验结果

AndroidControl 测试集准确率。本节实验在 AndroidControl-Low 与 AndroidControl-High 测试集上对两个微调后的 VLM 进行性能评估：首先构造 10K 的 K 步界面转移任务（ $K = 1$ ），并分别使用 SFT 和 RL 方法对 VLM 进行微调。表 3-8 和表 3-9 展示了实验结果，并有如下的观察：


表 3-8 AndroidControl-Low 测试集性能比较


<table><tr><td rowspan="2">VLM</td><td rowspan="2">训练方法</td><td colspan="6">准确率(%)</td></tr><tr><td>All</td><td>Click</td><td>Scroll</td><td>Navigation</td><td>Termination</td><td>Type</td></tr><tr><td rowspan="3">InternVL3-8B</td><td>/</td><td>90.12</td><td>92.13</td><td>77.18</td><td>95.43</td><td>70.95</td><td>95.62</td></tr><tr><td>SFT</td><td>33.14(56.98↓)</td><td>37.52</td><td>10.33</td><td>65.32</td><td>8.81</td><td>23.65</td></tr><tr><td>RL</td><td>88.92(1.2↓)</td><td>91.32</td><td>73.32</td><td>94.89</td><td>64.61</td><td>95.91</td></tr><tr><td rowspan="3">Qwen2.5-VL-7B</td><td>/</td><td>84.09</td><td>90.75</td><td>46.26</td><td>94.62</td><td>87.23</td><td>96.5</td></tr><tr><td>SFT</td><td>61.49(22.6↓)</td><td>56.92</td><td>64.69</td><td>62.63</td><td>75.81</td><td>91.53</td></tr><tr><td>RL</td><td>88.3(4.21↑)</td><td>93.37</td><td>63.38</td><td>85.75</td><td>71.21</td><td>96.2</td></tr></table>

（1）在AndroidControl-Low和AndroidControl-High测试集上，使用SFT得到的VLM，其性能相比于原始VLM都有显著的下降。特别的，在AndroidControl-Low数据集上，使用SFT微调InternVL3-8B模型后，测试集上的准确率下降高达  $56.98\%$  。其他情况下，准确率下降约  $20\% \sim 30\%$  。这体现了由于训练任务和评估任务输入/输出的不一致，SFT仅基于静态标注数据进行模仿学习的训练方法无法保证VLM在K步界面转移算法中学习到足够多的GUI状态转化知识，从而大幅度影响了GUI智能体性能。

（2）基于GRPO的训练方法仅在InternVL3-8B与AndroidControl-Low测试集上展现出了轻微的性能下降（ $1.2\%$ ）；在其他情况下，其性能相比于基础模型有不同程度的提高。其中，在AndroidControl-High数据集上，Qwen2.5-VL-7B模型性能提升高达  $7.48\%$  。这体现了本章所提出自监督训练任务，配合强化学习训练算法的优越性。

（3）具体到训练方法，经过强化学习GRPO微调后的VLM，相比于SFT的得到的VLM，平均任务完成率高了  $36.24\%$  。本文分析认为，其性

能提升归结于 GRPO 算法对同一条训练数据多次采样的设计赋予了 VLM 更大的探索空间，使得 VLM 可以同时从正向样本和负向样本中进行学习和优化，从而帮助 VLM 在复杂多变的移动端 GUI 界面上作出更准确的感知与决策。

（4）尽管大多数情况下，使用 GRPO 算法进行强化学习微调之后得到的 VLM 性能都得到了提升，但将其准确率分解到不同动作类型，可以观测到并非所有动作类型的准确率都有提升，特别是 Navigation 类动作（Navigate_back 和 Navigate_home）与 Termination 类动作降幅明显。这也表明了当前从 MobileViews 采样训练数据的策略可能导致动作类型分配不平衡的现象。若要进一步强化 VLM 性能，未来工作需要更完备的 GUI 数据筛选与验证流程。


表 3-9 AndroidControl-High 测试集性能比较


<table><tr><td rowspan="2">VLM</td><td rowspan="2">训练方法</td><td colspan="6">准确率(%)</td></tr><tr><td>All</td><td>Click</td><td>Scroll</td><td>Navigation</td><td>Termination</td><td>Type</td></tr><tr><td rowspan="3">InternVL3-8B</td><td>/</td><td>50.25</td><td>54.63</td><td>20.28</td><td>53.49</td><td>10.81</td><td>70.07</td></tr><tr><td>SFT</td><td>18.63(31.62↓)</td><td>21.71</td><td>5.24</td><td>44.35</td><td>8.73</td><td>6.42</td></tr><tr><td>RL</td><td>54.64(4.39↑)</td><td>59.41</td><td>21.97</td><td>52.15</td><td>17.89</td><td>79.56</td></tr><tr><td rowspan="3">Qwen2.5-VL-7B</td><td>/</td><td>60.08</td><td>67.19</td><td>18.04</td><td>62.1</td><td>72.86</td><td>81.46</td></tr><tr><td>SFT</td><td>41.19(18.89↓)</td><td>38.1</td><td>51.58</td><td>27.15</td><td>16.46</td><td>54.01</td></tr><tr><td>RL</td><td>67.56(7.48↑)</td><td>74.04</td><td>49.65</td><td>29.84</td><td>20.45</td><td>69.93</td></tr></table>

K与数据规模对VLM性能影响。上述实验是在  $\mathbf{K} = 1$  的情况下构造K步界面转移任务对VLM进行后训练。为了验证不同K值与不同数据规模对VLM性能的影响，本节实验额外构造了  $\mathrm{K} = 2$  的后训练任务，并在Qwen2.5-VL-7B模型上使用GRPO进行微调。其原理在于，如果VLM能够根据相隔较远的GUI状态推理出在首个GUI上应执行的动作，那么其有更大的概率学习到GUI之间的转换关系。本实验额外使用了2K和10K两个不同规模的数据对VLM进行微调，展示数据规模对模型性能的影响。同样在AndroidControl-Low和AndroidControl-High两个测试集上进行了实验。实验结果如表3-10所示，有如下观察：（1）在AndroidControl-Low测试集上，随着数据规模由2K扩大至10K，模型整体准确率呈现稳定提

升。例如，在  $\mathbf{K} = 1$  时准确率从  $86.26\%$  上升至  $88.3\%$  ，在  $\mathbf{K} = 2$  时则由  $82.53\%$  提升至  $87.41\%$  。这表明更大规模的数据能够帮助模型学习更全面的GUI状态转化模式。

（2）不同K值对性能的影响并不一致。在AndroidControl-Low中， $\mathbf{K} = 1$  通常优于  $\mathbf{K} = 2$  。这说明当仅基于相邻两张GUI状态推理动作时，更符合该测试集任务（即以动作指令为输入）的特征，从而带来更优表现。


表 3-10 K 值与数据规模对 Qwen2.5-VL-7B 性能影响


<table><tr><td rowspan="2">测试集</td><td rowspan="2">K</td><td rowspan="2">数据规模</td><td colspan="6">准确率(%)</td></tr><tr><td>All</td><td>Click</td><td>Scroll</td><td>Navigation</td><td>Termination</td><td>Type</td></tr><tr><td rowspan="5">AC-Low</td><td>/</td><td>/</td><td>84.09</td><td>90.75</td><td>46.26</td><td>94.62</td><td>87.23</td><td>96.5</td></tr><tr><td>1</td><td>2K</td><td>86.26</td><td>91.02</td><td>62.99</td><td>79.84</td><td>80.11</td><td>95.47</td></tr><tr><td>1</td><td>10K</td><td>88.3</td><td>93.37</td><td>63.38</td><td>85.75</td><td>71.21</td><td>96.2</td></tr><tr><td>2</td><td>2K</td><td>82.53</td><td>89.37</td><td>58.83</td><td>42.2</td><td>80.98</td><td>94.31</td></tr><tr><td>2</td><td>10K</td><td>87.41</td><td>93.26</td><td>58.75</td><td>91.4</td><td>80.85</td><td>92.55</td></tr><tr><td rowspan="5">AC-High</td><td>/</td><td>/</td><td>60.08</td><td>67.19</td><td>18.04</td><td>62.1</td><td>72.86</td><td>81.46</td></tr><tr><td>1</td><td>2K</td><td>69.41</td><td>74.89</td><td>49.04</td><td>39.52</td><td>33.56</td><td>80.15</td></tr><tr><td>1</td><td>10K</td><td>67.56</td><td>74.04</td><td>49.65</td><td>29.84</td><td>20.45</td><td>69.93</td></tr><tr><td>2</td><td>2K</td><td>70.44</td><td>75.31</td><td>48.8</td><td>57.26</td><td>34.91</td><td>79.42</td></tr><tr><td>2</td><td>10K</td><td>68.08</td><td>71.68</td><td>49.65</td><td>71.77</td><td>24.06</td><td>72.12</td></tr></table>

（3）在AndroidControl-High测试集上，增大K值则带来了性能增益。以  $\mathrm{K} = 1$  为例，数据规模从2K扩展到10K时，Click类准确率略有下降（ $74.89\%$  降至  $74.04\%$ ），但整体准确率依然保持较高水平（ $69.41\%$  对 $67.56\%$ ）。而在  $\mathrm{K} = 2$  时，无论是2K还是10K数据规模，相比  $\mathrm{K} = 1$  均取得更好表现。这表明在更复杂的任务场景下（以任务指令为输入），VLM通过跨越一张或多张GUI状态进行动作推理，更有利于捕捉与任务指令相关的意图。

（4）综合来看，数据规模与K值在不同测试集上具有互补作用：在任务粒度较细、状态转移更直接的AndroidControl-Low中，扩大数据规模是性能提升的主要来源；而在任务粒度更高、推理链更复杂的AndroidControl-High中，适度增大K值能够发挥更大作用。这表明，在后续训练过程中，应结合任务特性灵活选择K值，并配合足够规模的数据，才能在不同类型的GUI任务中实现最佳性能。

# 3.7 本章小结

本章围绕移动端屏幕助手在数据与能力上的瓶颈展开研究。现有开源移动端GUI数据集规模有限、应用覆盖不足，且缺乏完整的视图层级信息，难以支撑端侧多模态模型的高效训练与推理。为解决这一问题，本章提出了基于LLM强化的自动化应用遍历方法与SoC集群的高保真数据采集方案，构建了覆盖广泛应用、同时包含截图与视图层级的MobileViews数据集。基于该数据集的实验结果表明，模型在多项下游任务上的性能得到显著提升，验证了大规模高质量数据对于端侧模型训练的重要价值。在此基础上，本章进一步提出基于自监督强化学习的K步界面转移任务，充分利用未标注的GUI操作轨迹，结合GRPO训练方法，增强模型在界面变化感知与动作决策上的能力。实验表明，该方法能够有效克服传统监督微调的局限，使模型在复杂任务场景中展现出更强的感知与决策能力。

综上所述，本章的研究不仅提出了高效可扩展的移动端数据收集方法，填补了高质量大规模数据资源的空缺，还探索了自监督强化学习在移动端GUI智能体中的应用路径，为构建更智能、更可靠的屏幕助手提供了坚实基础。

# 第四章 面向GUI任务的高效测试时扩展方法

# 4.1 本章概述

近年来，LLM和VLM快速发展，测试时扩展方法也快速兴起。测试时扩展通过在推理阶段分配更多计算资源，例如生成更长的链式思维、采样更多候选推理路径或利用外部搜索机制，使模型能够在不改变参数的前提下提升性能[20,121]。已有研究表明，测试时扩展方法在数学推理、代码生成以及跨模态问答等领域显著提高了模型的准确性和鲁棒性[20,111,112]。这表明，合理利用测试时的额外计算，可以在训练数据与模型规模之外开辟新的性能提升空间。然而，现实应用中的复杂性仍带来诸多挑战。移动端GUI任务要求智能体直接感知复杂多样的屏幕界面，理解用户指令，并通过多步交互实现目标。这一过程兼具环境感知的不确定性与决策规划的多阶段性，远比静态推理任务复杂。移动端GUI界面通常具有复杂的界面布局、多样的功能，且需要多步推理与连续交互才能实现用户目标。因此，现有不具备推理或扩展机制的GUI智能体往往在真实环境下表现不足，成功率难以满足实际应用需求[34,103,155]。如何在不增加训练成本的情况下，仅依赖测试时扩展方法来提升移动端GUI智能体的表现，成为亟待解决的科学问题。

在模型层级，推理型 VLM 通过基于思维链的推理过程，在模型推理阶段分配额外计算，有望改善任务理解和行动决策。本章首先针对这一问题开展了实证研究，系统比较了两对商用模型（如 Gemini 2.0 Flash[54]与 Claude 3.7 Sonnet[55]）在推理与非推理模式下是否能提升移动端 GUI 智能体性能。实证研究结果表明，在静态基准（如 ScreenSpot[98]和 AndroidControl[53]）上，推理模型相较于非推理版本仅带来有限提升，甚至在部分配置下出现显著下降。这主要源于静态基准测试设计上的局限（如任务指令模糊、缺乏多样化交互路径），以及推理模型在屏幕理解环节的不足。相较之下，在交互式环境 AndroidWorld[34]中，推理模型在部分场景下展现出更强优势。例如，Claude 3.7 Sonnet 的推理版本将任务完成率提升至  $64.7\%$  ，创下当前最优水平；但也存在如 Gemini 2.0 Flash 推理版本导致性能下降的情况。另一方面，推理模型显著增加了计算与延迟开销：在所有测试基准中，推理过程导致输出令牌（Token）数量至少增加 3 倍，最高可达 14 倍，进而带来更高的资源消耗与响应延迟。综上所述，VLM 的推理能力在移动端 GUI 智能体中既非银弹，也非无效手段。在真实交互环

境下它展现出一定潜力，但在静态数据集和部分模型配置中仍表现出不稳定性与高成本问题，因此亟需进一步研究如何动态调用推理能力，以及如何在性能提升与资源开销之间实现平衡。

除了在模型层级使用具有推理能力的 VLM 支撑移动端 GUI 智能体，另一层级的工作是在移动端 GUI 智能体的工作流引入测试时扩展方法。这些测试时扩展方法通常包括在智能体提示词中加入历史记忆[38,40]、增加 GUI 智能体和环境交互的步数限制[86,113,123]和在执行流程中加入自我反思与自我改进模块[32,33,36,108,122]。现有移动端 GUI 智能体通常采用顺序的“感知-决策-执行”循环，在每一步生成并执行一个动作，直至任务完成。OpenAI 的 Computer-Using Agent[86]和 Claude 的 Computer Use[85]等多个系统均采用这种逐步执行的方式。然而，这种顺序执行的方式在面对复杂或未见过的任务时可能效率低下，并容易陷入卡顿。如图 4-1(a)所示，GUI 智能体执行流程是一种深度优先的试错策略：一旦某步选错操作，后续需要回退至之前状态，并重新进行动作决策。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/91e893cb371a3ad58d27500f412134ff825e2e992903f8d92d52f7851c29c394.jpg)



(a) 顺序执行


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/c7bb5a33fb05022befb0cd970e08f9be3f246556dc75df03df757bcc54dd7c6b.jpg)



(b) 并行推测执行



图4-1GUI智能体顺序执行与并行推测执行示意图


尽管在工作流层级引用测试时扩展方法能够提升GUI智能的任务完成率，但同时也显著增加了任务完成时延。具体而言，AndroidWorld[34]中的M3A-SoM智能体集成了反思模块，即在每次动作执行后，调用VLM以判断当前移动端环境状态是否与动作执行后的预期状态相匹配；若二者不匹配，则触发回溯或重试机制。该方法虽然有效提升了任务完成率，但也显著增加了任务完成时延。表4-1展示了在AndroidWorld的简单任务子集上进行实验，不同GUI智能体的任务完成时延与任务完成率。结果显示，M3A-SoM集成的反思模块将VLM调用次数增加了  $81.9\%$  ，任务完成时延提高了  $55.5\%$  。进一步地，在M3A-SoM基线方法上应用顺序推测执行（即将本章后文提出的并行推测执行方法串行化）后，任务完成时延进一步增加了  $55.8\%$  。在实际应用场景中，过高的任务完成时延会显著降低用户体验。因此，本章提出应重新审视此类范式，从而提高移动端GUI智能体在时间效率和计算资源利用方面的整体性能。


表 4-1 移动端 GUI 智能体任务完成时延与完成率


<table><tr><td>GUI 智能体</td><td>VLM调用次数</td><td>VLM调用时延（秒）</td><td>总时延（秒）</td><td>任务完成率</td></tr><tr><td>M3A-SoM-反思模块</td><td>577</td><td>3228.51</td><td>6785.97</td><td>44.26%</td></tr><tr><td>M3A-SoM</td><td>1050</td><td>6969.01</td><td>10551.74</td><td>55.74%</td></tr><tr><td>M3A-SoM+顺序推测执行</td><td>1444</td><td>9760.11</td><td>16439.43</td><td>57.38%</td></tr></table>

为解决该问题，本章提出了一种新的范式——并行推测执行（PSE），以减轻由顺序且冗长的顺序执行循环带来的时间开销。并行推测执行范式允许智能体在多个环境中并行地进行交互。如图4-1(b)所示，在每一个循环中，智能体在动作生成阶段会产生多个可能有助于任务完成的候选动作。随后，这些动作会在当前环境的多个复制得到的状态相同的移动端环境上同时执行。不同的动作可能会将这些复制环境引导至不同的状态；智能体会保留一个或多个产生良好状态的环境，并进入下一轮循环。借助这一范式，GUI智能体能够并行地推测多种潜在的正确操作，从而加快任务完成速度，并有望提高任务成功率。

然而，在真实移动端环境中实现并行推测执行面临多项技术挑战，主要包含如下三方面：（1）冗余动作采样。从相同GUI状态中采样多个候选动作时，智能体往往会生成语义相似甚至等效的操作，导致多个分支通向相同的状态。例如，Fan等人指出，未经协调的并行GUI探索可能产生高达  $42\%$  的重复状态[156]。这种冗余行为会极大浪费时间与算力，因此需要机制确保候选动作之间具有足够的差异性。（2）并行执行的前提是能够将当前环境精确复制成多个可操作的实例。然而，复制完整的应用或系统状态不仅资源消耗大，而且现有操作系统并不提供原生支持，导致环境复刻在实践中难以高效实现。（3）在多个推测分支并行推进的过程中，智能体需判断哪些分支更有可能完成任务，以避免状态空间急剧膨胀。虽然可以利用VLM对各个分支状态进行评分或排序[41-44,116]，但这种方法计算代价高，且容易被表面视觉特征误导，准确性不足。因此，有必要设计一种成本低、效果稳定的分支筛选策略，以及时剔除无效路径。

本研究构建了首个支持并行推测执行的移动端GUI智能体。为了解决上述三个挑战，本章提出如下方法：（1）动作采样与意图驱动的去沉重机制：在每轮动作生成中，智能体不仅生成多个候选动作，同时要求VLM为每个动作明确标注其意图。随后，使用VLM对这些“意图-动作”对进行分析，识别并剔除可能通向相同状态的冗余动作，从而保留语义多样性强、效率更高的操作集合；（2）基于快照的环境复制：为实现并行环境的一致

性，该方法采用Android模拟器提供的快照功能[157]，完整捕获当前设备状态。随后通过OverlayFS提供的写时复制机制，实现模拟器快速复制。复制的模拟器实例加载该快照后，即可在多个并行环境中保持同步起始状态，避免因状态偏差导致实验不一致；（3）基于子任务进度的分支筛选：智能体在任务开始时将任务指令拆分为若干子任务。在每个动作执行后，会检查各分支当前达成的子任务进度。落后于最远进度分支的其他分支将被及时裁剪，而无需依赖VLM打分，这种方法更轻量、稳定，也减少了潜在的视觉误导。该分支评估逻辑可自然嵌入任务执行过程中现有的VLM调用流程，几乎不增加额外令牌或时间开销。

本章在AndroidWorld[34]测试平台上对所提出的PSE智能体进行了系统评估，并选用了Gemini2.0Flash[54]和Gemini2.5Flash[158]两个商用模型进行实验。AndroidWorld包含116个具有不同复杂度的GUI交互任务；在全部116个任务中，PSE智能体配合Gemini-2.0-Flash的完成率为  $41.4\%$  ，不仅超越了AndroidWorld提出的M3A-SoM智能体，且后者即使被给予4倍的测试步数预算，在性能上仍处于劣势。更值得注意的是，PSE智能体在保持较低交互步数的前提下，成功率比M3A-SoM智能体在3倍预算配置下提高了  $5.2\%$  ，且未额外增加令牌消耗。在进一步使用Gemini-2.5-Flash进行的分析中，针对两个智能体都成功完成的54个任务，PSE在其中  $74\%$  的任务上取得胜出或表现持平。这一结果进一步验证了PSE在实际移动端GUI自动化任务中的时间效率优势。

本章的主要贡献如下：

（1）本章面向模型层级的测试时扩展进行了实证研究，评估了VLM的推理能力在移动端GUI智能体中的作用，揭示其增益有限与局限性，并通过错误分析为后续研究提供了启示。

（2）本章提出了一种适用于移动端GUI智能体面向工作流层级的测试时计算扩展新范式——并行推测执行，有效提升了当前以顺序探索为主的方法的效率。

（3）本章实现了基于PSE的GUI智能体，并在真实移动端GUI自动化任务上完成了性能评估。通过对比实验与消融研究验证，PSE智能体在减少VLM令牌消耗的同时仍能保持较高任务完成率，并在时间效率方面相较于传统顺序执行方法取得显著提升。

# 4.2 基于推理模型的测试时扩展方法与实证研究

# 4.2.1 问题描述

推理能力已成为近年来大语言模型与视觉语言模型发展的关键突破

点。通过长链式思维和测试时扩展方法，模型能够显著提升在复杂任务中的表现。OpenAI于2024年推出的ol系列模型[159]率先在商用系统中引入了测试时扩展技术，其核心理念是“先思考，再作答”。在回答阶段之前，这类模型会进入“思考阶段”，对用户请求进行解析，并生成详细的CoT[121]以完成自我反思和推理。通过在该阶段分配更多的计算资源，模型能够给出更为准确的最终答案，并在复杂数学问题、代码生成以及多模态推理任务中表现出显著优势[20,111,112]。随着这一趋势的发展，具备强推理能力的商用LLMs/VLMs陆续涌现，包括DeepSeek-R1[111]、Gemini 2.0 Flash Thinking[54]、Claude 3.7 Sonnet[55]和Grok 3 Beta[160]等。

然而，基于推理模型的测试时扩展方法能否应用在在移动端GUI智能体任务上仍缺乏深入探讨。特别是移动端GUI任务伴随着复杂的界面布局、多样的应用功能，以及为实现用户指令所需的多步推理与交互。图4-2展示了Gemini 2.0 Flash推理模型在移动端GUI自动化任务中的输出。当用户提出“Set my DM Spam filter to ‘Do not filter direct messages’ on Discord app”的任务时，模型首先会显式输出任务指令，并用自然语言描述当前界面的状态。随后，它会分析任务需求与界面可见元素之间的关系，推断出最相关的元素与操作，例如“点击名为You的标签”。在确定操作后，模型还会对目标、GUI状态及所选操作进行反思与验证，最终在回答中清晰地给出经过确认的动作。通过这一推理链条，智能体能够建立对任务与环境的深度理解，并据此输出更为可靠的交互决策。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/80fe8747981dc38f75f935a55214e787a72de1e1be9045964571c7093794f6e8.jpg)


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/13e85ac8ea5fe178888f4b175f18bbab93752588e7e415a8e061bcf9a9b4d6de.jpg)



图4-2 Gemini 2.0 Flash推理模型输出示例


尽管这一类推理过程展现出良好潜力，目前仍缺乏系统性的实证研究

来验证 VLM 的推理能力对移动 GUI 智能体的实际益处。现有工作多局限于在高度受控的环境下使用 CoT 提示词进行实验[38,40], 难以直接反映真实场景中的复杂性与不确定性。因此, 本研究内容旨通过对具备内在推理能力的 VLM 进行大规模、系统性的实证评估, 探讨推理是否能够有效提升移动 GUI 智能体在真实场景中的性能表现。

# 4.2.2 实证研究方法

本节首先介绍所选用的基准测试数据集，并解释其选择背后的原因。接着详细说明所使用的 VLM，包括具备推理能力和不具备推理能力的模型，以及基于这些模型构建的移动 GUI 智能体。最后，本节概述了用于评估其性能的指标。

基准测试选择。为了评估移动端GUI智能体，选择合适的基准数据集至关重要。已有研究曾大量使用静态数据集对移动GUI智能体进行测试[34,53,88,98,161]，但这些方法在处理真实移动端环境方面效率较低[34,110]。因此，本节实证研究结合了具有代表性的静态和交互式基准数据集，具体如下：（1）ScreenSpot[98]是一个GUI定位数据集，包含600多个GUI界面和超过1200条任务指令，旨在评估VLM的基础定位能力。定位任务要求VLM在给定一条任务指令情况下，需要识别对应的GUI组件并输出其坐标。该基准的目标是验证推理能力是否能提升移动GUI智能体的基础定位能力。本节的实验使用了ScreenSpot中的mobile子集。（2）AndroidControl[53]是一个由Google提出的静态数据集，用于训练和评估移动GUI智能体，包含800多个Android应用中的  $14\mathrm{K}+$  任务。该数据集与以往静态数据集的关键区别在于其高质量的任务标注、全面的GUI表征方式以及涵盖了单步任务指令，有助于评估不同的VLM提示策略。本节实验遵循AndroidControl的实验设置与评估方法，并随机选择500个任务来近似完整测试集的效果。（3）AndroidWorld[34]是由Google提出的交互式移动端GUI智能体评估基准。该数据集通过预定义的函数调用访问应用内部状态进行任务完成情况的验证，从而实现更准确的评估。本节采用AndroidWorld的全部116个任务，以评估现有移动GUI智能体在真实场景下的能力。

VLM与智能体选择。VLM与提示词共同构成了移动端GUI智能体。本节实验使用两对VLM模型——Gemini2.0Flash[54]与Claude3.7Sonnet[55]，每对模型均包括一个基础版本（不具备推理能力）和一个具备推理能力的版本。此外，实验中额外使用了不具备推理能力的GPT-4o[21]作为性能参考。本节研究中所构建的移动端GUI智能体基于上述VLM，但其提示词设计各不相同，主要使用如下来自己有研究中发布或开源的智能

体：（1）在ScreenSpot任务中，智能体要求输出三种格式的GUI组件定位结果：归一化边界框（如[0.08,0.688,0.92,0.735]），像素级边界框（如[127,34,235,978])和归一化中心点（如[255,370]）。（2）在AndroidControl任务中，实验使用ER提示词作为智能体实现[53]，该提示词以任务指令和之前的动作列表为输入。此外，本节实验还对输入进行了三种变体的修改：仅使用任务指令，仅使用单步动作指令和同时使用任务和单步动作指令。（3）在AndroidWorld任务中，使用其工作开源的三种GUI智能体[34]：使用Set-of-Mark方式增强屏幕截图作为GUI表征的M3A-SoM，使用VH层级信息增强屏幕截图作为GUI表征的M3A，和使用VH层级信息作为GUI表征的T3A。

评估指标。在静态测试基准中，本节实验分别报告ScreenSpot的定位准确率和AndroidControl的动作预测准确率。其中，AndroidControl的评估方法遵循其原始论文中的设定[53]。在AndroidWorld中，实验评估端到端的任务完成率。

# 4.2.3 实证研究结论与分析

本评估旨在探究具有推理能力的VLM是否相比于无推理能力的VLM在移动端GUI任务性能上是否有提升。本节分析首先从整个基准测试层级分析平均任务完成率；接着从任务层级分析不同VLM下的任务完成情况；最后，对推理过程中产生的错误类型进行分类整理，并得出启示。

# 4.2.3.1 基准测试层级结果与分析

在静态基准测试中，实验结果（见表4-2和表4-3）显示：具有推理能力的VLM并未普遍提升移动端GUI智能体的表现，反而在部分配置下显著降低了性能。以ScreenSpot为例，实验在不同VLM模型和定位输出格式下评估GUI定位准确率。结果表明，在使用归一化边界框或像素级边界框时，推理过程通常会降低准确率。例如，Gemini Thinking和Claude Thinking在归一化边界框上的准确率分别下降了  $28.7\%$  和  $16.1\%$  。但在归一化中心点的设置中，推理效果因模型而异：Gemini Thinking准确率下降 $29.7\%$  ，而Claude Thinking反而提升了  $8.6\%$  。这说明推理的有效性高度依赖于具体模型和任务类型，部分情形下（如Claude的中心点定位）可能确实带来帮助。

在AndroidControl任务中（见表4-3），引入推理能力的VLM对准确率提升非常有限。Gemini2.0Flash的推理模式仅提升  $0.75\%$  的动作预测准确率，ClaudeThinking提升  $2.3\%$  。综上，在两个静态GUI基准任务中的评估结果表明，集成推理能力的VLM并不能稳定提高GUI智能体的性能，

甚至在部分场景中起到了负面作用。


表 4-2 ScreenSpot 测试集上定位任务准确率


<table><tr><td rowspan="2">VLM</td><td colspan="3">定位任务输出格式</td></tr><tr><td>格式化 BBox 坐标</td><td>BBox 坐标</td><td>格式化 中心点</td></tr><tr><td>GPT-4o</td><td>33.5%</td><td>4.4%</td><td>27.7%</td></tr><tr><td>Gemini 2.0 Flash</td><td>50.2%</td><td>14.9%</td><td>53.0%</td></tr><tr><td>Gemini 2.0 Flash</td><td>21.5%</td><td>13.6%</td><td>23.3%</td></tr><tr><td>Thinking</td><td>(28.7%↓)</td><td>(1.3%↓)</td><td>(29.7%↓)</td></tr><tr><td>Claude 3.7 Sonnet</td><td>27.5%</td><td>2.8%</td><td>6.4%</td></tr><tr><td>Claude 3.7 Sonnet</td><td>11.4%</td><td>2.8%</td><td>15.0%</td></tr><tr><td>Thinking</td><td>(16.1%↓)</td><td>(-)</td><td>(8.6%↑)</td></tr></table>


表 4-3 AndroidControl 测试集上动作预测任务准确率


<table><tr><td rowspan="2">VLM</td><td colspan="4">移动端GUI智能体输入</td></tr><tr><td>任务指令</td><td>单步动作指令</td><td>任务+单步动作指令</td><td>任务指令+历史动作</td></tr><tr><td>GPT-40</td><td>39.2%</td><td>66.4%</td><td>68%</td><td>44.8%</td></tr><tr><td>Gemini 2.0 Flash</td><td>40.8%</td><td>64.8%</td><td>62.8%</td><td>46%</td></tr><tr><td>Gemini 2.0 Flash</td><td>42.6%</td><td>65.6%</td><td>63.6%</td><td>45.6%</td></tr><tr><td>Thinking</td><td>(1.8%↑)</td><td>(0.8%↑)</td><td>(0.8%↑)</td><td>(0.4%↓)</td></tr><tr><td>Claude 3.7 Sonnet</td><td>42.4%</td><td>59.4%</td><td>58.2%</td><td>43%</td></tr><tr><td>Claude 3.7 Sonnet</td><td>43.6%</td><td>63.8%</td><td>60.8%</td><td>44%</td></tr><tr><td>Thinking</td><td>(1.2%↑)</td><td>(4.4%↑)</td><td>(2.6%↑)</td><td>(1%↑)</td></tr></table>

在AndroidWorld平台上，表4-3结果显示，不同模型组合的表现差异显著。对于Gemini2.0Flash模型，启用推理后任务完成率均下降，其中T3A(a11y tree)智能体下降幅度最大，为  $3.5\%$  。该现象说明Gemini2.0Flash的推理能力在交互式环境下未能有效提升智能体性能。相比之下，Claude3.7Sonnet的推理能力在AndroidWorld上带来了平均  $6.3\%$  的性能提升，表现出正向作用。尤其值得注意的是，在M3A(SoM)配置下，Claude3.7Sonnet的推理模式将任务完成率提升了  $9.5\%$  ，任务完成率达到  $64.7\%$  ，达到了AndroidWorld环境下的最优水平。该实验结果展现了VLM的推理能力在特定VLM与智能体设计配置下的显著潜力。


表 4-4 AndroidWorld 测试集上端到端任务完成率


<table><tr><td rowspan="2">VLM</td><td colspan="3">移动端 GUI 智能体设计</td></tr><tr><td>M3A (SoM)</td><td>M3A (a11y tree)</td><td>T3A (a11y tree)</td></tr><tr><td>GPT-4o</td><td>44.8%</td><td>23.3%</td><td>46.6%</td></tr><tr><td>Gemini 2.0 Flash</td><td>35.3%</td><td>25.9%</td><td>39.7%</td></tr><tr><td>Gemini 2.0 Flash</td><td>32.8%</td><td>23.2%</td><td>36.2%</td></tr><tr><td>Thinking</td><td>(2.5%↓)</td><td>(2.7%↓)</td><td>(3.5%↓)</td></tr><tr><td>Claude 3.7 Sonnet</td><td>55.2%</td><td>44.8%</td><td>54.3%</td></tr><tr><td>Claude 3.7 Sonnet</td><td>64.7%</td><td>50%</td><td>58.6%</td></tr><tr><td>Thinking</td><td>(9.5%↑)</td><td>(5.2%↑)</td><td>(4.3%↑)</td></tr></table>

# 4.2.3.2 任务层级结果与分析

上述章节结果显示，在静态基准测试中，是否启用推理对移动端GUI智能体的表现影响不大，甚至在部分情况下表现更差。在交互式平台AndroidWorld中，推理VLM所带来的性能提升因模型而异，整体幅度有限。此外，此前的结果未细化到具体任务层级，难以判断推理能力是否真正改善了任务表现。因此，本节对各基准中的单个任务进行深入分析，以评估推理VLM的正负效应。本节重点分析两类情况：（1）原本非推理模型无法完成，但加入推理后完成的任务（表4-5和表4-6中的  $\mathbf{F}\rightarrow \mathbf{T}$  ），可体现推理带来的潜在益处；（2）原本能完成的任务在使用推理后失败（表4-5和表4-6中的  $\mathbf{T}\to \mathbf{F}$  ），则反映推理能力的不足。


表 4-5 Gemini 2.0 Flash 推理与非推理模型任务完成率


<table><tr><td rowspan="2">测试集</td><td rowspan="2">实验设置</td><td colspan="4">Gemini 2.0 Flash</td></tr><tr><td>T→F</td><td>F→T</td><td>T→T</td><td>F→F</td></tr><tr><td rowspan="3">ScreenSpot</td><td>归一化边界框</td><td>36.06%</td><td>7.37%</td><td>14.14%</td><td>42.43%</td></tr><tr><td>像素级边界框</td><td>11.55%</td><td>10.16%</td><td>3.39%</td><td>74.90%</td></tr><tr><td>归一化中心点</td><td>35.26%</td><td>5.58%</td><td>17.73%</td><td>41.43%</td></tr><tr><td rowspan="2">Android</td><td>任务指令</td><td>8.42%</td><td>10.22%</td><td>32.46%</td><td>48.9%</td></tr><tr><td>单步动作指令</td><td>5.0%</td><td>5.8%</td><td>59.8%</td><td>29.4%</td></tr><tr><td rowspan="2">Control</td><td>任务+单步动作指令</td><td>6.8%</td><td>7.6%</td><td>56.0%</td><td>29.6%</td></tr><tr><td>任务指令+历史动作</td><td>9.2%</td><td>8.8%</td><td>36.8%</td><td>45.2%</td></tr><tr><td rowspan="3">Android World</td><td>M3A-SoM</td><td>12.17%</td><td>10.43%</td><td>22.61%</td><td>54.78%</td></tr><tr><td>M3A (a11y tree)</td><td>10.43%</td><td>6.09%</td><td>15.56%</td><td>67.83%</td></tr><tr><td>T3A (a11y tree)</td><td>12.28%</td><td>9.65%</td><td>27.19%</td><td>50.88%</td></tr></table>


表 4-6 Claude 3.7 Sonnet 推理与非推理模型任务完成率


<table><tr><td rowspan="2">测试集</td><td rowspan="2">实验设置</td><td colspan="4">Claude 3.7 Sonnet</td></tr><tr><td>T→F</td><td>F→T</td><td>T→T</td><td>F→F</td></tr><tr><td rowspan="3">ScreenSpot</td><td>归一化边界框</td><td>17.93%</td><td>1.79%</td><td>9.56%</td><td>70.72%</td></tr><tr><td>像素级边界框</td><td>0.40%</td><td>0.40%</td><td>2.39%</td><td>96.81%</td></tr><tr><td>归一化中心点</td><td>0.99%</td><td>9.56%</td><td>5.38%</td><td>84.06%</td></tr><tr><td rowspan="4">Android Control</td><td>任务指令</td><td>3.4%</td><td>4.6%</td><td>39.0%</td><td>53.0%</td></tr><tr><td>单步动作指令</td><td>1.0%</td><td>5.4%</td><td>58.4%</td><td>35.2%</td></tr><tr><td>任务+单步动作指令</td><td>3.2%</td><td>5.8%</td><td>55.0%</td><td>36.0%</td></tr><tr><td>任务指令+历史动作</td><td>5.8%</td><td>6.8%</td><td>37.2%</td><td>50.2%</td></tr><tr><td rowspan="3">Android World</td><td>M3A-SoM</td><td>0.86%</td><td>10.43%</td><td>54.31%</td><td>34.48%</td></tr><tr><td>M3A (a11y tree)</td><td>6.03%</td><td>11.21%</td><td>38.79%</td><td>43.97%</td></tr><tr><td>T3A (a11y tree)</td><td>5.17%</td><td>9.48%</td><td>49.14%</td><td>36.21%</td></tr></table>

实验观察显示，引入推理能力后，任务完成情况出现明显波动。首先，在原本可以顺利完成的任务中，引入推理后反而出现大量失败。例如，在ScreenSpot归一化边界框任务中，Gemini和Claude模型在使用推理后分别有  $36\%$  和  $18\%$  的任务失败；在AndroidControl中，Gemini Thinking和Claude Thinking分别有平均37个和16个任务失败。这表明推理过程可能干扰原本已学会的判断，降低了智能体的稳定性和准确性。

其次，推理 VLM 在原本无推理模型无法完成的任务中确实带来了中等程度的性能提升。以 Gemini Thinking 为例，其在三个基准中的平均提升为  $7.7\%$ 、 $8.1\%$  和  $8.8\%$ 。然而，除 Claude Thinking 在 AndroidWorld 的 M3A-SoM 配置中表现出明显优势外，其余情况下的提升不足以弥补整体任务完成率的下降。因此，从整体评估来看，目前的推理 VLM 在提升移动端 GUI 智能体性能方面仍存在一定负面影响，尚需进一步优化。

# 4.2.3.3 错误分析

本节针对推理 VLM 导致的“由成到败”（ $\mathrm{T} \rightarrow \mathrm{F}$ ）任务进行了深入分析。结果显示，在 ScreenSpot 任务中，几乎所有失败均源于定位坐标预测错误，凸显了当前 VLM 推理机制在 GUI 定位上的重大局限，而这一点正是移动端 GUI 智能体的核心需求。相比之下，在 AndroidControl 中，因引入了更完善的智能体设计（结合视图层级信息与屏幕截图作为输入），定位错误几乎被消除，使得推理过程能更精确地捕捉 GUI 元素的位置。但即便如此，仍存在大量任务在推理模式下失败，而这些任务在非推理模式下本可成功完成。


表 4-7 推理流程引发失败任务人工归因与分类


<table><tr><td>错误来源</td><td>错误类型</td><td>解释</td><td>比例</td></tr><tr><td rowspan="3">测试集</td><td>评估方法薄弱</td><td>各种假阴性动作仍有可能完成任务</td><td>47.8%</td></tr><tr><td>静态GUI输入局限</td><td>GUI智能体只接收单张GUI表征,无上下文信息</td><td>14.9%</td></tr><tr><td>任务指令不清晰</td><td>任务指令不明确或模棱两可</td><td>11.9%</td></tr><tr><td rowspan="3">VLM</td><td>GUI理解局限</td><td>VLM无法理解GUI上下文</td><td>10.5%</td></tr><tr><td>推理-回答不一致</td><td>推理过程正确,但回答与推理结论不一致</td><td>8.9%</td></tr><tr><td>其他错误</td><td>定位错误,推理错误,幻觉等</td><td>6.0%</td></tr></table>

为了进一步剖析问题，本节对Claude与Claude Thinking在所有配置下的  $\mathrm{T} \rightarrow \mathrm{F}$  任务进行人工归因与分类（共67个任务），并在表4-7中呈现错误类型、解释及其分布比例。选择Claude模型进行实验的原因是其API能够完整输出推理链条，方便诊断，而Gemini Thinking目前尚不支持这一功能。结果表明，基准相关错误占比超七成。其中，“评估方法不足”占 $47.8\%$  ，表现为多种合理操作能完成任务，但基准只接受单一答案，导致其他操作被判错，这是静态基准的固有限制[34,110]。其次，“静态GUI输入局限”占  $14.9\%$  ，由于基准仅提供单一界面，推理VLM无法确认先前状态是否满足指令，因此可能尝试回溯验证，从而生成错误结果。此外，部分任务指令语义模糊，连人类也难以理解，使其并不适合作为GUI智能体的评估任务。VLM相关错误占  $25.4\%$  。其中最常见的是“GUI理解有限”，即推理阶段模型误读界面语境，导致错误响应。更棘手的是，即使模型在推理中推导出正确结果，推理结论与最终输出仍存在不一致现象，进而影响性能。此外实验中还观察到少量定位错误、推理错误和幻觉现象[120,162,163]。

# 4.2.3.4 启示

根据本节实验结果可知，尽管推理机制有助于VLM更深入地理解任务与界面，但在静态基准中，由于评估方法的固有限制，这种能力未能稳定转化为性能提升。针对未来的研究与开发，本节提出以下几点启示：

对VLM而言：亟需基于大规模、高质量的标注数据集进行训练，以增强其定位和界面理解能力。同时，需要结合面向应用场景的强化学习奖励函数，以减少推理过程与最终输出结果的不一致性。

对移动端GUI智能体而言：若要充分发挥推理VLM的优势，必须在输入中融入更丰富的上下文信息，例如结合外部工具或在系统提示中提供全局性信息。否则，推理输出可能缺乏有效性。同时，引入自适应推理机

制也至关重要，它能够有效降低长时延和高令牌开销，确保智能体在实际应用中的可行性。

对基准测试而言：静态基准受限于设计缺陷，因此交互式基准应成为评估的重点。相比之下，真实交互环境可提供更充分的上下文信息，使智能体能够开展更细粒度的推理。下一节将在真实交互环境AndroidWorld中，探究工作流层级的测试时扩展方法。

# 4.3 基于并行推测执行测试时扩展方法

# 4.3.1 问题描述

基于VLM的移动端GUI智能体普遍采用“感知-决策-执行”的循环范式，即通过获取屏幕截图，推理下一步动作，并逐步执行直至任务完成。这种逐步探索的方法虽具备普适性，但在复杂或未知任务中容易陷入低效的深度优先试错：一旦动作错误，智能体需要在后续循环中回溯纠正，从而导致执行延迟、交互轮次增加以及用户体验下降。现有的顺序扩展方法虽然能在增加计算预算的前提下提升成功率，但代价高昂且效率低下。因此，亟需一种能够在推理与执行过程中更高效扩展计算资源的新范式。

为此，本节提出并行推测执行范式，允许GUI智能体在每轮迭代中生成多种候选动作，并在多个环境副本中并行执行。这一机制能够同时探索潜在的正确路径，避免顺序试错的时间浪费，并提升任务完成率。然而，将PSE应用于真实移动端环境仍面临多重挑战：其一，同一GUI上采样的多个候选动作可能语义等价，导致大量冗余探索；其二，环境状态复制代价高昂，且现有移动系统缺乏直接支持；其三，多个并行分支会迅速膨胀，如何高效判断和保留有前景的分支成为关键。因此，本节针对上述挑战，提出了意图感知的高效并行探索方法，以及基于任务进度自感知的路径剪枝策略，并在AndroidWorld平台上系统评估其有效性。

# 4.3.2 系统架构设计与执行流程

图4-3展示了PSE驱动的移动端GUI智能体的工作流程，其执行流程可分为以下几个阶段：首先，GUI智能体将用户的自然语言任务拆解为有序的子任务清单，以此作为任务进度的标记。在每一轮迭代中，智能体结合任务指令与当前GUI状态生成多个候选的<意图，动作>对，并通过意图感知过滤器去除语义重复的动作，仅保留互不等价的候选动作。随后，这些候选动作被分别分配至复制的GUI环境中并行执行，从而同时展开多条潜在探索路径。随着执行推进，各分支会记录已达成的子任务进度索引，而路径剪枝策略则会淘汰落后分支，确保计算资源集中在最有前景的探索方向。上述循环不断进行，直至任务被完成或达到预设步数上限。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/59f198dd0cd4ef851143161517f058417f77dd34a807d8df7e4ca8e643980d29.jpg)



图4-3并行推测执行工作流程


算法4-1系统性地描述了并行推测执行智能体的执行机制。整体流程以用户自然语言任务为输入，并通过以下几个阶段完成：首先，智能体在初始环境中启动目标应用，并将整体任务分解为有序的子任务序列，以此作为进度衡量的显式标记。随后，系统在初始状态下生成初始分支，并将其置入候选分支集合中，进入循环执行阶段。在每一轮循环中，智能体会并行处理当前所有候选分支。具体而言，智能体首先感知每个分支的GUI状态，并基于任务指令生成最多K个候选的<意图，动作>对。所有候选动作经过意图感知去重步骤，从而消除语义冗余，仅保留功能上互不等价的候选动作(5-7行)。之后，每个候选动作均在复制的环境实例中独立执行，形成新的探索分支（8-13行）。对于新生成的分支，系统首先检查该动作是否直接完成任务：若任务达成，则将该分支加入完成集合并终止循环（14-16行）；若动作不可行或超出预设步数上限，则该分支被丢弃。若分支仍然有效，则智能体调用反思模块，对比动作前后的环境状态，结合任务描述与子任务列表，判定子任务进度是否推进，并相应更新分支的子任务索引。所有通过验证的分支被加入候选集合，以备后续迭代（16-20行）。迭代结束阶段，系统依据子任务索引对所有候选分支进行筛选，仅保留已达到最大子任务进度的分支集合，借此实现路径剪枝与计算资源的动态集中（25-26行）。循环过程持续进行，直至某一分支成功完成任务，或所有分支均终止。最终，算法根据完成集合是否为空，返回任务执行结果为完成（success）或失败（failed）。

# 算法4-1PSE驱动的GUI智能体的工作流程

输入： 任务指令  $\mathcal{T}$  ，最大执行步数  $B$  ，每个屏幕动作采样数  $K$

输出： 任务执行结果：成功（completed）或失败（failed）

初始化：1. 启动基础执行环境  $env_0$  至初始化应用状态

2. 将任务指令  $\mathcal{T}$  分解为有序的子任务列表  $S = [s_{1}, s_{2}, \ldots, s_{m}]$

3. 创建初始执行分支  $b_0 \gets (\mathrm{env}_0, \mathrm{subtask\_idx} = 1, \mathrm{steps} = 0)$

4. 设置前沿集合  $\mathcal{B} \gets \{b_0\}$ ，设置已完成集合Completed  $\leftarrow \emptyset$


续算法4-1PSE驱动的GUI智能体的工作流程


1: while Complete  $=\varnothing$  and  $\mathcal{B}\neq \emptyset$  do  
2:  $C\gets \emptyset$    
3: terminate  $\leftarrow$  false;  
4: foreach  $b\in \mathcal{B}$  in parallel do  
5: 从分支b的环境b/env中观测当前GUI状态x;  
6:  $\{\bigl ((\mathrm{intent}_{i},a_{i})\bigr)_{i = 1}^{K^{\prime}}\gets \mathrm{SampleActionWithIntents}(x,T,K);$  //动作采样  
7:  $\mathcal{A}\gets \mathrm{IntentDedup}(\{\mathrm{(intent}_{i},a_{i})\})$  ; //动作去重  
8:  $m\gets |\mathcal{A}|$  ; //获取去重后动作数量  
9: for  $j\gets 1$  to m in parallel do  
10: 令  $(\text{intent}_j,a_j)$  是  $\mathcal{A}$  中第  $j$  个元素；  
11: env'  $\leftarrow$  ReplicateEnvInstance(b/env); //复制当前环境  
12: ExecuteOneStep  $\left(\mathrm{env}_i',a_j\right)$  ; 观测新GUI状态  $x_i'$    
13:  $b_j'\gets (\mathrm{env}_j,\mathrm{subtask\_idx} = k_j',\mathrm{steps} = b.\mathrm{steps} + 1)$  ; //创建新执行分支  
14: if CheckTaskCompleted  $(a_j,T)$  then//检测任务已完成  
15: 将新分支  $b_j'$  添加到Completed集合中；  
16: terminate  $\leftarrow$  true; //设置任务终止标记  
16: else if not CheckTaskInfeasible  $(a_j,T)$  and  $b_j'.steps < B$  then//检测任务未完成，且未超过最大步数  
17: advanced  $j\gets$  ReflectProgress  $\left(x,x_j',T,S,\mathrm{int}tent_j\right)$  ; //调用VLM反思当前子任务进度  
18:  $k_j'\gets b.\mathrm{subtask\_idx} + \mathbb{I}[advanced_j]$    
19:  $b_j'.\mathrm{subtask\_idx}\gets k_j'$  ; //更新子任务索引  
20: 将新分支  $b_j'$  添加到集合C中；  
21: if terminate then//如果任务终止标志为true，跳出循环  
22: break  
23: if terminate then  
24: break  
25:  $k_{\max}\gets \max_{b'\in C}b'.\mathrm{subtask\_idx}; / /$  临时集合C中找到最大的子任务索引  
26:  $\mathcal{B}\gets \{b'\in C\mid b'.\mathrm{subtask\_idx} = k_{\max}\}$  ; //只保留最大子任务索引的分支  
27: return Completed  $\neq \emptyset$  ?completed : failed

后续章节将分别介绍PSE系统中的两个核心设计：意图感知的高效并行探索方法和基于任务进度自感知的路径剪枝策略。

# 4.3.3 意图感知的动作采样与去重方法

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/519bd461833ef2771670fba1743071f7e758d75a55cd314de2d0b3edd4127d57.jpg)



图4-4动作采样与VLM驱动的去重示例


图4-4展示了本节所提出子模块涉及到的动作采样和VLM驱动的动作去重的示例。为了加速顺序任务的执行或回溯过程，本节提出一种直接的策略：引导移动端GUI智能体同时探索多个可能推动任务进展的候选动作，而非每次仅生成一个动作。已有多项研究尝试在动作生成阶段进行多动作采样[41-43,116-118]，但在真实环境中逐个执行这些采样动作效率低下。为此，常见做法是通过奖励模型对动作进行排序，仅保留得分最高的前K个候选。然而，实验发现，这种方法常常选中了当前状态下虽然不同、但在后续几步中会收敛至相同状态的动作。为更有效地区分这些动作，本节使用强化动作生成阶段的提示词，使VLM显式生成每个动作背后的意图。并将动作与其意图一并传回VLM执行基于意图的去重处理。该提示词要求VLM仅保留执行效率更高（例如步骤更少）、且目的明确不重复的动作。此策略在减少动作采样偏差的同时，降低了环境复制带来的成本，从而帮助GUI智能体聚焦于最优的执行路径。使用提示词如表4-8所示。


表 4-8 意图-动作采样流程提示词设计


<table><tr><td>{action_generation_request}</td></tr><tr><td>Output an action from the above action spaces in the correct JSON format. Before each action, show the reason why you do that.</td></tr><tr><td>Your answer should look like:</td></tr><tr><td>Reason: ...</td></tr><tr><td>Action: {{{&quot;action_type&quot;: ..., &quot;action-param&quot;: ...}}}</td></tr><tr><td>Your answer:</td></tr></table>

随后，通过调整解码参数来配置VLM的API调用，以采样多个动作。然而，即便生成了多个候选动作，仍然会出现两类冗余。第一类来自采样

意图-动作对的设计。当从 VLM 采样多个动作时，有些响应的动作字段相同，但解释理由略有不同。这类动作应视为重复。第二类冗余则出现在意图和动作字段均不同的情况下，但这些动作在语义上等价，并最终将环境引导至同一状态。在某些情况下，这些动作追求相同目标并会立即收敛。在另一些情况下，动作可能最初引导至不同的中间状态，但最终仍会收敛。例如，一个动作可能直接在输入框中输入文本，而另一个则先点击 GUI 元素再输入。为减少此类冗余，该方法首先基于动作字段进行多数投票，去除仅在意图上不同的候选。这一步也会过滤掉出现频率较低的动作。接下来，将聚合后的动作及其对应的意图传递给同一个 VLM 进行基于意图的进一步去重。在这一过程中，该方法还提供当前 GUI 状态作为参考，以支持更精确的去重。VLM 被指示仅保留那些最有效率（例如需要更少步骤即可到达目标状态）且目的不同的动作。所使用的提示词如表 4-9 所示。

表 4-9 意图感知的动作去重提示词示例

You are an AI assistant analyzing multiple potential actions on an Android GUI. Your task is to identify actions that are functionally equivalent - meaning they would lead to the same end result or target state, even if they use different approaches.

For example:

- Clicking a text field and then typing text vs. directly typing into a field.

- Clicking a button labeled 'Submit' vs. clicking a button with a checkmark icon having the same function.

For each group of similar actions, select the most efficient one (the one that would accomplish the goal in fewer steps).

The actions are performed on the current screen: {GUI Context}

Candidate Actions:

Action  $\# \{\mathbf{i}\}$  (sampled  $\{\mathbf{n}\}$  times):  $\{\mathrm{action}\}$

Reason: {reason}

Instructions:

1. First, provide your analysis of these actions under 'ANALYSIS:

2. Then, under 'ACTIONS TO KEEP':, list ONLY the action numbers you want to keep Format your response as:

ANALYSIS: [Your reasoning about which actions are duplicates and which are unique]

ACTIONS TO KEEP: [comma-separated list of action numbers to keep]

这一两阶段动作过滤过程缓解了动作采样阶段引入的偏差，并减少了

执行所有采样动作时环境复制的开销。这使得移动端GUI智能体能够专注于最优的执行路径。在实验中，这一设计平均去除了  $28.2\%$  的动作，证明了其有效性。节省的令牌使用量和降低的任务完成时延在实验章节的消融实验中进一步分析。

# 4.3.4 基于快速环境复制的分支并行执行方法

在动作生成阶段结束后，会有一个或多个动作需要执行。但这些动作无法按顺序依次执行，原因在于：（1）重新运行相同任务流程的时间开销过高；（2）即便采用回溯，也难以在不同的执行路径之间保持一致的系统或应用状态。因此，需要一种机制来复制完全相同的任务执行沙箱，从而在严格可比的条件下并行执行不同的动作候选。然而，现有操作系统并不原生支持复制单个应用状态或整个移动系统环境。

本研究聚焦于构建在Android平台上的移动端GUI智能体，其中Android Emulator已经成为广泛采用的实验平台。模拟器提供了快照机制，可以捕获完整的系统状态，包括内存、磁盘镜像以及运行时进程[157]。一种朴素的复制方法是：直接拷贝正在运行的Android虚拟设备（Android Virtual Device, AVD）文件夹，修改其元数据使之被识别为一个新设备，然后基于保存的快照启动模拟器。虽然这种方法在功能上可行，但效率极低：它需要复制数GB级别的目录，带来高磁盘I/O开销，并且每次都要重复支付模拟器启动和快照恢复的成本。微基准测试表明，AVD复制过程（占用6.2GB磁盘空间）在SSD上平均耗时6.6秒，而准备阶段（包括模拟器启动和快照加载）甚至可能超过10秒，这严重拖慢了GUI任务的执行。

为解决上述问题，本节设计了一种优化的复制框架。该框架首先利用Android 模拟器的快照功能，将当前系统状态转储到内存中，而非直接写入磁盘。随后，它将AVD中的可变与不可变组件进行解耦，并利用操作系统级的写时复制机制。核心思想是：将任意一个现有AVD（无论是原始的还是已复制的）视为复制时的基准状态，在此基础上创建新的模拟器实例时仅构建轻量级的覆盖层，而非完整拷贝。具体过程是：首先为选定的基准AVD创建只读视图，并将其作为OverlayFS[164]挂载的下层。这确保了基准AVD在复制过程中保持稳定，同时仍然可以在后续独立演化。然后，每个新的副本分配独立的upper目录和work目录，所有修改都会被重定向到这些目录中，而不影响基准。最后，每个复制的模拟器实例被异步启动并恢复到捕获的快照中。这样，不同的动作候选就能够在完全一致的初始条件下并行执行，同时每个模拟器仍可独立演化并保存新的快照。通过将复制操作从磁盘拷贝转变为基于元数据和写时复制的操作，该设计把模拟

器复制从一个缓慢的瓶颈转变为轻量化、可扩展的机制，从而支持对移动端GUI智能体在交互式环境中进行高效并行探索。

# 4.3.5 子任务进度自感知的执行分支剪枝策略

启用推测执行会导致任务执行分支数量呈指数级增长。为降低硬件资源消耗和令牌使用量，必须裁剪掉那些落后或不太可能完成任务的分支。一种直接的方法是利用VLM的能力，让其对多个环境进行排序。然而，该方法存在明显局限性：（1）将所有分支的执行轨迹（如屏幕截图）输入VLM可能超出上下文长度，从而破坏上下文完整性；（2）仅比较各分支的最新屏幕状态会使VLM过度依赖视觉线索，丢失全局任务执行上下文。此外，每次比较都调用VLM在时间与成本上都开销巨大。因此，需要一种高效的分支选择机制，能够利用累积的任务执行上下文进行有效裁剪。

本节介绍分支裁剪机制的设计。与其向 VLM 输入过量或不足的上下文，该方法采用可跟踪的子任务执行进度作为累积执行上下文。分支裁剪通过比较不同分支的子任务进度来识别最先进展的分支，而无需依赖 VLM 的经验性评估。更具体地说，在 GUI 任务执行开始时，GUI 智能体会被提示将高层次的任务指令分解为一系列可跟踪的子任务。这一设计在先前研究中已被广泛用于提升 GUI 智能体性能，而本节设计将其用于跟踪不同执行分支的推进情况。为实现这一目标，该方法提示 LLM 在以下约束下完成任务分解：（1）分解应尽量简洁，仅生成可能导致在视觉或语义上明显区分的子目标；（2）每个子目标必须以明确的祈使句表述，描述“要实现什么”，而非“如何操作具体 GUI 元素”；（3）每个子目标需附带简短的人类可读理由，用于说明其在整体任务推进中的作用，但不得引用具体 GUI 组件或坐标。表 4-10 展示了实际任务分解的输出示例。


表 4-10 子任务分解模块输入输出示例


<table><tr><td>任务指令</td><td colspan="2">How many kayaking activities did I do this week in the OpenTracks app? Assume the week starts from Monday. Express your answer as a single integer.</td></tr><tr><td rowspan="4">分解后子任务</td><td>子任务序号</td><td>子任务描述</td></tr><tr><td>1</td><td>Open the OpenTracks app and navigate to the list of recorded tracks</td></tr><tr><td>2</td><td>Filter or scroll to find all activities from the current week</td></tr><tr><td>3</td><td>Count the number of kayaking activities within the filtered list</td></tr></table>

在获得子任务列表后，每个任务均由一个高层指令和编号为1的首个子任务开始。在每一步中，GUI智能体将这两个指令作为输入，生成并执行动作。动作执行完成后，智能体会主动调用由VLM驱动的反思模块。该模块接收执行前后的屏幕状态、任务描述、已分解的子任务及当前动作，并判断当前子任务是否完成；若完成，则相应环境的子任务索引加1，进入下一个子任务。在执行预设步数后（在实验过程中，设置为1），GUI智能体调用裁剪模块。通过比较不同环境的子任务索引，落后的分支会被裁剪，从而释放硬件资源供后续推测执行使用。值得注意的是，反思过程可集成到现有的VLM调用中，这是当前GUI智能体的常见设计[32,34,36,108,115]，这种借助现有模块的免费整合有效降低了时间和令牌开销。在上述确定性裁剪机制应用后，若剩余分支数量仍然过多，该方法进一步引入VLM驱动的辅助裁剪模块。该模块通过比较最新屏幕状态与文本化的执行历史，从中选择前N个分支。这一机制作为必要的补充，用于在主要裁剪不足时限制分支数量。在实验中，该模块平均被调用8.2次，用于裁剪掉34.4个分支。

# 4.4 实验与分析

# 4.4.1 系统实现

本节基于AndroidWorld中提出的原始M3A-SoM框架[34]实现了一个由PSE驱动的移动端GUI智能体。在4.2节的实证研究中，证明其利用商用VLM取得了当前在AndroidWorld上的最优性能。在动作生成阶段，该系统通过修改提示词，鼓励智能体探索更多执行路径，而非过早判定任务不可行。在工作流程层面，系统针对每个屏幕采样多个候选动作，并通过本节提出的基于意图的动作去重模块进行筛选。剩余动作会在复制的环境中并行执行。同时，系统在M3A-SoM的反思模块中加入了额外的提示词，允许其判断任意一个执行分支所解决的子任务，用于任务执行后的进度跟踪。基于本章4.3.4节的设计，在Ubuntu 24.04服务器上实现了移动端模拟器环境的快速复制机制。

# 4.4.2 实验设置

测试集与评估指标。实验采用AndroidWorld作为交互式基准来评估移动端GUI智能体[34]。该基准包含116个任务，覆盖三个难度级别（简单、中等、困难），其中人类专家的任务完成率仅为  $80\%$  。实验主要关注以下指标：（1）任务完成率（Task Completion Rate, TCR）：由智能体成功完成的任务数与任务总数之比计算得到；（2）时间效率：通过每个任务执行的步数来估算时间开销，并假设PSE模块经过充分优化，使得多个动

作能够在不同环境中并行执行；（3）令牌开销：以每个任务消耗的令牌数衡量，作为测试时额外计算量的代表性指标。令牌使用量通过VLM的API获取。

GUI 智能体设置。实验将所提出的 PSE 智能体与原始 M3A-SoM 智能体进行比较，考虑两种测试时扩展范式：（1）顺序扩展：通过增加步数上限来扩展任务执行能力；（2）并行扩展：利用 PSE 模块实现的并行执行能力。对于顺序扩展，实验在步数预算为 1 倍、2 倍、3 倍和 4 倍时评估 M3A-SoM 智能体。对于所提出的智能体，在关闭 PSE 模块但保留提示层优化（每步仅生成一个动作）的情况下，分配 1.5 倍的步数预算。对于并行扩展，实验启用 PSE 模块，并配置智能体在每个屏幕采样 8 个候选动作。实验中使用两种 VLM：Gemini 2.0 Flash（版本：gemini-2.0-flash-001）和 Gemini 2.5 Flash（版本：gemini-2.5-flash），且均未启用推理功能。

# 4.4.3 实验结果

# 4.4.3.1 时间效率分析

实验结果如图4-5所示。在AndroidWorld全任务基准上评估时，PSE智能体在时间效率上优于通过增加最大步数上限来扩展计算量的原始M3A-SoM智能体。具体而言，基于Gemini2.0Flash模型，该智能体实现了  $41.4\%$  的任务完成率，即便与步数预算为4倍的M3A-SoM相比也有优势，同时平均每个任务所需步数更少。在Gemini2.5Flash上，其取得了与M3A-SoM在2倍和3倍步数预算下相当的任务完成率，但分别减少了  $5.7\%$  和  $12.6\%$  的步数开销。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/92c607bf524987d45da14ed69f1873b1a401cf8660633404bf8e6e2851df0017.jpg)



图4-5AndroidWorld任务完成率与任务平均消耗步数


然而，这种在完整基准上的时间效率提升并未在不同任务难度水平中保持一致。在 Gemini 2.0 Flash 上，PSE 智能体在简单和中等任务上表现更佳（散点图集中于左上角），但在困难任务上表现不及基于步数扩展的顺

序测试时扩展方法，具体表现在完成任务数量从6下降到1，表明PSE过程可能过早裁剪了潜在的正确路径，从而降低成功率。而在更强大的Gemini2.5Flash模型下，顺序探索配合更高步数预算在多数实验设置中对中等和困难任务的时间效率更优。这些结果表明，并行推测执行方法的效果依赖在不同任务难度和VLM能力上体现出不同实验结果，后续小节将进一步进行任务层级分析。

# 4.4.3.2 令牌消耗分析

另一个用于评估GUI智能体在测试时扩展方法的维度，是其在提升任务完成率的同时是否能够降低计算开销（在实验中体现为令牌开销）。基于前文定义的指标，图4-6展示了所有任务的任务完成率与令牌开销关系，并按任务难度进行了细分。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/84695b607b04560227ac8ed3449e5f4d4b74178031d54e7d52f5cd7cb1c95ad6.jpg)



图4-6AndroidWorld任务完成率与VLM令牌消耗


实验结果显示，两种模型呈现出不同特性。当使用 Gemini 2.0 Flash 时，与步数预算为 3 倍的 M3A-SoM 相比，PSE 智能体在任务完成率上提升了  $5.2\%$  ，且令牌使用量相当。单纯提高 M3A-SoM 的步数预算仅带来微弱的性能增益，却显著增加了令牌消耗。然而，该方法的令牌效率主要体现在简单和中等任务上；在困难任务上，其表现不如顺序扩展。应用 Gemini 2.5 Flash 模型时，在全任务范围内，该方法与 M3A-SoM（3 倍步数预算）的任务完成率相近，但令牌消耗增加了  $14.9\%$  。这表明时间效率提升伴随着额外的令牌成本。对于简单任务，PSE 智能体依然比 M3A-SoM 更具性价比；而额外的令牌开销主要来自对中等和困难任务的广泛探索，其中部分任务的复杂度甚至对人类而言也极具挑战。在这类任务上，顺序方法可能更具优势，因为它专注于单一路径，能够帮助智能体逐步积累应用特定知识，从而在何时终止执行方面做出更合理的决策。

# 4.4.3.3 消融实验

本节探究对原始 M3A-SoM 智能体在提示层面的优化。该优化旨在鼓励其探索更多潜在的执行路径，而非过早判定任务不可完成。本节通过去除 PSE 设计，仅评估系统提示词优化本身是否能提升性能。在该设置下，智能体每步仅生成一个动作，回到顺序交互模式；同时将最大步数上限提高至 1.5 倍，以模拟顺序方式的测试时扩展。实验结果如图 4-5 与图 4-6 所示，以蓝色圆圈（Ours-SingleAct）表示。

本节得到以下主要观察结果：首先，提高最大步数上限在所有难度水平上均显著提升了任务完成率，说明该方法优于未经修改的M3A-SoM。其次，在整体基准水平上，顺序交互扩展的表现优于PSE智能体。例如，在Gemini2.0Flash上，其任务完成率更高（47.4%对比  $41.2\%$  ），时间效率相当，但令牌开销减少约  $18\%$  。再次，从任务难度分布来看，PSE仅在Gemini2.0Flash的中等难度任务上表现出优势，而在其余设置中，其时间效率与令牌成本均不具备竞争力。需要注意的是，本分析基于已完成与未完成任务的混合结果，未完成任务的存在可能显著抬高平均令牌消耗与延迟。为此，下一节将对已完成任务进行更精细的逐任务分析，以进一步比较顺序与并行两类测试时扩展范式。

# 4.4.3.4 已完成任务集合的逐任务分析

本节基于 Gemini 2.5 Flash 的实验结果进行了逐任务分析。从 AndroidWorld 的 116 个任务中，本节分析筛选出在两种设置下均成功完成的 54 个任务：Ours-PSE 与 Ours-SingleAct（1.5 倍步数预算）。随后，本节对比每个任务的步数与令牌开销差异，以判断哪种测试时扩展范式更适合 GUI 智能体。在时间效率方面（结果展示于图 4-7），Ours-PSE 在  $40.7\%$  的任务上优于 Ours-SingleAct，在  $33.3\%$  的任务上表现相同，而在  $25.9\%$  的任务上落后。总体而言，PSE 在所有已完成任务中共减少了 18 步，显示并行探索在时间效率上的优势。在令牌开销方面（结果展示于图 4-8），Ours-PSE 在  $43\%$  的任务上消耗更少的令牌，但整体平均每个任务多消耗 6.5K 令牌，这大致相当于一次动作生成步骤的令牌成本（包含像素级屏幕截图和简化视图层级信息）。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/b40eccd7a5709ea91de21544ad9fc1222613278ddd0a156995745203f413793b.jpg)



图4-7 顺序扩展方法与并行扩展方法逐任务步数消耗


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/b69030b880d652eee976ae37cca9e782609dd07c2c994e2885e678e8f260b884.jpg)



图4-8 顺序扩展方法与并行扩展方法逐任务令牌消耗


综上，尽管并行探索在时间效率上有所收益，但其代价是更高的令牌消耗。目前仍然是一个开放性问题：哪些任务更适合顺序探索，哪些更适合并行探索。未来需要进一步研究测试时扩展策略与任务特征的匹配。

# 4.5 本章小结

本章研究了测试时扩展方法在移动端GUI智能体中的适用性及其性能提升潜力。测试时扩展在该场景下可分为模型层级（如以具备推理能力的VLM替代无推理能力的VLM)和工作流层级(如引入反思与回溯机制、增加交互步数等）。

在模型层级，本章对比了具备推理能力与不具备推理能力的VLM，并在静态与交互式平台上进行实证研究。结果表明，在交互式环境中，推理型VLM带来显著性能提升并达到当前最佳水平；但在静态测试集上，提升有限，甚至可能出现性能下降。错误分析进一步揭示了静态数据集的局限性，强调了在交互式环境中评估移动端GUI智能体的重要性。在工作流层级，本章发现现有GUI智能体依赖串行交互策略，测试时扩展方法虽能提升完成率，却显著增加任务完成时延。为此，本章提出了基于并行推测执行的扩展方法：在单一GUI状态下采样多个动作提议，并在快速复制的环境中并行执行，从而以更少步数完成任务。在交互式环境中测试的结果表明，该方法在  $74\%$  的任务上能以更快或相同时间完成任务，而额外令牌开销平均仅相当于一次动作生成调用的成本。

综上，本章从模型与工作流两个层面系统探究了测试时扩展在移动端GUI智能体中的作用。在提升性能的同时，本章的方法优化了任务响应时间，为该技术在实际应用中的落地奠定了基础。

# 第五章 基于关键状态匹配的自动化评估方法

# 5.1 本章概述

移动端智能体使用户能够通过自然语言与智能手机交互，从而免于繁琐、耗时的操作。这类智能体对视力或手部有障碍的用户，或在无法方便使用屏幕的情境（如驾驶）中尤其有帮助。典型的移动端智能体包括Apple Siri[1]和Google Assistant[4]，它们已成为智能手机中不可或缺的服务。近期，LLM与VLM的发展，使研究人员能够构建功能更强大的移动端智能体[91,92,97,105]。这些智能体的核心能力在于理解用户的自然语言指令，并在移动界面上执行相应操作，即移动端GUI任务自动化，例如“将Bob发来的最后一封邮件转发给Alice”。

尽管近期基于LLM的移动端智能体在任务自动化方面展现出强大能力，但其评估方法仍存在一定缺陷。与基于成熟静态数据集进行评估的传统机器学习模型不同，移动端智能体需要以智能手机动态且不确定的状态（如网络连接情况、动态内容）作为输入进行交互。此外，移动设备普遍采用触控和手势交互（如滑动、捏合），导致输入形式多样且含糊。这种多变性使得静态的基于动作的匹配算法更加复杂。因此，仅依赖数据集中确定性的智能手机状态对移动端智能体进行评估，难以揭示其真实能力[46,47,88]。

通常，移动GUI自动化任务的评估方法主要有两类，但都无法同时兼顾高保真性与高可扩展性。最直观的方法是让人工验证任务的完成情况。然而，人工评估难以复现[165]，且随着智能体数量、任务数量及评估平台数量的增加，所需的人力成本会急剧上升。目前最常用、也是多数现有研究采用的方法，是在既有数据集上进行精确动作匹配，类似于传统机器学习的评估方式[25,40,47,88,90,92]。其核心思想是让标注人员生成一条能够成功完成任务的正确动作序列作为数据标签，然后将智能体生成的动作与这些标签进行比较。尽管这种方法允许一定的误差容忍，例如点击位置的微小差异[47]，但它无法涵盖完成GUI自动化任务的所有可能甚至“无限”路径，因此会导致显著的假阴性率。例如，对于“在Expedia上预订一辆6月1日至7日、每日预算不超过60美元的洛杉矶租赁车”这一任务，三个筛选动作的执行顺序是可以互换的，而若仅将其中一种执行路径作为参考，就可能错误地判定一个实际上已完成的任务为失败。此外，已知基于LLM的

智能体具备自我纠正错误动作的能力[33]，这一能力对提升GUI任务自动化性能至关重要，但在静态数据集中无法进行评估。

为了解决上述问题，本研究提出了LlamaTouch，首个能够在真实移动端环境中评估移动端智能体的测试平台，同时兼顾评估的高保真性与高可扩展性。LlamaTouch的核心思想是将任务执行轨迹与标注人员定义的少量“关键状态”进行比对，而非依赖静态轨迹中预定义的动作序列。例如，对于任务“打开MicrosoftExcel应用（若未安装则先安装），进入登录界面，...”而言，其关键状态应包括：（1）应用“MicrosoftExcel”已被打开；（2）应用处于登录页面。至于安装应用等操作则被视为非关键状态，应在评估中忽略。在任务执行过程中，LlamaTouch仅允许移动端智能体从静态数据集中获取任务描述，而设备状态则直接从真实的移动设备上获取。智能体生成的动作会直接在这些设备上执行，并将所有GUI交互数据记录为任务执行轨迹。在评估阶段，LlamaTouch将任务执行轨迹与标注的关键状态进行比对，以判断任务是否完成。

为实现高保真且可扩展的评估，LlamaTouch融合了两种有效方法。（1）在关键状态标注方面，LlamaTouch采用细粒度标注机制，覆盖屏幕级与单个GUI组件级。它结合像素级截图与文本化的屏幕层级结构，显式突出重要的GUI组件。借助LlamaTouch提供的丰富标注原语，可以通过启发方式快速识别并标注关键状态的属性，从而减少人工工作量，例如文本框中的内容需精确匹配。这些标注后的GUI状态将用于后续的高保真评估。（2）LlamaTouch引入多层次状态匹配算法，对多样化的标注GUI状态结合模糊匹配与精确匹配。一方面，通过近似屏幕匹配，LlamaTouch能够适应动态的移动端环境与变化的屏幕内容；另一方面，通过混合GUI状态匹配，能够检测并匹配屏幕上的关键信息。

基于上述设计，本章构建了一个用于在真实移动端环境中评估移动GUI自动化任务的大规模数据集，该数据集包含经过预标注的关键状态，覆盖496个不同任务，涵盖多种常用Android应用。LlamaTouch还配套提供了一个易用的测试平台，使移动端智能体能够与真实的Android环境无缝交互。该平台提供了一组简洁且广泛适用的API，确保与大多数移动端智能体的兼容性。移动端智能体可方便地集成至LlamaTouch，并利用本章提出的数据集在真实场景下测试其移动端GUI任务自动化能力。

实验评估中，本章使用 Google Android Emulator[166]和一部 Google Pixel 5 智能手机作为真实移动端环境实现了 LlamaTouch。目前，LlamaTouch 内置了四个智能体，包括 Auto-UI[40]、AppAgent[107]、AutoDroid[92] 和 CoCo

Agent[167]。在以人工验证结果作为任务完成的真实标签的情况下，LlamaTouch 在真实环境中检测已完成任务的评估准确率接近  $80\%$  ，而此前基于动作匹配的评估方法未能达到这一水平。本章还揭示了当前移动端智能体在真实环境中执行任务时存在的局限性。

本章的主要贡献如下：

（1）本章观测到使用静态数据集评估移动端GUI任务自动化智能体时存在高假阴性率的问题。为此，提出了一种仅比较关键状态而非具体动作序列的评估设计。

（2）本章提出了一种基于多种标注原语的关键状态标注方法，将直观的屏幕截图与语义精确的视图层级信息相结合，实现了对GUI组件的细粒度、精准定位与标注。

（3）本章设计了一种新颖的任务评估方法，在不同GUI状态层面结合精确匹配与模糊匹配，既保证了评估的高保真性，又能够很好地适应动态执行环境。

（4）本章提出了LlamaTouch，这是首个能够在真实移动端环境中同时实现高保真度与高可扩展性的移动端GUI智能体的评估平台。LlamaTouch包含496个人工标注了关键状态的任务，并集成了四个移动端GUI智能体，实验结果验证了其在GUI自动化任务评估中的高保真性与高可扩展性。

# 5.2 交互式GUI任务评估系统LlamaTouch

图5-1展示了利用LlamaTouch评估平台在真实移动设备上执行任务与评估任务执行情况的工作流程。与人工评估和基于静态数据集的评估方法相比，LlamaTouch具备如下优势：首先，LlamaTouch允许GUI智能体在真实移动设备上执行任务。多数以往评估方法是在静态移动端GUI数据集上进行模拟评估。具体来说，GUI智能体在静态数据集提供的真值GUI表征序列上基于任务描述预测一系列动作；每个预测动作需要从头与数据集中的真值动作进行精确动作匹配，要求动作类型与参数均匹配成功，则判别为单个GUI上预测正确；当所有GUI动作都正确匹配时，则认为任务成功完成。而LlamaTouch则允许移动端GUI智能体在真实移动端环境中以任意路径执行GUI自动化任务，从而反映其在真实环境中的性能。其次，LlamaTouch提供了细粒度的应用关键状态标注方法。通过观察GUI自动化任务执行过程中应用内关键GUI元素状态的变化，LlamaTouch使标注人员能够显式标注应被检测和匹配以判定任务完成的应用关键状态。这一

方法避免了以往静态评估方法对轨迹确定性的依赖，从而降低了评估中假阴性的概率。最后，LlamaTouch 提供了高保真度且可扩展的任务评估方法。LlamaTouch 将真实移动端环境中采集的任务执行轨迹与已标注的关键状态进行比对，通过在不同应用状态下结合精确匹配与模糊匹配算法，实现了高保真且可扩展的评估。本节内容将对上述三个设计进行详细阐述。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/07232a354aca1d7aab701ecf8c57f93c12eca86328d35de69f90a02b8e149852.jpg)



图5-1 LlamaTouch工作流和各组件交互流程


# 5.2.1 基于交互式环境的GUI任务执行流程

第2.5章节的分析展示了基于静态数据集中与定义的确定性轨迹对移动端GUI智能体进行评估会导致多种假阴性评判，降低评估结果的准确性。为解决该问题，LlamaTouch支持在真实设备上执行移动端GUI任务，为揭示GUI智能体的真实性能提供执行环境基础。为简化这一过程，本节提出AgentEnv以作为现有移动端GUI智能体与真实移动端环境（如智能手机、Android模拟器和云端设备农场）之间的桥梁，并为GUI智能体提供标准编程接口，以简化其与执行环境交互的编程负担。借助AgentEnv，GUI智能体能够按照图5-2中的流程在真实移动端环境中执行任务。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/7b7be2df40143468be0cbfef9be1d88ea75e21f2b0bde48a0a4e3c7f6ffe46dc.jpg)



图5-2 交互式环境中GUI智能体任务执行流程


具体来说，GUI智能体、AgentEnv与移动端执行环境之间的交互流程如下：首先，智能体向AgentEnv请求来源于LlamaTouch任务集中的任务元数据（例如自然语言描述的任务指令）以进行初始化。随后，GUI智能体开始动作执行循环：每个循环中，其使用AgentEnv提供的统一编程接口向AgentEnv请求GUI表征（如屏幕截图与VH），AgentEnv再将该请求

转发至移动设备并返回结果。移动端智能体将任务元数据与GUI表征作为输入，预测当前界面应执行的动作，并将该预测结果发送至AgentEnv，由AgentEnv在移动端环境中完成实际执行。步骤2至步骤5会不断循环，直至GUI智能体判定任务完成或是到达最大循环数限制。


表 5-1 LlamaTouch 提供给 GUI 智能体的编程接口


<table><tr><td>类别</td><td>API</td><td>输入参数/返回值</td></tr><tr><td>元数据请求</td><td>get_task</td><td>返回值：task_description（文本形式的任务描述）</td></tr><tr><td rowspan="2">GUI状态请求</td><td>getScreen</td><td>返回值：encrypted.img（base64编码的图像表征）</td></tr><tr><td>get_vh</td><td>返回值：vh（文本形式的视图层级信息）</td></tr><tr><td rowspan="7">动作空间</td><td>task_COMPLETE</td><td rowspan="4">/</td></tr><tr><td>task_impossible</td></tr><tr><td>navigate_home</td></tr><tr><td>navigate_back</td></tr><tr><td>click</td><td>输入：[x,y]（归一化的x/y坐标）</td></tr><tr><td>type</td><td>输入：text（待输入文本）</td></tr><tr><td>swipe</td><td>输入：[touch_x, touch_y, lift_x, lift_y, duration]（滑动动作的开始/结束位置的归一化的坐标与动作时长）</td></tr></table>

在任务执行过程中，所有GUI表征及对应的动作均会被AgentEnv记录为任务执行轨迹，用于后续评估。AgentEnv提供两种格式的GUI表征：像素级屏幕截图和文本化的屏幕VH。此外，其还记录每个GUI对应的前台应用活动名称（Android Activity名称[168]），在该GUI上执行的具体动作，以及系统关键状态（如已安装应用列表），以确保高保真度的评估结果。表5-1列出了AgentEnv提供给移动端GUI智能体的API接口，以供现有移动端GUI智能体集成至LlamaTouch评估平台中。AgentEnv提供API也可方便扩展，以满足当前及未来不同GUI智能体需求（例如：通过adb调用Android系统意图[30]）。

# 5.2.2 关键状态标注原语设计

第2.5节中展示的两个主要案例阐述了基于静态基准测试的精确动作匹配是不准确的：首先，该评估方法要求智能体在单个GUI上生成的动作及其参数与基准数据集中的真值完全一致。其次，它将数据集提供的固定GUI交互序列作为参考，这使得它无法评估其他可能的任务执行路径。

为了克服静态基准测试评估的局限，受启发与屏幕问答任务只关注智能体回复结果的评估方式[101,169-171]，本节对移动端GUI任务的关键洞察

为：任务执行流程并非执行固定的动作序列，而是通过一系列动作，致使系统或应用状态发生转化。即使任务执行路径不同，应用状态仍会存在重叠。这些重叠的应用关键状态可用于判断任务是否达成预期的里程碑或成功完成。例如，通过识别任务意图“打开 Microsoft Excel 应用（若未安装则进行安装）、进入登录页面，……”可以得到两个可用于评估的潜在关键状态：（1）“Microsoft Excel”应用已打开；（2）应用已进入登录页面。其他动作，如检测应用是否已安装，或是执行应用安装流程，在评估过程中可忽略。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/8e8ec93a4e426c68aab1da07310ba90606ed7a8473a4e8b4f1b593634314b00b.jpg)



原始视图层级（VH）信息


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/a421e67306cd7b59a04b54ebf9184903155daf4b037893164ef9e4089408dd24.jpg)



简化后VH：可操作GUI组件列表



基于可操作GUI组件视觉强化的屏幕截图



图5-3 关键GUI组件提取与视觉强化流程示意图


若要实现上述评估方法，需要对GUI任务执行流程中的关键状态进行准确识别与标注。然而，仅在整体GUI表征层面（例如整个屏幕截图）标注应用状态并比较屏幕级相似度[172]，粒度过于粗糙，导致评估结果不准确。例如，在网页购物应用中，由于不确定性的滑动手势或动态加载内容，不同执行流程中的屏幕内容可能存在细微差异。如图5-3所示，为了实现准确且高效的关键状态标注，LlamaTouch将整体像素级GUI表征拆解为独立的GUI组件。实现方法是简化每个屏幕的VH（其涵盖了文本信息以精确表达每个GUI组件的属性、可点击性、边界框等），以提取可操作且可见的GUI组件。给定特定GUI任务的执行序列，使用上述方法提取出的GUI组件信息将用以强化屏幕截图，为标注者提供精确的覆盖边框与基于数字的唯一标识符。图5-4展示了任务“Empty the shopping cart on Bestbuy”提供给标注者的使用VH增强的屏幕截图示例。对于未展示在屏幕截图中但仍需标注的关键GUI组件，标注者可基于原始VH信息自行添加至可操作GUI组件列表，以补充完善自动化VH简化流程。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/53e48d3fe5c1a5b565b957849ae4e4336ba57f3eae7c45f8968454e84656586f.jpg)


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/f7f9526674cf7a395009468472cbfd25331f9bd8f7d2ad632c4eb8684a893f9d.jpg)


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/e3363bdaa48051769f9b6d63873443eadcdd16b73f4d9906311efbcd16f6560b.jpg)


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/af2051557b6f6938ceb2454ccf635fe3e55e692d494dc4112694c449e2ebcedc.jpg)


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/972a6150dec139afaa2deabdcb821e2cfad1231bfb37e9e546a38dcdf2a4cbe5.jpg)


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/610d9ed6bfe640b0ba0b906f02a4bfbe43c231a1a36661376777f313883800b9.jpg)



图5-4 使用VH强化的GUI任务轨迹与标注示意图


LlamaTouch 设计了一组用于关键状态标注的原语，用于标注屏幕或是系统的关键状态，并指示这些关键状态应如何评估。形式上，标注原语指一组最小、可组合的判定谓词，用于评估时对真值 GUI 轨迹  $R$  与被评估 GUI 轨迹  $H$  的关键状态进行验证。真值 GUI 轨迹上的关键状态由若干标注原语的合取组成： $S = \Lambda_{i} p_{i}(\cdot)$ 。当且仅当  $\forall_{i}, p_{i}(R, H) = True$  时，判定关键状态成功匹配。标注人员需明确识别并使用这些原语，以标注出在评估任务执行结果时具有信息性且确定性的关键状态。标注原语以关键字+参数的形式出现，并与 VH 简化后为每个 GUI 组件分配的数字 ID<n>绑定（例如 exact<n> 指向 ID 为 7 的组件）。表 5-2 中 6 类原语覆盖了 GUI 屏幕层、组件层、系统层与动作层的常见判据，并通过 9 个标注关键字示例化（例如 fuzzy<-1>, activity, exact<n>等）。LlamaTouch 所提供标注原语可根据其所表示的关键状态类型分为三类。


表 5-2 LlamaTouch 中的标注原语设计及用例说明


<table><tr><td>匹配类型</td><td>应用状态</td><td>标注原语</td><td>标注关键字</td><td>用例说明</td></tr><tr><td rowspan="2">模糊匹配</td><td rowspan="4">GUI状态</td><td>屏幕信息</td><td>fuzzy&lt;-1&gt;</td><td>验证两个屏幕上的内容是否大致相同</td></tr><tr><td>文本框</td><td>fuzzy&lt;n&gt;</td><td>验证目标文本框的内容是否在语义上与原始文本框&lt;n&gt;的内容相似</td></tr><tr><td rowspan="4">精确匹配</td><td>应用Activity</td><td>activity</td><td>一种粗粒度的标注与匹配方法,用于确定两个GUI是否表示应用程序中的相同功能界面</td></tr><tr><td>GUI组件</td><td>exact&lt;n&gt;,exclude&lt;n&gt;</td><td>验证是否存在GUI组件与原始GUI组件&lt;n&gt;完全相同,或者要求目标GUI不存在被标注GUI组件</td></tr><tr><td>系统状态</td><td>应用安装状态</td><td>installed,&lt;app&gt;,uninstalled,&lt;app&gt;</td><td>验证名为app的目标应用程序是否已成功安装或卸载</td></tr><tr><td>动作</td><td>动作</td><td>click&lt;n&gt;,typetext&gt;</td><td>验证两个操作及其参数是否相同</td></tr></table>

（1）GUI状态。GUI状态标注的目的是提取并比较两个屏幕是否整体

一致或相似。由于一个屏幕可能包含不同类型的组件，LlamaTouch 使用四种主要原语覆盖所有 GUI 组件的比较逻辑。（1）屏幕信息和文本框的模糊匹配用于比较整个屏幕或指定文本框的内容是否相似；其中文本框原语用于标注可能包含动态内容的文本框，例如网页购物应用中的搜索框，或网站的 URL。（2）应用 Activity 原语利用了移动端系统的 Activity 机制[168]，主要用于检查两个 GUI 表征是否处于同一屏幕（即应用程序的同一页面）。（3）GUI 组件用于确定性地标注 GUI 组件的状态，包括文本框、可点击按钮、图片等。例如，如果需要匹配某个文本框的内容，则该文本框应使用 exact 关键字标注；若要求某些 GUI 组件不出现在屏幕上，则可使用 exclude 关键字标注。

（2）系统状态。LlamaTouch 中的系统状态标注主要借鉴自先前构建的数据集 AITW 中的任务。例如，任务“安装 YouTube Kids 应用”可以直接通过 shell 命令访问系统状态（如 pm list packages）来检测，而无需涉及复杂的 GUI 状态。目前，LlamaTouch 支持以编程方式检查应用的安装状态；系统状态标注关键字与 LlamaTouch 测试集中涵盖任务相关，其可扩展以支撑更多类型系统状态评估（例如，蓝牙与 Wi-Fi 网络的开关）。

（3）执行动作。尽管 LlamaTouch 标注原语的设计准则是检测任务执行过程中系统及应用状态的转变，但在某些情况下，仍需要依赖在特定 GUI 表征上的动作来验证 GUI 智能体行为，尤其是在屏幕上缺乏足够状态标识时。LlamaTouch 提供了 click 和 type 两个关键词，用于覆盖最常见的动作类型。

关键状态标注可分为以下几个过程。首先，如图5-4所示，对于每个GUI表征，LlamaTouch会在截图上为所有功能性GUI组件叠加标注边框与数字标记。这一步通过从文本化VH中提取每个GUI组件的精确元数据完成。接着，标注人员将识别在评估中应检查的关键状态，以确保任务已完成。LlamaTouch简化了这一过程，标注人员只需明确指出应匹配的GUI元素ID以及应使用的标注原语。例如，在Bestbuy清空购物车后，屏幕显示内容为“Myrcartisempty”的文本框，该文本框被视为关键GUI组件，并在图5-4最后一张屏幕中以数字ID13的边框高亮显示。由于预期文本框内容需完全匹配，因此标注关键词为  $\mathrm{exact} < 13>$  。关键词  $\mathrm{exact} < 27>$  则用于验证两个屏幕是否都处于Bestbuy的购物车页面。所有标注结果连同GUI表征和任务描述一起，构成了由关键状态驱动的测试集。这些关键状态被LlamaTouch用于高准确率的任务评估。

# 5.2.3 多层次状态匹配的评估方法

在 LlamaTouch 中，测试集中的真值任务轨迹、标注的关键状态以及 AgentEnv 记录的任务执行轨迹被联合用于评估。评估模块会依次遍历关键状态中的标注原语，检验任务执行轨迹是否按照顺序匹配所有标注状态；若全部匹配，则判定任务完成。要实现高准确度的评估，其中最大的挑战在于如何确保执行轨迹与关键状态的有效对齐。由于真实移动端环境中的任务执行轨迹往往包含动态变化的屏幕内容，静态数据集中标注的关键状态必须适配到实际捕获的屏幕上，并且仅在关键信息层面进行精确匹配。为解决这一问题，LlamaTouch 提出了一种多层次状态匹配算法，该算法在整个屏幕和局部 GUI 组件两个层面结合模糊匹配与精确匹配，以提升评估的可靠性。算法首先基于 GUI 表征和应用 Activity 对两个屏幕进行近似匹配；随后，在已匹配的屏幕对中，根据标注原语将每个关键状态与目标屏幕中的 GUI 组件逐一比对。只有当所有关键状态均在目标屏幕中找到匹配对象时，任务才被视为完成。接下来，本节将详细介绍各类标注原语的匹配规则设计。

近似屏幕匹配用于确保在屏幕内容高度动态变化的情况下，依然能够判断两个屏幕是否属于同一应用的功能页面。这一过程是后续在屏幕内进行精确或模糊匹配的前置条件。为此，LlamaTouch 采用了两个标注原语：应用 Activity 与屏幕信息。

（1）应用Activity匹配。应用Activity表示用户与应用交互的入口点[168]，是判断两个屏幕是否属于同一应用页面的重要标识。通常，不同功能页面具有唯一的Activity名称，即便它们隶属于同一应用。例如，主设置页面的Activity为com.android.Settings.Settings，而Wi-Fi设置页面则为com.wifiadmin.Settings.WifiSettingsActivity。因此，精确的Activity匹配通常作为判断两个屏幕是否处于同一页面的基础过滤步骤。然而，部分应用由于设计原因，可能会让多个功能页面共享相同的Activity名称[173]。在这种情况下，LlamaTouch会进一步结合屏幕信息进行以实现更精确的匹配

（2）屏幕信息模糊匹配。当应用Activity不足以区分应用的不同功能页面时，LlamaTouch依赖屏幕信息原语进行辅助判断。其核心在于提取屏幕中的关键信息。具体而言，LlamaTouch将文本化的VH结构简化为紧凑的HTML表示（与以往研究[91,92]类似），并保留每个GUI组件的类型。随后，通过计算两个简化HTML表示的句向量余弦相似度[174]来比较屏幕。当相似度超过预设阈值（例如，本章实验中设置为0.85）时，判定两个屏幕相似。该模糊匹配机制使LlamaTouch能够在动态屏幕内容频繁变化的

情况下，依然保持评估结果的可靠性。

混合GUI状态匹配在完成近似屏幕匹配后应用于已匹配的屏幕，用于检查混合类型的已标注GUI状态，包括GUI组件、动作以及系统状态的精确与模糊匹配。

（1）精确GUI组件匹配要求两个屏幕中的已标注组件在VH中的所有属性（如class、text、selected）完全一致。这种方式在评估文本框内容、按钮状态（如是否被选中）或可选择图标时尤为有效。LlamaTouch通过遍历目标屏幕的VH节点来寻找对应组件，若未找到则视为匹配失败。在LlamaTouch测试集中，精确组件匹配约占所有关键状态的一半（1379条中的698条）。

（2）模糊文本框匹配适用于文本内容存在轻微差异但语义等价的场景。若文本框内容符合任务语义并能产生相同结果，则判定为匹配。例如，在 Google Play Store 中搜索“Microsoft Excel”或“Excel”都会返回目标应用。LlamaTouch 会提取已标注文本框的内容，并采用与屏幕信息模糊匹配相同的方法，将其与目标屏幕 GUI 组件进行比较。

（3）动作匹配。尽管LlamaTouch的核心是检测任务过程中的状态转变，但在部分场景下仍需验证具体动作的正确性，如点击目标组件或输入验证码。LlamaTouch直接比较已标注动作及参数与屏幕上的对应操作。与以往研究[47]不同，LlamaTouch使用从VH中提取的目标组件的XPath[175]作为点击动作的参数，而非精确坐标，从而提供更高的容错性。

（4）系统状态匹配对涉及确定性系统状态的任务（如检测应用是否已安装），该方法比单纯的GUI状态比较更高效、更准确。LlamaTouch目前支持检测应用安装情况，并在任务执行过程中由AgentEnv记录。在匹配时，系统会验证标注的系统状态是否与执行轨迹末屏记录的状态一致。

通过多层次状态匹配算法，LlamaTouch 在真实 GUI 任务执行轨迹上实现了高保真度的评估，同时保持了静态数据集的可扩展性优势。上述评估逻辑已封装在 LlamaTouch 评估器中。GUI 智能体只需定义如何为任务加载执行轨迹（如图 5-1 右侧所示），评估器便可自动运行并输出包括任务完成率在内的指标。后续实验章节将展示 LlamaTouch 评估算法的准确性。

# 5.3 基于关键状态标注原语的任务集构建

本节将介绍 LlamaTouch 在应用关键状态标注原语与多层次评估方法基础上构建的评估任务集，并给出相应的统计信息，以阐明任务集的构成

特点及其在评估移动端GUI智能体中的适用性。

# 5.3.1 任务集构建方法

LlamaTouch 任务集由先前的 AITW 数据集[47]中的任务与 LlamaTouch 自主标注的任务组合而成，涵盖多种类别与主流应用。新增的热门应用任务弥补了现有文献中的空白，更好地支持对移动端 GUI 智能体通用性与真实性能的评估。图 5-5 展示了数据集构建的工作流程，每个任务样本均需经过以下三个步骤。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/9c7e9b578d1fd7c75808085b813f973838100687726d534643a0db753c9f805c.jpg)



图5-5 LlamaTouch任务集构建流程


任务指令生成。任务描述由人工与大语言模型协同生成，所用信息来自 Google Play Store 热门应用的元数据（如应用名称、类别与描述）。对于 AITW 数据集中的任务，在去重后抽取部分任务，由人工验证以排除重复、难以被 LlamaTouch 标注原语标注以及超出人类与现有 GUI 智能体能力范围的高复杂度任务。经过这一过程，该方法从 AITW 数据集中采样了 102 个任务（涵盖 26 个不同应用），并生成了 394 个新任务（涵盖 46 个不同应用）。这些任务描述覆盖多种应用类别，包括工具类（如 Zoom、Expedia）、社交媒体类（如 Discord、Instagram）以及电商类（如 Walmart、Amazon）。图 5-5 展示了这些类别的比例分布。来自于 AITW 的任务描述中，超过  $50\%$  的任务与 Google 应用相关；LlamaTouch 新生成的任务中，社交媒体类与工具类占比近一半（ $27\%$  与  $24\%$ ），其余任务面向娱乐、新闻、以及 Google 应用等。

GUI交互轨迹录制。在任务生成与验证完成后，标注者会进入交互轨迹录制阶段。AITW数据集中的任务同样需要经过此流程，因为其原始数据未包含视图层级信息。本节工作基于ws-scrcpy构建了移动端GUI交互

轨迹录制工具，其通过adb连接移动端模拟器或是真实移动设备，并在其上执行标注者下发的动作。为了保证数据的真实性与可复现性，工具提供了接近真实设备的操作界面，标注者可以通过触控、滑动、输入等自然方式完成任务。在录制过程中，标注者需根据任务描述选择最简洁的操作路径，避免任何无关或冗余的交互步骤。这一要求与AITW[47]的标注规范保持一致，确保收集的轨迹既准确又高效。例如，在执行“Empty the shopping cart on Bestbuy”这一任务时，标注者必须连续点击5次才能完成，系统会逐步捕捉到每一次操作并生成对应的界面变化。录制工具会同时采集多模态信息，包括：操作序列（点击、输入、滑动等）；视图层级信息，以树结构保存每个GUI组件及其属性；当前界面应用Activity名称，用于表示当前屏幕所属的功能模块；屏幕截图，提供界面在视觉层面的完整表征。这些数据共同组成一条完整的GUI交互轨迹，为后续的关键状态标注和评估提供了坚实的基础。

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-08/c4a5b480-c3c0-4229-86f9-fb4b474edd65/6017b8ec516fef28ddae50ac2d8b4b36f9d67bf14a0eddaa0c1933cf9fae817a.jpg)



图5-6 关键状态标注系统用户界面示意图


关键状态标注。在交互轨迹录制完成后，标注者需进一步识别并标注关键状态。为此，本节开发了关键状态标注系统。图5-6展示了该标注系统的Web端界面，其左侧为基于VH强化的屏幕截图，每一张截图中潜在被标注的GUI组件均添加了数字索引，以帮助标注者更直观地定位目标元素。标注者需仔细对比任务描述与轨迹，判断哪些界面转折点代表任务的关键里程碑，例如“购物车被清空”、“登录页面出现”或“支付流程完成”。在标注过程中，标注者会使用第5.2章节提出的关键状态标注原语（如GUI组件状态、文本输入、动作触发）对这些状态进行精确描述，并将其与数字索引绑定，从而确保标注的可复现性与可比性。为了保证标注的客观性和一致性，所有存在歧义的状态必须由三名及以上标注者共同讨

论并达成共识后才会最终确认，从而保证 LlamaTouch 的数据质量，避免了因个人偏差或疏漏带来的潜在问题。

最终，LlamaTouch数据集共包含496个任务，覆盖57个不同的Android应用，涵盖从工具类到电商类的多种应用场景。任务在复杂度上也具有广泛分布，既包括单步点击操作，也涉及多步骤、多页面跳转的复杂流程，为移动端GUI智能体的评估提供了一个高质量、细粒度且多样化的交互式评估基准。

# 5.3.2 任务集统计信息

本节展示了LlamaTouch任务集的统计信息。表5-3通过完成任务所需的平均步骤数（即动作数）量化了任务的复杂度。结果表明，AITW中的任务较LlamaTouch新生成的任务更为复杂，平均步骤数分别为7.35和5.67。整体来看，完成一个任务平均需要约7步，复杂度范围从最少2步到最多42步不等，体现了任务在难度层面上的显著多样性。


表 5-3 LlamaTouch 任务集统计数据


<table><tr><td>任务来源</td><td>任务数量</td><td>应用数量</td><td>任务复杂度（平均完成所需步数）</td></tr><tr><td>AITW</td><td>102</td><td>26</td><td>7.35 (2-19)</td></tr><tr><td>LlamaTouch</td><td>394</td><td>46</td><td>5.67 (3-42)</td></tr><tr><td>总计</td><td>496</td><td>57</td><td>7.01 (2-42)</td></tr></table>

为了进一步刻画数据集的特征，本节对真值任务中的动作类型进行了分析。表5-4展示了动作类别的分布情况。总体而言，所有496个任务均涉及点击动作（click）；其中，292个任务包含滑动动作（swipe），147个任务涉及输入动作（input）。少量任务还使用了Home键（navigate_home）或返回（navigate_back）进行导航。通过将动作总数除以包含该动作的任务数，可以看到每个数据样本平均包含超过4次点击操作，而其他类型动作的平均次数均不足2次。这表明点击是移动端任务执行中最核心、最频繁的交互形式，而滑动与输入则起到辅助作用。

在关键状态的统计方面，表5-5给出了详细结果。核心观察为，绝大多数任务(441个)仅在任务完成后需要检查一个GUI表征即可判断结果，说明多数任务的关键状态集中于终止状态。另有53个任务和2个任务分别需要在两个和三个GUI表征中标注关键状态。就关键状态类型而言，精确GUI组件匹配占比最高（ $51\%$ ），其次是应用Activity匹配，占比 $36\%$ 。超过 $84\%$ 的任务（418/496）至少包含这两类关键状态之一。与此同时，模糊匹配在评估中同样不可或缺：共有44个任务涉及51个模糊GUI组件匹配，另有31个任务涉及模糊屏幕信息匹配。


表 5-4 LlamaTouch 任务集动作类型统计信息


<table><tr><td>动作类型</td><td>动作数量</td><td>含有该动作的任务数量</td><td>均值/方差</td></tr><tr><td>click</td><td>2192</td><td>496</td><td>4.42/2.51</td></tr><tr><td>swipe</td><td>376</td><td>292</td><td>1.29/0.9</td></tr><tr><td>input</td><td>173</td><td>147</td><td>1.18/0.55</td></tr><tr><td>navigate_home</td><td>44</td><td>43</td><td>1.02/0.15</td></tr><tr><td>navigate_back</td><td>6</td><td>5</td><td>1.2/0.45</td></tr></table>

综上，LlamaTouch任务集在复杂度、动作类型和关键状态类型上均展现出较高的多样性。数据集中既包含单一最终状态即可判定的简单任务，也涵盖需要跨多屏判断的复杂场景；既覆盖了高频的点击操作，也涉及滑动、输入及导航操作；同时还包含精确与模糊多种匹配方式。这些特征确保了LlamaTouch能够为移动端GUI智能体提供全面、细粒度的评估基础。下一节将进一步展示基于该数据集的实验结果。


表 5-5 LlamaTouch 任务集关键状态统计信息


<table><tr><td>匹配类型</td><td>关键状态</td><td>关键状态数量</td><td>含有该关键状态的任务数量</td><td>均值/方差</td></tr><tr><td rowspan="6">精确匹配</td><td>GUI组件</td><td>698</td><td>418</td><td>1.67/0.92</td></tr><tr><td>应用 Activity</td><td>490</td><td>442</td><td>1.11/0.32</td></tr><tr><td>动作: Click</td><td>98</td><td>93</td><td>1.05/0.23</td></tr><tr><td>动作: Type</td><td>1</td><td>1</td><td>1/0</td></tr><tr><td>系统状态: 应用安装</td><td>7</td><td>7</td><td>1/0</td></tr><tr><td>系统状态: 应用卸载</td><td>3</td><td>3</td><td>1/0</td></tr><tr><td rowspan="2">模糊匹配</td><td>GUI组件</td><td>51</td><td>44</td><td>1.16/0.43</td></tr><tr><td>屏幕信息</td><td>31</td><td>31</td><td>1/0</td></tr></table>

# 5.4 实验与分析

本节首先对所提出的 LlamaTouch 评估方法进行系统性实验，以验证其相较于传统评估方法的准确性。随后，本节通过消融实验论证所提出的关键状态标注原语在评估过程中的作用与贡献。最后，通过对比分析现有移动端 GUI 智能体在 LlamaTouch 评估平台上的任务完成率，全面揭示当前移动端 GUI 智能体的能力现状。

# 5.4.1 实验设置

真实移动端环境。LlamaTouch 主要基于 x86-64 架构的 Android Emulator[166] 作为任务执行环境。该模拟器运行 Android 12 系统（API level 31），启用 Google Play 服务，并部署在 Ubuntu 18.04 操作系统的 Linux 服

务器上。对于与模拟器不兼容的应用（如Snapchat），实验使用搭载Android14的GooglePixel5设备进行测试。

任务设置。LlamaTouch的任务设置分为两个阶段。首先，为Android Emulator构建Android系统镜像，以在特定任务状态下提供一个预先准备的环境，确保任务具有可控且可复现的起始点。其次，基于Android UIAutomator2库[176]开发任务初始化脚本，用于管理具有复杂约束条件的任务环境（例如需要手动登录或必须依赖真实设备运行的应用）。这些脚本可自动完成环境初始化，从而在最少人工干预下确保任务顺利开始执行。

移动端GUI智能体。本节选取了四类代表性移动端GUI智能体进行对比评估；包括监督学习模型、LLM以及VLM：（1）Auto-UI[40]：采用基于BLIP-2[177]与T5[178]变体的多模态Transformer模型作为决策核心。其输入为屏幕截图与任务描述。（2）AutoDroid[92]：使用LLM进行设备控制。该方法首先获取屏幕的文本化VH，并将其简化为可读性更高的HTML表示，随后将该HTML输入LLM生成相应的操作。本研究中，AutoDroid的后端选用GPT-4（gpt-4-0125-preview）[83]。（3）AppAgent[107]：利用GPT-4V[179]理解屏幕截图并生成动作。本实验使用了未进行文档预探索的AppAgent简化版本。（4）CoCo-Agent[167]：采用LLaVA[180]作为核心模型，并在原AITW数据集的子集上进行训练。

评估方法。为了判定移动端GUI智能体是否完成任务，本节实验比较了以下评估方法：

（1）单步动作匹配：该方法在多个移动端GUI智能体的基准测试中被广泛使用[40,47,88,92]。其核心思想是让移动端GUI智能体在LlamaTouch的交互式环境中执行任务。随后，单步动作匹配方法将智能体在交互式环境中生成的动作序列与任务集中的真值动作序列进行比较。当两个动作序列完全一致时，任务被视为完成。实验采用AITW[47]中的动作匹配算法：仅当两个动作在动作类型与参数上完全相同，才视为匹配。

（2）基于最长公共子序列（Longest Common Subsequence，LCS）的动作匹配：该方法由AndroidArena[50]提出，在单步动作匹配的基础上允许在真值动作之间出现冗余动作，只要执行序列包含真值序列作为子序列，即认为任务完成。

（3）LlamaTouch 评估方法：通过将智能体的执行轨迹与标注的关键状态进行比对，判断任务是否完成。

（4）人工评估：人工评估方法被广泛应用机器学习、大语言模型应用的评估[165,181]。在移动端GUI任务场景下，人工评判者根据GUI智能体的

执行轨迹判断任务是否完成。评估流程中，评估人员不仅被要求关注每个GUI表征上的动作及其参数，还会考虑任务执行过程中的整体应用状态变化。人工评估的结果被视为真值，用于比较三种自动化评估方法的准确性。

实验主要关注两方面的评估参数：（1）不同移动端GUI智能体的端到端任务完成率；（2）不同评估方法的准确率，该数值以人工评估的结果作为真值基准。

端到端任务完成率。在真实移动端环境中执行任务会产生两种结果：任务完成或任务未完成。在单步动作匹配要求执行序列与真值序列完全一致；基于LCS的匹配则允许冗余，只要包含真值序列即可；LlamaTouch则判断执行轨迹是否依次经过真值任务集中的所有关键状态。某一评估方法的端到端任务完成率计算公式为：评估为完成的任务数除以任务集总任务数。

评估准确率。本实验将基于GUI智能体执行轨迹的人工评估结果作为任务完成的真值。设  $N$  表示包含所有任务的真值数据集，对于数据集中  $N$  的任务  $n \in N$  ，人工验证结果记为  $H_{n} \in \{\text{True}, \text{False}\}$ ，其中 True 表示人工验证任务完成，False 表示人工验证任务未完成；当使用某一特定评估方法对相同的执行轨迹进行评估时，结果记为  $E_{n} \in \{\text{True}, \text{False}\}$  。因此，准确率定义为：

$$
\text {a c c u r a c y} = \frac {\sum_ {n \in N} \delta \left(E _ {n} , H _ {n}\right)}{| N |} \tag {5-1}
$$

其中， $\delta(E_n, H_n)$  为指示函数，当且仅当评估结果与人工评估一致时取1，否则取0。因此，某一评估方法的准确率被定义为其正确评估任务数占任务集任务总数  $|N|$  的比例。

# 5.4.2 任务完成率与评估方法准确率


表 5-6 GUI 智能体端到端任务完成率与评估方法准确率


<table><tr><td rowspan="2">GUI
智能体</td><td colspan="2">单步
动作匹配</td><td colspan="2">LCS
动作匹配</td><td colspan="2">LlamaTouch</td><td>人工
评估</td></tr><tr><td>TCR</td><td>Acc.</td><td>TCR</td><td>Acc.</td><td>TCR</td><td>Acc.</td><td>TCR</td></tr><tr><td>Auto-UI</td><td>0.00</td><td>98.18</td><td>0.00</td><td>98.18</td><td>4.44</td><td>96.57</td><td>1.82</td></tr><tr><td>AutoDroid</td><td>0.00</td><td>85.98</td><td>0.00</td><td>85.98</td><td>14.84</td><td>91.87</td><td>14.02</td></tr><tr><td>AppAgent</td><td>0.00</td><td>93.33</td><td>0.61</td><td>93.13</td><td>10.91</td><td>94.95</td><td>6.67</td></tr><tr><td>CoCo-Agent</td><td>0.00</td><td>97.97</td><td>0.00</td><td>97.97</td><td>4.47</td><td>96.34</td><td>2.03</td></tr><tr><td>Average</td><td>0.00</td><td>93.86</td><td>0.15</td><td>93.81</td><td>8.67</td><td>94.93</td><td>6.14</td></tr></table>

表5-6展示了不同评估方法的端到端任务完成率与准确率。三种方法的平均准确率均超过  $90\%$  ，但单步动作匹配与基于LCS的动作匹配在判别

GUI 智能体是否能正确完成的任务时表现不佳，多个 GUI 智能体任务完成率几乎为  $0\%$  。这种“高准确率一低完成率”的现象主要源于任务集中大量任务未被成功完成——人工验证结果表明，仅有约  $6\%$  的任务被判定为完成。相比之下，LlamaTouch 的任务完成率为  $8.67\%$  ，与人工验证结果高度接近，说明其能够更准确地反映任务执行结果。

为进一步展示LlamaTouch在缓解传统方法普遍存在的假阴性问题上的优势，实验进一步聚焦于人工验证认为已成功完成的任务，并剔除所有未完成任务。表5-7列出了这些任务中不同智能体的完成数量及其评估准确率。整体来看，各智能体平均成功完成约30个任务。在这些任务上，单步动作匹配与基于LCS的动作匹配的评估准确率的均值均不超过  $1\%$  ，几乎无法对真实环境中的任务完成情况做出有效判断。这一结果揭示了静态数据集动作匹配的局限性：由于真实环境下任务完成路径存在多样性，而静态数据集仅提供单一参考序列，导致匹配难度极高。与之相比，LlamaTouch在这些任务上的平均准确率达到  $79\%$  ，显著降低了假阴性比例，凸显了其在真实场景下对GUI自动化任务进行高保真评估的能力与优势。


表 5-7 人工评估成功任务中评估方法准确率


<table><tr><td rowspan="2">GUI 智能体</td><td>单步动作匹配</td><td>LSC动作匹配</td><td>LlamaTouch</td><td>人工评估</td></tr><tr><td>Acc.</td><td>Acc.</td><td>Acc.</td><td>成功完成任务数</td></tr><tr><td>Auto-UI</td><td>0.00</td><td>0.00</td><td>77.78</td><td>9</td></tr><tr><td>AutoDroid</td><td>0.00</td><td>0.00</td><td>73.91</td><td>69</td></tr><tr><td>AppAgent</td><td>0.00</td><td>3.03</td><td>93.94</td><td>33</td></tr><tr><td>CoCo-Agent</td><td>0.00</td><td>0.00</td><td>70.00</td><td>10</td></tr><tr><td>Average</td><td>0.00</td><td>0.76</td><td>78.91</td><td>30</td></tr></table>

# 5.4.3 消融实验

LlamaTouch 在真实环境中评估移动端 GUI 智能体执行轨迹时能够保持较高的准确率，这主要得益于其多类关键状态标注原语的设计与实现。本节重点评估了 LlamaTouch 中精确匹配与模糊匹配两类机制的有效性。本节汇总了实验中所有移动端智能体的执行轨迹，并通过依次关闭精确匹配或模糊匹配、以及逐一启用不同原语的方式进行性能对比。结果分别针对（1）全部任务，（2）AITW 任务，以及（3）LlamaTouch 新生成的任务进行评估，任务完成率与准确率结果如表 5-8 所示。

实验结果表明，精确匹配在提升评估准确率方面发挥了决定性作用。例如，当移除所有精确匹配原语时，LlamaTouch 在全部任务上的评估准确率

由  $95\%$  骤降至  $19\%$  。在所有精确匹配原语中，应用Activity匹配对准确率的贡献最为显著，这主要得益于其能够以简单高效的方式定位应用的功能页面，且该标注原语在任务集中占比高达  $35\%$  。GUI组件匹配同样保证了LlamaTouch的精确评估结果，其通常与应用Activity等屏幕定位原语结合使用，以在已匹配的屏幕中检测关键状态信息。相比之下，动作匹配与系统状态匹配对准确率的提升幅度相对较小，原因在于任务集中标注这类原语的任务数量有限。


表 5-8 LlamaTouch 针对不同关键状态标注原语的消融实验


<table><tr><td rowspan="2">评估方法</td><td colspan="2">所有任务</td><td colspan="2">AITW 任务</td><td colspan="2">LlamaTouch 任务</td></tr><tr><td>TCR</td><td>Acc.</td><td>TCR</td><td>Acc.</td><td>TCR</td><td>Acc.</td></tr><tr><td>LlamaTouch</td><td>8.67</td><td>94.93</td><td>17.46</td><td>89.43</td><td>6.38</td><td>96.36</td></tr><tr><td>LlamaTouch（去除精确匹配）</td><td>86.77</td><td>18.85</td><td>82.55</td><td>29.75</td><td>87.87</td><td>16.02</td></tr><tr><td>+应用 Activity 匹配</td><td>23.21</td><td>81.60</td><td>41.05</td><td>68.29</td><td>18.58</td><td>85.06</td></tr><tr><td>+动作匹配</td><td>86.62</td><td>19.01</td><td>81.81</td><td>30.49</td><td>87.87</td><td>16.02</td></tr><tr><td>+GUI 组件匹配</td><td>15.86</td><td>88.15</td><td>40.30</td><td>68.55</td><td>9.51</td><td>93.24</td></tr><tr><td>+系统状态匹配</td><td>85.41</td><td>20.22</td><td>75.91</td><td>36.40</td><td>87.87</td><td>16.02</td></tr><tr><td>LlamaTouch（去除模糊匹配）</td><td>10.54</td><td>93.26</td><td>23.36</td><td>84.02</td><td>7.21</td><td>95.66</td></tr><tr><td>+屏幕信息模糊匹配</td><td>10.24</td><td>93.36</td><td>22.13</td><td>84.76</td><td>7.15</td><td>95.60</td></tr><tr><td>+GUI 组件模糊匹配</td><td>8.97</td><td>94.83</td><td>18.69</td><td>88.69</td><td>6.45</td><td>96.43</td></tr></table>

与精确匹配相比，模糊匹配在当前实验中对评估准确率的提升并不显著。尽管数据集中包含30个屏幕级模糊匹配原语与52个文本框模糊匹配原语，但由于所评估的四个智能体能够成功完成含有这些原语的任务数量有限，其贡献未能得到充分体现。然而，模糊匹配原语在处理真实环境中常见的屏幕内容变化与GUI布局动态方面具有独特价值。随着未来移动端GUI智能体性能的提升并具备完成更复杂任务的能力，这类原语预计将在评估中发挥更大的作用，从而进一步提升LlamaTouch在真实场景下的适用性与鲁棒性。

# 5.4.4 GUI智能体性能分析

本节展示了不同移动端智能体在移动GUI自动化任务中的绝对能力。为便于分析，本节首先按照任务来源将其划分为AITW与本研究新生成的数据集两类，旨在评估在成熟数据集上训练过的智能体是否能够适应新的应用与任务。其次，依据任务集中完成任务所需的步骤数，将任务划分为三个难度等级：简单（steps≤4）、中等（4<steps≤8）和困难（steps>8）。

跨数据集性能。在四个被评估的智能体中，Auto-UI与CoCo-Agent曾在AITW数据集上进行过训练，而AutoDroid与AppAgent则分别调用GPT-4与GPT-4V。实验分别统计了它们在AITW任务与LlamaTouch新生成任

务上的端到端任务完成率，以衡量其在新任务与新应用中的泛化能力。表5-9结果表明，AutoDroid与AppAgent在AITW任务上的完成率显著高于在LlamaTouch任务上的表现，这主要归因于两个数据集任务复杂度的差异。相较之下，Auto-UI与CoCo-Agent在AITW任务上的完成率为  $12.75\%$  ，但在LlamaTouch任务中显著下降，仅为  $2.29\%$  与  $2.31\%$  。这一差距揭示了它们在应对未见任务时的局限性。考虑到现实世界中应用种类繁多且任务多样，具备强泛化能力以适应新场景的移动端GUI智能体无疑更具竞争力。


表 5-9 不同任务来源下 GUI 智能体性能分析


<table><tr><td rowspan="2">GUI 智能体</td><td colspan="4">端到端 TCR</td></tr><tr><td>所有任务</td><td>AITW 任务</td><td>LlamaTouch 任务</td><td></td></tr><tr><td>Auto-UI</td><td>4.44</td><td>12.75</td><td>2.29</td><td></td></tr><tr><td>AutoDroid</td><td>14.84</td><td>22.77</td><td>12.79</td><td></td></tr><tr><td>AppAgent</td><td>10.91</td><td>21.57</td><td>8.14</td><td></td></tr><tr><td>CoCo-Agent</td><td>4.47</td><td>12.75</td><td>2.31</td><td></td></tr></table>

不同任务复杂度下的性能。根据完成任务所需的步骤数，该分析将任务划分为三种难度等级，四种智能体性能如表5-10所示。整体而言，智能体在步骤较少的简单任务上更易取得成功，而随着任务复杂度的提升，完成率显著下降。在三类任务中，AutoDroid在总体表现上优于其他三种智能体，而AppAgent在各类任务上的完成率与AutoDroid接近。这一优势得益于GPT-4/4V具备的大规模知识储备和强大的跨模态解析能力，使其能够更好地理解自然语言指令并对像素级或文本化GUI表征做出合理推理。然而，尽管AutoDroid与AppAgent相较于其他模型表现突出，实验结果依然表明，现有移动端智能体在移动端GUI任务自动化方面的整体能力仍有显著提升空间。未来工作仍需进一步提升移动端GUI智能体的性能，以满足真实环境下用户的多样、复杂需求。


表 5-10 不同任务复杂度下 GUI 智能体性能分析


<table><tr><td rowspan="2">GUI 智能体</td><td colspan="4">端到端 TCR</td></tr><tr><td>所有任务</td><td>Steps≤4</td><td>4&lt;Steps≤8</td><td>Steps&gt;8</td></tr><tr><td>Auto-UI</td><td>4.44</td><td>4.95</td><td>4.19</td><td>4.82</td></tr><tr><td>AutoDroid</td><td>14.84</td><td>27.00</td><td>12.94</td><td>7.32</td></tr><tr><td>AppAgent</td><td>10.91</td><td>16.83</td><td>10.97</td><td>3.61</td></tr><tr><td>CoCo-Agent</td><td>4.47</td><td>7.92</td><td>3.91</td><td>2.41</td></tr></table>

# 5.5 本章小结

本章提出了面向真实移动环境的移动端GUI智能体评估系统LlamaTouch，旨在解决传统基于静态数据集的精确动作匹配方法在真实环

境中普遍存在的高假阴性问题。LlamaTouch 以关键状态而非固定动作序列作为评估基元，通过定义和标注少量关键状态，有效容纳了任务完成路径的多样性，从而更真实地反映 GUI 智能体的任务执行能力。该框架由三部分构成：其一，AgentEnv 提供统一接口以支持在模拟器和真实设备中执行任务并采集交互轨迹；其二，关键状态标注原语体系覆盖屏幕、组件和系统多层级状态，结合截图与视图层级信息实现细粒度和可复现的状态描述；其三，多层次状态匹配算法在屏幕和组件两个层面融合精确匹配与模糊匹配，以适应动态环境和多样化 GUI 布局。基于该框架，本章构建了包含 496 个任务、涵盖 57 款热门应用的交互式评估任务集，并在其中集成了四类代表性移动端 GUI 智能体。实验结果表明，LlamaTouch 的评估准确率接近  $80\%$  ，显著优于传统单步动作匹配方法。该研究不仅实现了兼顾高保真度与可扩展的评估机制，还揭示了现有 GUI 智能体在泛化性能及复杂任务执行中的不足，为后续研究提供了坚实的实验基础和统一的评估标准。

# 第六章 工作总结与展望

# 6.1 论文主要研究工作总结

本论文系统地研究了视觉语言模型驱动的移动端GUI智能体在构建与评估方面面临的核心挑战。随着智能手机成为连接人与数字世界的关键枢纽，复杂的人机交互给用户带来了日益沉重的操作负担。为此，以VLM为核心的移动端GUI智能体技术应运而生，旨在通过自动化操作来提升交互效率与体验。然而，当前该领域的研究在模型能力、执行效率和评估方法三个维度上仍存在显著的局限性，制约了其实际应用与发展。针对这些挑战，本文从数据、系统和评估三个层面展开了深入研究，提出了一套完整的解决方案。本文的主要研究贡献包括：第一，构建了大规模、高质量的移动端GUI数据集MobileViews，并设计了新颖的自监督学习范式：K步界面转移任务，以强化VLM的感知与决策能力；第二，提出了一种创新的并行推测执行框架，降低了GUI智能体的任务完成时延并提升了任务完成率；第三，设计了基于关键状态匹配的高保真自动化评估方法与评估平台LlamaTouch，解决了传统评估方法严重失真的问题。这些研究工作共同为构建更强大、更高效、且可科学评估的移动端GUI智能体提供了坚实的理论基础与实践方案。

在模型层面，为解决现有 VLM 缺乏高质量移动端 GUI 数据、难以准确理解复杂界面结构并进行可靠决策的问题，本文设计了一个可扩展的自动化数据采集系统：在硬件上利用由 120 个 Qualcomm Snapdragon 865 SoC 组成的 2 台 SoC 阵列服务器提供高密度原生 Android 运行环境，在软件上引入 LLM 辅助的应用遍历流程以处理登录、弹窗等复杂交互，配合必要的人类介入保证应用的可持续探索。依托该系统，本文收集并开源 MobileViews 数据集，包含来自超过 2 万个真实应用的近 60 万张独特 GUI 表征，在应用数量和屏幕数量上分别达到此前最大规模的开源数据集 Rico 的约 2 倍和近 10 倍；在此基础上，本文提出了“K 步界面转移”自监督强化学习任务，使模型直接从这些无标注 GUI 轨迹中学习界面状态变化与可执行动作之间的因果关系。实验结果显示，该训练范式在 GUI 动作预测任务上的准确率最高提升  $7.5\%$  ，相比传统有监督微调方案平均提升  $36\%$  ，显著强化了 VLM 的屏幕感知与任务决策能力，并为后续移动端 GUI 智能体

的感知-决策模块提供了数据和方法论基础。

在系统层面，针对现有GUI智能体因顺序执行模式而导致的任务完成效率低下问题，本文首先系统性分析了基于推理模型的测试时扩展方法在移动端GUI任务上的收益，指出仅凭静态基准上的逐步动作匹配无法真实反映智能体在交互任务中的有效性。基于这一分析，本文创新性地提出了交互式环境中并行推测执行的工作流，将传统的单路径探索优化为多路径并行探索。该系统的核心设计在于：在每个决策点，利用VLM生成多个具有潜在可能性的候选动作，并在通过快速状态复制技术生成的多个并行环境中同步执行。为确保该工作流的高效运行，本文设计了三个关键优化机制：一是意图感知的动作采样与去重，利用VLM分析动作背后的语义意图，过滤功能等价的冗余操作；二是基于写时复制的轻量级环境复制技术，大幅降低并行执行的环境创建开销；三是任务进度自感知的执行分支剪枝策略，通过将复杂任务分解为子目标，动态评估并裁剪进展落后的探索分支，实现计算资源的聚焦。结果表明，相比传统顺序式GUI智能体，该方法在交互式环境中以更少的操作步数实现了  $6.2\%$  的任务完成率提升，并在已完成任务中有  $74\%$  的任务以相等和更高效率执行。该方法为构建高响应速度与强鲁棒性的移动端GUI智能体提供了全新的系统级解决方案。

在评估层面，针对当前评估方法因依赖静态数据和精确动作匹配而导致的评估结果失真问题，本文构建了一套基于应用关键状态匹配的自动化评估方法，并开源了交互式评估平台LlamaTouch，涵盖496个移动端GUI自动化任务。LlamaTouch的核心原理是判断任务完成状态，而非逐步复现静态动作序列。为支持该方法，本文设计了一套多层级关键状态标注原语与匹配机制，可在屏幕、GUI组件与系统层面实现灵活的精确或模糊匹配，从而兼容真实环境中的动态界面变化与多样化任务路径。任务执行阶段，LlamaTouch在统一的移动端交互环境中自动采集GUI智能体执行轨迹，并通过多层次状态匹配算法判定其是否到达预定义关键状态，实现高保真度的自动化评估。实验结果显示，LlamaTouch的评估结果与人工判定结果的一致性接近  $80\%$  ，显著优于传统基于动作序列的静态评估方法，在保证高保真度的同时，推动了移动端GUI智能体评估标准向更贴近真实世界应用场景的方向演进，为移动端GUI智能体模型和系统的演进提供反馈。

综上所述，本文以VLM驱动的移动端GUI智能体的构建与评估为核心，系统性地探索了从数据获取、系统设计到评估方法的一体化研究路径。在数据层面，通过设计高效的自动化数据采集框架并构建MobileViews数据集，并在其之上构建自监督强化学习训练任务，突破了VLM在感知与

决策方面受限于训练数据的瓶颈；在系统层面，提出并实现了并行推测执行方法，解决了传统顺序执行模式下效率与成功率之间的矛盾，提升了智能体在复杂任务场景下的响应速度与鲁棒性；在评估层面，开发了LlamaTouch评估平台，创新性地引入关键状态匹配方法，使评估结果更贴近真实交互情境，极大提升了评估的科学性与可信度。以上方案为未来构建具备更强泛化能力、更高交互效率和更科学评估体系的移动端GUI智能体奠定了坚实的基础。

# 6.2 未来研究工作与展望

VLM的迅速发展为移动端智能体带来了新的机遇与挑战。本文工作从模型强化、系统优化与评估方法设计三方面解决VLM驱动的移动端GUI智能体在当前面临的基础问题。然而，移动端GUI智能体当前仍面临如下重大挑战，使得其难以在现实世界、真实用户的手机上部署。

首先，当前VLM的能力仍然受限，这直接导致基于VLM的GUI智能体在执行复杂任务时成功率偏低。特别是在跨应用组合的任务、操作步骤冗长的长序列任务，以及从未见过的新应用或新任务上，GUI智能体的表现尤为不稳定。例如，在AndroidWorld等基准环境中，最先进的开源VLM驱动的GUI智能体任务成功率也仅在  $70\%$  左右[182]。此外，当遇到训练中未出现过的界面或操作目标时，模型往往缺乏泛化能力，无法正确理解界面元素或做出准确操作[34]。低任务完成率和较差的泛化能力使得当前基于VLM难以充当稳定可靠的通用GUI智能体。未来的研究需要在提升VLM的基础能力和任务泛化性方面做出努力。一方面，可以通过更大规模、更多样化的GUI数据进行训练或增加中间任务微调，来增强VLM对不同应用程序GUI的感知和理解能力。另一方面，VLM需要学习GUI之间的转换关系，其核心在于为GUI智能体建立一种有效的自监督学习范式，以对标GPT等大型语言模型在自然语言处理领域中应用的下一词元预测（Next Token Prediction）机制。本研究认为，若干潜在的自监督任务值得深入探索，例如：基于历史交互序列的“下一动作预测”或是关注界面动态变化的“操作因果关系预测”。这类任务的共同目标是使智能体能够从未标注的大规模GUI交互数据中自主学习隐含的界面逻辑与操作规律。这不仅是通向真正通用GUI智能体的关键路径，也为其能力的持续涌现提供了理论基础。本研究提出关注界面动态变化的“K步界面转移”任务，作为从海量GUI轨迹中构造自监督训练任务的尝试。虽然实验结果展示了基于该任务训练的VLM具有泛化到其他基准测试的能力，但受到硬件资源

限制，实验中数据量以及VLM大小仍然受限，未来需要在更大规模数据与模型上进行实验以证明其有效性。在此基础上，在交互式环境中结合强化学习等在线微调策略，对预训练后的VLM进行持续优化，有望显著提升其在处理复杂任务时的决策精度与鲁棒性，最终缩小其与人类在真实世界跨应用场景下的表现差距。

当前移动端GUI智能体的系统工作流程仍以串行方式为主，即智能体需依次完成感知GUI状态、调用VLM决策、执行动作并进入下一步。这种顺序执行模式在面对复杂多步骤任务或未见过的场景时往往效率较低。本研究提出的并行推测执行方法能够在每个GUI状态下同时探索多条潜在操作路径，从而提升任务成功率与执行效率。然而，该方法目前仅在Android模拟器环境中得到验证，如何在资源受限的真实移动设备上实现多路径并行仍是一大挑战。首先，移动设备的操作系统属于封闭环境，难以像模拟器中那样灵活修改以支持多个沙盒并行运行动作候选。即便底层机制允许创建并行沙盒，移动端硬件资源有限，过多分支也可能导致过载或系统崩溃。一种可行的思路是利用基于GUI世界模型的推演替代真实环境中的动作执行[41-43]，但这要求世界模型具有精确的GUI建模和预测能力。另一类方案是借助云端虚拟化环境并行执行任务，从而显著提升效率并降低能耗[183,184]，但需通过隐私保护机制实现终端应用状态与云端环境的快速同步。此外，并行推测执行虽优于传统顺序模式，但因其在每个步骤均依赖VLM进行感知与决策，时延累积明显，甚至可能导致简单任务耗时达分钟级。未来可行的移动端GUI智能体或需采用大小模型协同机制（例如由大模型负责高层规划，小模型执行具体操作），或将GUI动作与传统基于API的自动化方法结合，以在保证决策质量的同时进一步提升响应速度[35]。

最后，在评估方法方面，LlamaTouch与同期的工作AndroidWorld[34]和OSWorld[185]都使用了应用关键状态匹配以判断任务是否成功执行。然而，这些基准的测试任务数量相对有限，难以全面反映GUI智能体在真实日常应用场景下的表现。例如，AndroidWorld仅涵盖20个应用的116个任务；本研究提出的LlamaTouch虽然在应用覆盖数量和任务数上有所提升，但规模仍然远小于计算机视觉领域ImageNet[186]数以百万计的标注样本。这一巨大差距表明，现有的评估基准尚不足以系统刻画GUI智能体的通用性与鲁棒性。未来亟需构建大规模、开放式的移动GUI智能体评估平台。与传统依赖人工采集和标注的方式相比，更高效的方法是通过自动化方法动态生成任务与交互轨迹，并利用界面层级结构与操作逻辑进行自动标注。

这不仅能够以低成本持续扩充任务库，还可以提升评估平台对新应用和新任务的适应性。更进一步，这类交互式的开放平台还可作为在线强化学习环境，使GUI智能体在评估过程中不断收集和积累交互经验，形成反馈闭环，从而进一步增强VLM的推理与决策能力。通过这种方式，评估体系不仅能够客观衡量现有能力，还能够持续推动智能体在真实应用场景中的演化与提升。

# 参考文献



[1] Apple. Siri[EB/OL].(2024). https://www.apple.com/siri/.





[2] 小米. 小爱同学[EB/OL].(2025). https://xiaoai.mi.com/.





[3] 华为．小艺[EB/OL].(2025). https://xiaoyi.huawei.com/.





[4] Google Inc. Google Assistant on Your Phone[EB/OL].(2025). https://assistant.google.com/platforms/phones/.





[5] 曹亚如, 张丽萍, 赵乐乐. 多轮任务型对话系统研究进展[J]. 计算机应用研究, 2022, 39(2): 331-341.





[6] 王博，陈冲，邓明，等．移动应用GUI测试自动生成技术综述[J].软件学报，2025,36(6):2713-2746.





[7] 赵阳洋，王振宇，王佩，等．任务型对话系统研究综述[J].计算机学报，2020,43(10):1862-1896.





[8] 张少坤，李元春，雷瀚文，等．基于多模态表征的移动应用GUI模糊测试框架[J].软件学报，2024,35(7):3162-3179.





[9] IFTTT. Automate business & home[EB/OL].(2025). https://ifttt.com/.





[10] Joaoapps. Tasker[EB/OL].(2025). https://tasker.joaoapps.com/.





[11] Apple. Shortcuts User Guide[EB/OL].(2025). https://support.apple.com/guide/shortcuts/welcome/ios.





[12] Zhang C, He S, Qian J, et al. Large Language Model-Brained GUI Agents: A Survey[J]. arXiv preprint arXiv:2411.18279, 2024: 1-100.





[13] Hu X, Xiong T, Yi B, et al. OS Agents: A Survey on MLLM-based Agents for Computer, Phone and Browser Use[C]//Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics, ACL 2025, Vienna, Austria, Jul 27-Aug 1, 2025: 7436-7465.





[14] Liu Z, Chen C, Wang J, et al. Make LLM a Testing Expert: Bringing Human-like Interaction to Mobile GUI Testing via Functionality-aware Decisions[C]// Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, ICSE 2024, Lisbon, Portugal, April 14-20, 2024, 100: 1-13.





[15] Wang J, Huang Y, Chen C, et al. Software Testing With Large Language Models: Survey, Landscape, and Vision[J]. IEEE Transactions on Software Engineering, 2024, 50(4): 911-936.





[16] Liu Z, Chen C, Wang J, et al. Testing the Limits: Unusual Text Inputs Generation





for Mobile App Crash Detection with Large Language Model[C]//Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, ICSE 2024, Lisbon, Portugal, April 14-20, 2024, 137: 1-12.





[17] Rewind AI, Inc. Rewind[EB/OL].(2025). https://www.rewind.ai/.





[18] Microsoft. Retrace your steps with Recall - Microsoft Support[EB/OL].(2025). https://support.microsoft.com/en-us/windows/retrace-your-steps-with-recall-AA03f8a0-a78b-4b3e-b0a1-2eb8ac48701c.





[19] Borges J, Levene M. Data Mining of User Navigation Patterns[C]//International Workshop on Web Usage Analysis and User Profiling, Berlin, Heidelberg, 1999: 92-112.





[20] Snell C V, Lee J, Xu K, et al. Scaling LLM Test-Time Compute Optimally Can be More Effective than Scaling Parameters for Reasoning[C]//Proceedings of the Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025.





[21] OpenAI. GPT-4o System Card[EB/OL].(2024). https://openai.com/index/openai-o1-system-card/.





[22] Team G, Anil R, Borgeaud S, et al. Gemini: A Family of Highly Capable Multimodal Models[J]. arXiv preprint arXiv:2312.11805, 2023: 1-90.





[23] Anthropic. Introducing the next generation of Claude[EB/OL].(2024). https://www.anthropic.com/news/claude-3-family.





[24] Android. View | API reference[EB/OL].(2025). https://developer.android.com/reference/android/view(View.





[25] Yan A, Yang Z, Zhu W, et al. GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation[J]. arXiv preprint arXiv:2311.07562, 2023: 1-12.





[26] Yang J, Zhang H, Li F, et al. Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V[J]. arXiv preprint arXiv:2310.11441, 2023: 1-23.





[27] Li Y, Li G, He L, et al. Widget Captioning: Generating Natural Language Description for Mobile User Interface Elements[C]//Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020: 5495-5510.





[28] Bai S, Chen K, Liu X, et al. Qwen2.5-VL Technical Report[J]. arXiv preprint arXiv:2502.13923, 2025: 1-23.





[29] Shen H, Li Y, Meng D, et al. ShortcutsBench: A Large-Scale Real-world Benchmark





for API-based Agents[C]//Proceedings of the Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025.





[30] Xie W, Zhang L, Wang S, et al. DroidCall: A Dataset for LLM-powered Android Intent Invocation[J]. arXiv preprint arXiv:2412.00402, 2024: 1-19.





[31] Wu P, Ma S, Wang B, et al. GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior[J]. arXiv preprint arXiv:2506.08012, 2025: 1-35.





[32] Liu Y, Li P, Wei Z, et al. InfGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection[J]. arXiv preprint arXiv:2501.04575, 2025: 1-14.





[33] Pan L, Wang B, Yu C, et al. AutoTask: Executing Arbitrary Voice Commands by Exploring and Learning from Mobile GUI[J]. arXiv preprint arXiv:2312.16062, 2023: 1-26.





[34] Rawles C, Clinckemaillie S, Chang Y, et al. AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents[C]//Proceedings of the Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025.





[35] Zhang C, Huang H, Ni C, et al. UFO2: The Desktop AgentOS[J]. arXiv preprint arXiv:2504.14603, 2025: 1-24.





[36] Li N, Qu X, Zhou J, et al. MobileUse: A GUI Agent with Hierarchical Reflection for Autonomous Mobile Operation[J]. arXiv preprint arXiv:2507.16853, 2025: 1-20.





[37] Song L, Dai Y, Prabhu V, et al. CoAct-1: Computer-using Agents with Coding as Actions[J]. arXiv preprint arXiv:2508.03923, 2025: 1-12.





[38] Zhang J, Wu J, Yihua T, et al. Android in the Zoo: Chain-of-Action-Thought for GUI Agents[C]//Findings of the 2024 Association for Computational Linguistics, EMNLP 2024, Miami, Florida, USA, November 12-16, 2024: 12016-12031.





[39] Zhang L, Gao L, Xu M. Does Chain-of-Thought Reasoning Help Mobile GUI Agent? An Empirical Study[J]. arXiv preprint arXiv:2503.16788, 2025: 1-16.





[40] Zhang Z, Zhang A. You Only Look at Screens: Multimodal Chain-of-Action Agents[C]//Findings of the 2024 Association for Computational Linguistics, ACL 2024, Bangkok, Thailand, August 11-16, 2024: 3132-3149.





[41] Chae H, Kim N, Ong K T, et al. Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation[C]//Proceedings of the Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025.





[42] Gu Y, Zhang K, Ning Y, et al. Is Your LLM Secretly a World Model of the Internet?





Model-Based Planning for Web Agents[J]. arXiv preprint arXiv:2411.06559, 2024: 1-22.





[43] Luo D, Tang B, Li K, et al. ViMo: A Generative Visual GUI World Model for App Agents[J]. arXiv preprint arXiv:2504.13936, 2025: 1-25.





[44] Koh J Y, McAleer S, Fried D, et al. Tree Search for Language Model Agents[J]. arXiv preprint arXiv:2407.01476, 2024: 1-24.





[45] Deka B, Huang Z, Franzen C, et al. Rico: A Mobile App Dataset for Building Data-Driven Design Applications[C]//Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology, UIST 2017, Quebec City, QC, Canada, October 22-25, 2017: 845-854.





[46] Li Y, He J, Zhou X, et al. Mapping Natural Language Instructions to Mobile UI Action Sequences[C]//Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020: 8198-8210.





[47] Rawles C, Li A, Rodriguez D, et al. AndroidInTheWild: A Large-Scale Dataset For Android Device Control[C]//Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10-16, 2023, 36: 59708-59728.





[48] Toyama D, Hamel P, Gergely A, et al. AndroidEnv: A Reinforcement Learning Platform for Android[J]. arXiv preprint arXiv:2105.13231, 2021: 1-13.





[49] Zhang D, Shen Z, Xie R, et al. Mobile-Env: Building Qualified Evaluation Benchmarks for LLM-GUI Interaction[J]. arXiv preprint arXiv:2305.08144, 2023: 1-36.





[50] Xing M, Zhang R, Xue H, et al. Understanding the Weakness of Large Language Model Agents within a Complex Android Environment[C]//Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2024, Barcelona, Spain, August 25-29, 2024: 6061-6072.





[51] Zhou S, Xu F F, Zhu H, et al. WebArena: A Realistic Web Environment for Building Autonomous Agents[C]//Proceedings of the Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024.





[52] Wang X, Wang B, Lu D, et al. OpenCUA: Open Foundations for Computer-Use Agents[J]. arXiv preprint arXiv:2508.09123, 2025: 1-42.





[53] Li W, Bishop W E, Li A, et al. On the Effects of Data Scale on UI Control Agents[C]//Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024,





Vancouver, BC, Canada, December 10-15, 2024, 37: 92130-92154.





[54] Google. Gemini 2.0 Flash | Generative AI on Vertex AI[EB/OL].(2024). https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash.





[55] Anthropic. Claude 3.7 Sonnet and Claude Code[EB/OL].(2025). https://www.anthropic.com/news/claude-3-7-sonnet.





[56] Aneja J, Deshpande A, Schwing A G. Convolutional Image Captioning[C]//Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018: 5561-5570.





[57] Hossain M Z, Sohel F, Shiratuddin M F, et al. A Comprehensive Survey of Deep Learning for Image Captioning[J]. ACM Computing Surveys, 2019, 51(6): 1-36.





[58] Ghandi T, Pourreza H, Mahyar H. Deep Learning Approaches on Image Captioning: A Review[J]. ACM Computing Surveys, 2023, 56(3): 1-39.





[59] You Q, Jin H, Wang Z, et al. Image Captioning with Semantic Attention[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016: 4651-4659.





[60] Herdade S, Kappeler A, Boakye K, et al. Image Captioning: Transforming Objects Into Words[C]//Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, Vancouver, BC, Canada, December 8-14, 2019, 32: 11135-11145.





[61] Vinyals O, Toshev A, Bengio S, et al. Show and tell: A neural image caption generator[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015: 3156-3164.





[62] Li Q, Tao Q, Joty S, et al. VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual Questions[C]//Proceedings of the 15th European Conference on Computer Vision, ECCV 2018, Munich, Germany, September 8-14, 2018, 552-567.





[63] Antol S, Agrawal A, Lu J, et al. VQA: Visual Question Answering[C]//2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015: 2425-2433.





[64] Goyal Y, Khot T, Summers-Stay D, et al. Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering[C]//Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017: 6325-6334.





[65] Singh A, Natarajan V, Shah M, et al. Towards VQA Models That Can Read[C]//Proceedings of the 2019 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019: 8317-8326.





[66] Guo M-H, Xu T-X, Liu J-J, et al. Attention Mechanisms in Computer Vision: A Survey[J]. Computational Visual Media, 2022, 8(3): 331-368.





[67] Vaswani A, Shazeer N, Parmar N, et al. Attention is All you Need[C]//Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, NIPS 2017, Long Beach, CA, USA, December 4-9, 2017, 30: 5998-6008.





[68] Xu K, Ba J, Kiros R, et al. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention[C]//Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, July 6-11, 2015: 2048-2057.





[69] Anderson P, He X, Buehler C, et al. Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering[C]//Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018: 6077-6086.





[70] Liu Y, Ott M, Goyal N, et al. RoBERTa: A Robustly Optimized BERT Pretraining Approach[J]. arXiv preprint arXiv:1907.11692, 2019: 1-13.





[71] Yang Z, Dai Z, Yang Y, et al. XLNet: Generalized Autoregressive Pretraining for Language Understanding[C]//Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, Vancouver, BC, Canada, December 8-14, 2019, 32: 5754-5764.





[72] Radford A, Wu J, Child R, et al. Language Models are Unsupervised Multitask Learners[J]. OpenAI blog, 2019: 1-24.





[73] Devlin J, Chang M-W, Lee K, et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding[C]//Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics, NAACL 2019, Minneapolis, MN, USA, June 2-7, 2019: 4171-4186.





[74] Tan H, Bansal M. LXMERT: Learning Cross-Modality Encoder Representations from Transformers[C]//Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, November 3-7, 2019: 5099-5110.





[75] Lu J, Batra D, Parikh D, et al. ViLBERT: Pretraining Task-Agnostic Visiolinguistic





Representations for Vision-and-Language Tasks[C]//Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, Vancouver, BC, Canada, December 8-14, 2019, 32: 13-23.





[76] Li L H, Yatskar M, Yin D, et al. VisualBERT: A Simple and Performant Baseline for Vision and Language[J]. arXiv preprint arXiv:1908.03557, 2019: 1-14.





[77] Chen Y-C, Li L, Yu L, et al. UNITER: UNiversal Image-Text Representation Learning[C]//Proceedings of the 16th European Conference on Computer Vision, ECCV 2020, Glasgow, UK, August 23-28, 2020: 104-120.





[78] Radford A, Kim J W, Hallacy C, et al. Learning Transferable Visual Models From Natural Language Supervision[C]//Proceedings of the 38th International Conference on Machine Learning, ICML 2021, Virtual Event, July 18-24, 2021: 8748-8763.





[79] Jia C, Yang Y, Xia Y, et al. Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision[C]//Proceedings of the 38th International Conference on Machine Learning, ICML 2021, Virtual Event, July 18-24, 2021: 4904-4916.





[80] Li J, Li D, Xiong C, et al. BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation[C]//Proceedings of the 39th International Conference on Machine Learning, ICML 2022, Baltimore, Maryland, USA, July 17-23, 2022: 12888-12900.





[81] Li J, Li D, Savarese S, et al. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models[C]//Proceedings of the 40th International Conference on Machine Learning, ICML 2023, Honolulu, Hawaii, USA, July 23-29, 2023: 19730-19742.





[82] Alayrac J-B, Donahue J, Luc P, et al. Flamingo: a Visual Language Model for Few-Shot Learning[C]//Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, Nov 28-Dec 9, 2022, 35: 23716-23736.





[83] OpenAI, Achiam J, Adler S, et al. GPT-4 Technical Report[J]. arXiv preprint arXiv:2303.08774, 2023: 1-100.





[84] Zhang C, Li L, He S, et al. UFO: A UI-Focused Agent for Windows OS Interaction[C]//Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics, NAACL 2025, Albuquerque, New Mexico, USA, April 29-May 4, 2025: 597-622.





[85] Anthropic. Introducing computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku[EB/OL].(2024). https://www.anthropic.com/news/3-5-models-and-computer-use.





[86] OpenAI. Computer-Using Agent[EB/OL].(2025). https://openai.com/index/computer-using-agent/.





[87] OpenAI. Introducing Operator[EB/OL].(2025). https://openai.com/index/introducing-operator/.





[88] Sun L, Chen X, Chen L, et al. META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022: 6699-6712.





[89] 肖泳利. 面向智能个人助理的用户多意图识别方法研究[D]. 哈尔滨工业大学, 2022.





[90] Hong W, Wang W, Lv Q, et al. CogAgent: A Visual Language Model for GUI Agents[C]//Proceedings of the 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024: 14281-14290.





[91] Wang B, Li G, Li Y. Enabling Conversational Interaction with Mobile UI using Large Language Models[C]//Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, CHI 2023, Hamburg, Germany, April 23-28, 2023, 432: 1-17.





[92] Wen H, Li Y, Liu G, et al. AutoDroid: LLM-powered Task Automation in Android[C]//Proceedings of the 30th Annual International Conference on Mobile Computing and Networking, ACM MobiCom 2024, Washington D.C., DC, USA, November 18-22, 2024: 543-557.





[93] Qin Y, Liang S, Ye Y, et al. ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs[C]//Proceedings of the Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024.





[94] Schick T, Dwivedi-Yu J, Dessi R, et al. Toolformer: Language Models Can Teach Themselves to Use Tools[C]//Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10-16, 2023, 36: 68539-68551.





[95] M. Bran A, Cox S, Schilter O, et al. Augmenting Large Language Models with Chemistry Tools[J]. Nature Machine Intelligence, 2024, 6(5): 525-535.





[96] Microsoft. Getting started with Copilot on Windows[EB/OL].(2025). https://support.microsoft.com/en-us/topic/getting-started-with-copilot-on-windows-1159c61f





86c3-4755-bf83-7fbff7e0982d.





[97] Li Y, Wen H, Wang W, et al. Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security[J]. arXiv preprint arXiv:2401.05459, 2024: 1-62.





[98] Cheng K, Sun Q, Chu Y, et al. SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents[C]//Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand, August 11-16, 2024: 9313-9332.





[99] Wu Z, Wu Z, Xu F, et al. OS-ATLAS: Foundation Action Model for Generalist GUI Agents[C]//Proceedings of the Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025.





[100] Li K, Meng Z, Lin H, et al. ScreenSpot-Pro: GUI Grounding for Professional High-Resolution Computer Use[J]. arXiv preprint arXiv:2504.07981, 2025: 1-17.





[101] Hsiao Y-C, Zubach F, Baechler G, et al. ScreenQA: Large-Scale Question-Answer Pairs Over Mobile App Screenshot[C]//Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics, NAACL 2025, Albuquerque, New Mexico, USA, April 29-May 4, 2025: 9427-9452.





[102] Baechler G, Sunkara S, Wang M, et al. ScreenAI: A Vision-Language Model for UI and Infographics Understanding[C]//Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI 2024, Jeju, South Korea, August 3-9, 2024: 3058-3068.





[103] Wen H, Tian S, Pavlov B, et al. AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation[J]. arXiv preprint arXiv:2412.18116, 2024: 1-13.





[104] Zhao S, Wen H, Du W, et al. LLM-Explorer: Towards Efficient and Affordable LLM-based Exploration for Mobile Apps[J]. arXiv preprint arXiv:2505.10593, 2025: 1-15.





[105] Lee S, Choi J, Lee J, et al. MobileGPT: Augmenting LLM with Human-like App Memory for Mobile Task Automation[C]//Proceedings of the 30th Annual International Conference on Mobile Computing and Networking, ACM MobiCom 2024, Washington D.C., DC, USA, November 18-22, 2024: 1119-1133.





[106] Taeb M, Swearngin A, Schoop E, et al. AXNav: Replaying Accessibility Tests from Natural Language[C]//Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI 2024, Honolulu, HI, USA, May 11-16, 2024, 962: 1-16.





[107] Zhang C, Yang Z, Liu J, et al. AppAgent: Multimodal Agents as Smartphone Users[C]//Proceedings of the 2025 CHI Conference on Human Factors in Computing





Systems, CHI 2025, Yokohama, Japan, April 26-May 1, 2025, 70: 1-20.





[108] Wu Q, Gao P, Liu W, et al. BacktrackAgent: Enhancing GUI Agent with Error Detection and Backtracking Mechanism[J]. arXiv preprint arXiv:2505.20660, 2025: 1-23.





[109] Wang B, Li G, Zhou X, et al. Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning[C]//Proceedings of the 34th Annual ACM Symposium on User Interface Software and Technology, UIST 2021, Virtual Event, USA, October 10-14, 2021: 298-510.





[110] Zhang L, Wang S, Jia X, et al. LlamaTouch: A Faithful and Scalable Testbed for Mobile UI Task Automation[C]//Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology, UIST 2024, Pittsburgh, PA, USA, October 13-16, 2024, 132: 1-13.





[111] DeepSeek-AI, Guo D, Yang D, et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning[J]. arXiv preprint arXiv:2501.12948, 2025: 1-22.





[112] Wang Y, Chen W, Han X, et al. Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning[J]. arXiv preprint arXiv:2401.06805, 2024: 1-29.





[113] Qin Y, Ye Y, Fang J, et al. UI-TARS: Pioneering Automated GUI Interaction with Native Agents[J]. arXiv preprint arXiv:2501.12326, 2025: 1-44.





[114] Gao L, Zhang L, Xu M. UIShift: Enhancing VLM-based GUI Agents through Self-supervised Reinforcement Learning[J]. arXiv preprint arXiv:2505.12493, 2025: 1-15.





[115] Wang Z, Xu H, Wang J, et al. Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks[J]. arXiv preprint arXiv:2501.11733, 2025: 1-21.





[116] Fang T, Zhang H, Zhang Z, et al. WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model[J]. arXiv preprint arXiv:2504.21024, 2025: 1-17.





[117] Hu Z, Xiong S, Zhang Y, et al. Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation[J]. arXiv preprint arXiv:2504.16073, 2025: 1-17.





[118] Putta P, Mills E, Garg N, et al. Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents[J]. arXiv preprint arXiv:2408.07199, 2024: 1-22.





[119] Madaan A, Tandon N, Gupta P, et al. Self-Refine: Iterative Refinement with Self





Feedback[C]//Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10-16, 2023, 36: 46534-46594.





[120] Wang X, Wei J, Schuurmans D, et al. Self-Consistency Improves Chain of Thought Reasoning in Language Models[C]//Proceedings of the Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.





[121] Wei J, Wang X, Schuurmans D, et al. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models[C]//Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, Nov 28-Dec 9, 2022, 35: 24824-24837.





[122] Zhang Z, Fang T, Ma K, et al. Enhancing Web Agents with Explicit Rollback Mechanisms[J]. arXiv preprint arXiv:2504.11788, 2025: 1-12.





[123] Shen J, Bai H, Zhang L, et al. Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction[J]. arXiv preprint arXiv:2506.07976, 2025: 1-41.





[124] Deka B, Huang Z, Franzen C, et al. Rico: A Mobile App Dataset for Building Data-Driven Design Applications[C]//Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology. Québec City QC Canada: ACM, 2017: 845-854.





[125] Burns A, Arsan D, Agrawal S, et al. A Dataset for Interactive Vision-Language Navigation with Unknown Command Feasibility[C]//Proceedings of the 17th European Conference on Computer Vision, ECCV 2022, Tel Aviv, Israel, October 23-27, 2022: 312-328.





[126] You K, Zhang H, Schoop E, et al. Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs[C]//Proceedings of the 18th European Conference on Computer Vision, ECCV 2024, Milan, Italy, Sep 29-Oct 4, 2024: 240-255.





[127] Google. Use Google Assistant to summarize web pages[EB/OL].(2024). https://support.google.com/assistant/answer/14163109?hl=en.





[128] Samsung. How to set up and use the Browsing assist features on the Galaxy S24[EB/OL].(2024). https://www.samsung.com/levant/support/mobile-devices/how-to-set-up-and-use-the-browsing-assist-features-on-the-galaxy-s24/.





[129] OpenAI. GPT-4V(ision) System Card[EB/OL].(2023). https://openai.com/research/gpt-4v-system-card.





[130] Team G, Georgiev P, Lei V I, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context[J]. arXiv preprint arXiv:2403.05530, 2024: 1-154.





[131] Tom W. Microsoft's Recall AI feature won't be available for Windows testers until October[EB/OL].(2024). https://www.theverge.com/2024/8/21/24225439/microsoft-recall-windows-ai-feature-october-testing.





[132] 钱文君, 沈晴霓, 吴鹏飞, 等. 大数据计算环境下的隐私保护技术研究进展[J]. 计算机学报, 2022, 45(4): 669-701.





[133] Gunter T, Wang Z, Wang C, et al. Apple Intelligence Foundation Language Models[J]. arXiv preprint arXiv:2407.21075, 2024: 1-47.





[134] Google. UI/Application Exerciser Monkey[EB/OL].(2023). https://developer.android.com/studio/test/other-testing-tools/monkey.





[135] Yuanchun Li, Ziyue Yang, Yao Guo, et al. DroidBot: a lightweight UI-Guided test input generator for android[C]//Proceedings of the 39th International Conference on Software Engineering, ICSE 2017, Buenos Aires, Argentina, May 20-28, 2017: 23-26.





[136] Zhang L, Fu Z, Shi B, et al. More is Different: Prototyping and Analyzing a New Form of Edge Server with Massive Mobile SoCs[C]//Proceedings of the 2024 USENIX Annual Technical Conference, USENIX ATC 2024, Santa Clara, CA, USA, July 10-12, 2024: 285-302.





[137] xiaocong. xiaocong/uiautomator: Python wrapper of Android uiautomator test tool[EB/OL].(2024). https://github.com/xiaocong/uiautomator.





[138] Google. Android Debug Bridge (adb)[EB/OL].(2024). https://developer.android.com/tools/adb.





[139] Lin H, Qiu J, Wang H, et al. Virtual Device Farms for Mobile App Testing at Scale: A Pursuit for Fidelity, Efficiency, and Accessibility [C]//Proceedings of the 29th Annual International Conference on Mobile Computing and Networking, ACM MobiCom 2023, Madrid, Spain, October 2-6, 2023, 45: 1-17.





[140] Qualcomm. Qualcomm Snapdragon 865 5G Mobile Platform [EB/OL].(2019). https://www_qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-865-5g-mobile-platform.





[141] Buchner J. ImageHash: An image hashing library written in Python[EB/OL].(2024). https://github.com/JohannesBuchner/imagehash.





[142] Fok R, Zhong M, Ross A S, et al. A Large-Scale Longitudinal Analysis of Missing





Label Accessibility Failures in Android Apps[C]//Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, CHI 2022, New Orleans, LA, USA, Apr 29-May 5, 2022, 461: 1-16.





[143] Apple. Human Interface Guidelines - Accessibility[EB/OL].(2024). https://developer.apple.com/design/human-interface-guidelines/accessibility.





[144] Google Inc. Google Accessibility Guidelines[EB/OL].(2024). https://developer.android.com/guide/topics/ui/accessibility/apps.





[145] Gao L, Zhang L, Wang S, et al. MobileViews: A Large-Scale Mobile GUI Dataset[J]. arXiv preprint arXiv:2409.14337, 2024: 1-14.





[146] Tian Y, Yang S, Zeng J, et al. Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation[C]//Proceedings of the Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025.





[147] Brandfonbrener D, Nachum O, Bruna J. Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation[C]//Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10-16, 2023, 36: 66953-66978.





[148] Zapolsky S, Drumwright E M. Inverse Dynamics with Rigid Contact and Friction[J]. Autonomous Robots, 2017, 41(4): 831-863.





[149] Shao Z, Wang P, Zhu Q, et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models[J]. arXiv preprint arXiv:2402.03300, 2024: 1-30.





[150] Hu E J, Shen Y, Wallis P, et al. LoRA: Low-Rank Adaptation of Large Language Models[C]//Proceedings of the Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022.





[151] Yao Y, Yu T, Zhang A, et al. MiniCPM-V: A GPT-4V Level MLLM on Your Phone[J]. arXiv preprint arXiv:2408.01800, 2024: 1-26.





[152] Chen Z, Wu J, Wang W, et al. InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks[C]//Proceedings of the 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024: 24185-24198.





[153] Rajpurkar P, Zhang J, Lopyrev K, et al. SQuAD: 100,000+ Questions for Machine Comprehension of Text[C]//Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA,





November 1-4, 2016: 2383-2392.





[154] Shen H, Liu P, Li J, et al. VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model[J]. arXiv preprint arXiv:2504.07615, 2025: 1-14.





[155] Agashe S, Wong K, Tu V, et al. Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents[J]. arXiv preprint arXiv:2504.00906, 2025: 1-18.





[156] Fan Y, Wang S, Fei Z, et al. Can Cooperative Multi-Agent Reinforcement Learning Boost Automatic Web Testing? An Exploratory Study[C]//Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering, ASE 2024, Sacramento, CA, USA, Oct 27-Nov 1, 2024: 14-26.





[157] Google. Emulator Snapshots[EB/OL].(2025). https://developer.android.com/studio/run/emulator-snapshots.





[158] Google. Gemini 2.5 Flash[EB/OL].(2025). https://deepmind.google/models/gemini/flash/.





[159] OpenAI. o1 System Card[EB/OL].(2024). https://openai.com/index/openai-o1-system-card/.





[160] Grok. Grok 3 Beta[EB/OL].(2025). https://x.ai/blog/grok-3.





[161] Deng S, Xu W, Sun H, et al. Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents[C]//Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand, August 11-16, 2024: 8813-8831.





[162] Huang L, Yu W, Ma W, et al. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions[J]. ACM Transactions on Information Systems, 2025, 43(2): 1-55.





[163] Yang Z, Li L, Lin K, et al. The Dawn of LMMs: Preliminary Explorations with GPT-4V(ison)[J]. arXiv preprint arXiv:2309.17421, 2023: 1-166.





[164] Neil B. Overlay Filesystem — The Linux Kernel documentation [EB/OL].(2025). https://docs.kernel.org/filesystems/overlayfs.html.





[165] Chiang C-H, Lee H. Can Large Language Models Be an Alternative to Human Evaluations?[C]//Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, ACL 2023, Toronto, Canada, July 9-14, 2023: 15607-15631.





[166] Google. Run apps on the Android Emulator | Android Developers[EB/OL].(2023). https://developer.android.com/studio/run/emulator.





[167] Ma X, Zhang Z, Zhao H. CoCo-Agent: A Comprehensive Cognitive MLLM Agent for Smartphone GUI Automation[C]//Findings of the 62nd Annual Meeting of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand, August 11-16, 2024: 9097-9110.





[168] Google. Android Activity [EB/OL].(2024). https://developer.android.com/reference/android/app/Activity.





[169] Chang Y, Cao G, Narang M, et al. WebQA: Multihop and Multimodal QA[C]//Proceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022: 16474-16483.





[170] OpenAI. BrowseComp: a benchmark for browsing agents[EB/OL].(2025). https://openai.com/index/browsecomp/.





[171] Yao S, Zhao J, Yu D, et al. ReAct: Synergizing Reasoning and Acting in Language Models[C]//Proceedings of the Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.





[172] Feiz S, Wu J, Zhang X, et al. Understanding Screen Relationships from Screenshot of Smartphone Applications[C]//Proceedings of the 27th International Conference on Intelligent User Interfaces, Helsinki, Finland, March 22-25, 2022: 447-458.





[173] Ian L. Single activity: Why, when, and how[EB/OL].(2018). https://www.youtube.com/watch?v=2k8x8V77CrU.





[174] Reimers N, Gurevych I. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks[C]//Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019: 3980-3990.





[175] W3C. XML Path Language 3.1[EB/OL].(2017). https://www.w3.org/TR/xpath-31/.





[176] Appium. UIAutomator2[EB/OL].(2024). https://github.com/appium/appium-uautomator2-driver.





[177] Li J, Li D, Savarese S, et al. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models[C]//Proceedings of the 40th International Conference on Machine Learning, ICML 2023, Honolulu, Hawaii, USA, July 23-29, 2023: 19730-19742.





[178] Raffel C, Shazeer N, Roberts A, et al. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer[J]. Journal of Machine Learning Research,





2020, 21(140): 1-67.





[179] Zheng B, Gou B, Kil J, et al. GPT-4V(ison) is a Generalist Web Agent, if Grounded[C]//Proceedings of the 41th International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024.





[180] Liu H, Li C, Wu Q, et al. Visual Instruction Tuning[C]//Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10-16, 2023, 36: 34892-34916.





[181] Mosqueira-Rey E, Hernández-Pereira E, Alonso-Ríos D, et al. Human-in-the-loop machine learning: a state of the art[J]. Artificial Intelligence Review, 2023, 56(4): 3005-3054.





[182] Ye J, Zhang X, Xu H, et al. Mobile-Agent-v3: Foundamental Agents for GUI Automation[J]. arXiv preprint arXiv:2508.15144, 2025: 1-30.





[183] Kosta S, Aucinas A, Pan Hui, et al. ThinkAir: Dynamic Resource Allocation and Parallel Execution in the Cloud for Mobile Code Offloading[C]//Proceedings of the 2012 IEEE Conference on Computer Communications, IEEE INFOCOM 2012, Orlando, FL, USA, March 25-30, 2012: 945-953.





[184] Apple Inc. Blog - Private Cloud Compute: A new frontier for AI privacy in the cloud[EB/OL].(2025). https://security.apple.com/blog/private-cloud-compute/.





[185] Xie T, Zhang D, Chen J, et al. OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments[C]//Advances in Neural Information Processing Systems 37: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10-15, 2024, 37: 52040-52094.





[186] Deng J, Dong W, Socher R, et al. ImageNet: A Large-Scale Hierarchical Image Database[C]//Proceedings of the 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, CVPR 2009, Miami, Florida, USA, June 20-25, 2009: 248-255.




附录


<table><tr><td>缩略语</td><td>英文全称</td><td>中文含义</td></tr><tr><td>VLM</td><td>Vision-Language Model</td><td>视觉语言模型</td></tr><tr><td>LLM</td><td>Large Language Model</td><td>大语言模型</td></tr><tr><td>GUI</td><td>Graphical User Interface</td><td>图形用户界面</td></tr><tr><td>PSE</td><td>Parallel Speculative Execution</td><td>并行推测执行</td></tr><tr><td>API</td><td>Application Programming Interface</td><td>应用编程接口</td></tr><tr><td>ADB</td><td>Android Debug Bridge</td><td>安卓调试桥</td></tr><tr><td>TTS</td><td>Test-Time Scaling</td><td>测试时扩展</td></tr><tr><td>VH</td><td>View Hierarchy</td><td>视图层级</td></tr><tr><td>PE</td><td>Prompt Engineering</td><td>提示词工程</td></tr><tr><td>CoT</td><td>Chain of Thought</td><td>思维链</td></tr><tr><td>SoC</td><td>System-on-Chip</td><td>片上系统</td></tr><tr><td>RL</td><td>Reinforcement Learning</td><td>强化学习</td></tr><tr><td>CoW</td><td>Copy on Write</td><td>写时复制</td></tr><tr><td>VQA</td><td>Visual Question Answering</td><td>视觉问答</td></tr><tr><td>CDF</td><td>Cumulative Distribution Function</td><td>累计分布函数</td></tr><tr><td>SFT</td><td>Supervised Fine-Tuning</td><td>有监督微调</td></tr><tr><td>BBox</td><td>Bounding Box</td><td>边界框</td></tr><tr><td>AVD</td><td>Android Virtual Device</td><td>安卓虚拟设备</td></tr><tr><td>SSD</td><td>Solid-State Drive</td><td>固态硬盘</td></tr><tr><td>I/O</td><td>Input/Output</td><td>输入/输出</td></tr><tr><td>GRPO</td><td>Group Relative Policy Optimization</td><td>群体相对策略优化</td></tr><tr><td>LoRA</td><td>Low-Rank Adaption</td><td>低秩自适应</td></tr><tr><td>LCS</td><td>Longest Common Subsequence</td><td>最长公共子序列</td></tr><tr><td>URL</td><td>Uniform Resource Locator</td><td>统一资源定位符</td></tr><tr><td>TCR</td><td>Task Completion Rate</td><td>任务完成率</td></tr></table>